<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

<!-- generator="WordPress/4.9.10" created="2019-04-01 03:42" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>Bits and Atoms</title>
	<link>https://bitsandatoms.co</link>
	<description></description>
	<pubDate>Mon, 01 Apr 2019 03:42:25 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>https://bitsandatoms.co</wp:base_site_url>
	<wp:base_blog_url>https://bitsandatoms.co</wp:base_blog_url>

	<wp:author><wp:author_id>1</wp:author_id><wp:author_login><![CDATA[nikolasjdawson@gmail.com]]></wp:author_login><wp:author_email><![CDATA[nik@bitsandatoms.co]]></wp:author_email><wp:author_display_name><![CDATA[nik@bitsandatoms.co]]></wp:author_display_name><wp:author_first_name><![CDATA[Nik]]></wp:author_first_name><wp:author_last_name><![CDATA[Dawson]]></wp:author_last_name></wp:author>


	<generator>https://wordpress.org/?v=4.9.10</generator>

	<item>
		<title>The Perils of Progress</title>
		<link>https://bitsandatoms.co/the-perils-of-progress/</link>
		<pubDate>Thu, 17 Aug 2017 21:40:05 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=55</guid>
		<description></description>
		<content:encoded><![CDATA[<h4><span style="font-weight: 400;">On the implications of intelligent machines and the transition to the knowledge economy.</span></h4>
<h5>INTRODUCTION</h5>
<span style="font-weight: 400;">Knowledge has become currency in the global economy. The knowledge economy is elevating Information as the primary source of value, overtaking material resources from our Industrial past. This is because Information feeds and defines intelligent machines.[note]'Intelligent Machines' or 'Artificial Intelligence' refers to a non-organic autonomous entities that are able to sense and act upon an environment to achieve specific goals. Intelligent agents may also learn or use knowledge to achieve these goals, which are governed by algorithms that are made by people.[/note]</span><span style="font-weight: 400;"> And it’s these intelligent technologies that leverage human knowledge, which help us to achieve more. </span>

<span style="font-weight: 400;">So, as human knowledge is supplemented by intelligent machines, more humans are becoming a function of their ability to interact with intelligent machines. Therefore, equipping people with technical skills is of growing importance. </span>

<span style="font-weight: 400;">Particularly important are the technical skills that enable people to build, manage, and improve the software and hardware powering intelligent machines. This includes skills such as design engineering, software development, and robotics. But it equally extends to areas such as Data Analysis, Human-Computer Interaction, or other technical disciplines helping steer intelligent technologies. Ultimately, it’s technical skillsets such as these that enhance human knowledge and help unlock idle capacity.</span>

<span style="font-weight: 400;">However, these technical disciplines demand high levels of skill. They require training and preparation, often with a strong foundation in Science, Technology, Engineering and Mathematics (STEM). As the demand for high-skilled labour outpaces lower-skilled work,[note]Pew Research Center (2016), <a href="http://www.pewsocialtrends.org/2016/10/06/1-changes-in-the-american-workplace/"><i>The State of American Jobs</i></a><i>: The changing demand for job skills and preparation</i>.[/note]</span><span style="font-weight: 400;"> industry is unable to meet these demands because people aren’t upskilling fast enough. This is concerning because low and medium-skilled jobs are at the greatest risk of automation.[note]‘Low and medium-skilled labour’ refers to routine and rules-based physical and cognitive tasks. Furman, Jason (2016), <a href="https://obamawhitehouse.archives.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf"><i>Is This Time Different? The Opportunities and Challenges of Artificial Intelligence</i></a>, Council of Economic Advisers to the White House, New York University, pg. 5-6.[/note]</span>

<span style="font-weight: 400;">There are two central problems underlying these labour market dynamics:</span>
<ol>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The majority of the world’s labour force are low and medium-skilled workers, with a disproportionate amount of whom live in the Global South[note]‘Global South refers to developing countries, which are located primarily in the Southern Hemisphere. United Nations <a href="http://ssc.undp.org/content/dam/ssc/documents/exhibition_triangular/SSCExPoster1.pdf">South-South Cooperation</a> (2007).[/note]</span><span style="font-weight: 400;">; </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Not nearly enough is being done to upskill and prepare workers for the demands of the knowledge economy, particularly in the Global South.</span></li>
</ol>
<img class="aligncenter wp-image-82" src="https://bitsandatoms.co/wp-content/uploads/2017/08/Global-South.png" alt="" width="718" height="368" />
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">Map of the Global South in red and Global North in blue</span></p>
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">Source: Wikimedia Foundation</span></p>
<span style="font-weight: 400;">The implications of widespread skill shortages are significant. Income inequality will rise; social unrest will ensue; and entire populations could lose their opportunity to contribute. While this dystopian outlook is intentionally hyperbolic, there are signs that this is already happening.[note]McKinsey Global Institute (2016), <i>Poorer than their parents? Flat or Falling Incomes in Advanced Economies</i>.[/note]</span>
<blockquote>It doesn’t have to be this way. This doesn’t have to be our future.</blockquote>
<span style="font-weight: 400;">Through systematic education and training we can raise the hidden talents and untapped potential of current and future workers. However, it will require a different conception of how skills are developed and education is delivered. We’ll need a strategy that follows a demand-led approach, which engages industry as central partners in the design and delivery of skills education. A movement away from the tired education model of ‘credentialing’, and a reorientation towards in-demand skill acquisition. Governments and International Institutions are central to helping facilitate these solutions.</span>

<span style="font-weight: 400;">A core focus needs to be placed upon the development of greater technical skills. This will enable people to build, manage, and interact with intelligent machines in the workforce. Equally important are the social and interpersonal skills that are the basis of human cooperation and the ability to navigate quickly changing labour markets. These are the broad skillsets that are in demand now and in the future. More specifically, Governments and Educational Institutions would do well to emphasise and develop the following three main skill areas:</span>
<ul>
 	<li style="list-style-type: none;">
<ul>
 	<li style="font-weight: 400;"><b>Software Development</b><span style="font-weight: 400;">, by equipping people with the skills to build and manage software applications;</span></li>
 	<li style="font-weight: 400;"><b>Design Engineering</b><span style="font-weight: 400;">, by learning to design, improve, and build objects through disciplines such as Robotics, 3D printing, and Mechatronics;</span></li>
 	<li style="font-weight: 400;"><b>Work readiness<span style="font-weight: 400;">, by training and mentoring people to navigate modern labour markets, pursue in-demand education, or start their own businesses.</span></b></li>
</ul>
</li>
</ul>
This alone is not the answer, but building creative responses around these skill development areas should be part of the solution. Building an inclusive future takes work. To leverage intelligent machines that harness the full power of human creativity requires careful planning, technical skills, and a deliberate view of the future.
<h5>THIS IS A BIG DEAL</h5>
<span style="font-weight: 400;">As intelligent technologies intersect with more parts of our lives, it’s changing how we live and how we work, regardless of geography. It’s shifting the primary sources of value from material assets to knowledge. This transition is most obviously seen in the stock market. </span><a href="https://en.wikipedia.org/wiki/List_of_public_corporations_by_market_capitalization#1997"><span style="font-weight: 400;">In</span><span style="font-weight: 400;"> 1997</span></a><span style="font-weight: 400;">, technology companies made up 2 out of the top 10 companies by market capitalisation; fast forward 20 years, it’s jumped to 5 out of 10. </span>

<img class="aligncenter wp-image-83" src="https://bitsandatoms.co/wp-content/uploads/2017/08/largest-companies-by-market-cap-chart.jpg" alt="" width="425" height="438" />
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">Source: Visual Capitalist, August 2016</span></p>
<span style="font-weight: 400;">This is significant because economic value is an indication of social authority. It represents the growing role that technology plays in our lives. And it highlights the greater authority we grant technology to inform our social dynamics.</span>

<span style="font-weight: 400;">When we need to travel from one place to another, we seldom ask a friend or close passerby for detailed directions. Instead, we pull out our smart phones and ask Google Maps. For countless routine tasks, we’ve elevated intelligent machines as the higher source of authority over people. Google knows best.</span>
<blockquote><span style="font-weight: 400;">The ability of intelligent machines to complete cognitive tasks is revolutionary. Cognitive tasks have historically been reserved to human labour; now they’re routinely performed by machines at breakneck speeds.</span></blockquote>
<span style="font-weight: 400;">Amazon’s book recommendations is a familiar example of an intelligent machine. While browsing, Amazon recommends books based on information like your purchase history, search history, and user profile, with the goal of selling more books. Your recommendations adapt as new information is provided by you and others. The more information Amazon collects, the more accurate their recommendations become, the higher their book sales rise. The wise local bookstore owner doesn’t stand a chance.</span>

<span style="font-weight: 400;">All of this speaks to the higher value we’re placing on knowledge. We see this in the growing market value of technology firms, the rapid wage growth of software developers,[note]Burning Glass Technologies (2016), <i>Beyond Point and Click: The expanding demand for coding skills</i> pg. 3.[/note]</span><span style="font-weight: 400;"> and the average 10 hours per day[note]Ernst &amp; Young (2016), <i>Digital Australia: State of the Nation 2015-16</i> pg. 13.[/note]</span><span style="font-weight: 400;"> we spend on internet connected devices. All of these indicators show that we’ve entered the ‘Information Age’. And information is transferred through applied technologies that feeds ‘knowledge’ in the knowledge economy.</span>
<h5>THE IMPORTANCES OF CONTRIBUTION</h5>
<span style="font-weight: 400;">Equipping people with technical skills is important for two main reasons:</span>
<ul>
 	<li><b>Personal contributions<span style="font-weight: 400;"> - This applies to both economic and social contributions. Acquiring the technical skills of a software developer places an individual in high demand. They’re able to attract a higher salary, which can help improve the well-being of themselves and their family. Less measurable, but no less significant, are the well-being and identity benefits that arise from contributing to important work. This isn’t to say that all important work is reserved to people with technical skills. They’re not. But as technology is increasingly applied to help solve our most pressing problems, like climate change, healthcare, and education, a strong technical skillset is often required to be part of these solutions. This requirement for technical skills to help solve hard problems will continue to increase.</span></b></li>
 	<li><strong>Collective contributions</strong><span style="font-weight: 400;"> - The degree of change instigated by technology should not be dismissed by the magnitude of what’s left to do. While Twitter may feel like a passing fad, it’s still global instant communication, for</span><span style="font-weight: 400;"> free</span><span style="font-weight: 400;">. That’s a significant social advancement. What’s even more significant is how small the group of people behind these changes are. They’re a tiny proportion of the world’s population, with highly technical skills. So, imagine what we could collectively achieve if we broadened the talent pool, even just a little. A rising tide lifts all boats.</span></li>
</ul>
<span style="font-weight: 400;">The point is, enabling people to meaningfully contribute in a future with more intelligent technologies is good for the individual and the collective.</span>
<h5>THE TIME IS RIPE</h5>
<span style="font-weight: 400;">All signs point to a future where intelligent technologies intersect with more parts of our lives. This greater role will largely be made possible by progress in Artificial Intelligence (AI).[note]Artificial Intelligence is the broad discipline of non-organic intelligent technologies, which includes subset disciplines such as Machine Learning, Natural Language Processing, and Computer Vision.[/note]</span><span style="font-weight: 400;"> We’ve seen false starts and similarly large claims</span> <span style="font-weight: 400;">about AI before,</span><span style="font-weight: 400;">[note]Chen, Frank (2016), <i>AI, Deep Learning, and Machine Learning: A Primer </i>[Video], Andreessen Horowitz.[/note]</span><span style="font-weight: 400;"> but we’re on the precipice of something big. </span>

<span style="font-weight: 400;">The breakneck advancements in AI are being brought about through a confluence of developments. The driving factors are:</span>
<ul>
 	<li><b>Greater computational power</b></li>
 	<li style="list-style-type: none;">
<ul>
 	<li style="list-style-type: none;">
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Moore’s Law has largely held constant since 1971, with CPU transistor counts doubling every two years. The same applies for quality adjusted microprocessor prices, memory capacity, sensors, and pixels in digital cameras.[note]The Economist (2016) <i>After Moore’s Law: Technology Quarterly</i> June Quarterly Edition.[/note]</span></li>
</ul>
</li>
</ul>
</li>
 	<li><b>Increased quantities and access to training data</b></li>
 	<li style="list-style-type: none;">
<ul>
 	<li style="list-style-type: none;">
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The proliferation of devices and applications have exploded, which has caused a 50% compound annual growth rate (CAGR) from 2010.[note]Purdy, Mark; &amp; Daugherty, Paul (2016) <i>Why Artificial Intelligence is the Future of Growth</i> Accenture publications pg. 11.[/note]</span></li>
</ul>
</li>
</ul>
</li>
 	<li><b>Better algorithms</b></li>
 	<li style="list-style-type: none;">
<ul>
 	<li style="list-style-type: none;">
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The processes and techniques that construct algorithms have become more sophisticated. Algorithm methods such as convolutional, feedforward, and adversarial networks are improving large-scale data processing.</span></li>
</ul>
</li>
</ul>
</li>
 	<li><b>Broad investment</b></li>
 	<li style="list-style-type: none;">
<ul>
 	<li style="list-style-type: none;">
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Growth in private AI investment has increased by 6X between 2011 to the beginning of 2016 alone.[note]CB Insights (2016) <i>Artificial Intelligence Explodes: New Deal Activity Record for AI Startups</i> [Blog][/note]</span></li>
</ul>
</li>
</ul>
</li>
 	<li><b>Open source frameworks and libraries</b></li>
 	<li style="list-style-type: none;">
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The open sourcing of AI tools such as Google’s Tensor Flow, Facebook’s Torch, and Amazon’s DSSTNE have made it more accessible than ever for companies and individuals to apply their data to some of the most powerful AI technologies.</span></li>
</ul>
</li>
</ul>
<img class="aligncenter wp-image-85" src="https://bitsandatoms.co/wp-content/uploads/2017/08/AlphaGo-1024x683.jpg" alt="" width="586" height="391" />
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">AlphaGo plays Lee Sedol in 2016</span></p>
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">Source: New Scientist, 4 January 2017</span></p>
<span style="font-weight: 400;">These advancements are driving the progress of intelligent machines. As they continue to develop, they will increasingly intersect with more parts of our lives, including work.</span>
<h5>1984 OR UTOPIA?</h5>
<span style="font-weight: 400;">Too often, the impact of AI on the future of work is debated from two extreme ends of the spectrum. On one side, the Dystopians anticipate humanity’s obsolescence; on the other, the Utopians predict Heaven on Earth. </span>
<blockquote><span style="font-weight: 400;">Both make headlines. Both Brave New Worlds with more AI. Neither are particularly helpful.</span></blockquote>
<span style="font-weight: 400;">While these predictions are still possibilities that shouldn’t be ruled out, they offer little explanation with how we get there. More helpful is a view of history, rooted in the present. </span>

<span style="font-weight: 400;">Let’s consider the effects of the Industrial Revolution on employment. The labour market has historically been split into three main sectors: agriculture, industry, and services. Until around 1800, the majority of Americans were employed in the agricultural sector. Just one century later, the employment rate in agriculture had halved. Significant portions of agricultural labour were automated, people moved to urban centres for industry-based jobs, and as education improved, a growing populace took up service professions. By 1900, the three sectors were almost evenly distributed.[note]Gallman, Robert E.; &amp; Weiss, Thomas J. (1969) "The Service Industries in the Nineteenth Century." In Production and Productivity in the Service Industries, ed. Victor R. Fuchs, 287-352. New York: Columbia University Press.[/note]</span><span style="font-weight: 400;"> This was a seismic shift from the fields and herds, to the cities and factories.</span>

<span style="font-weight: 400;">And the pace of change continued to accelerate. By 2010, only 2 percent of Americans were employed in agriculture, 20 percent in industry, and the 78 percent majority assumed service-based employment.[note]1950–2010: <a href="http://www.bea.gov/">Bureau of Economic Analysis</a>, National Income and Product Accounts.[/note]</span>

<img class="aligncenter wp-image-86" src="https://bitsandatoms.co/wp-content/uploads/2017/08/Distribution-of-labour-force-by-sector-1024x633.png" alt="" width="656" height="405" />
<p style="text-align: center;">Source: Lewis Johnston, MinnPost[note]<span style="font-weight: 400;">Johnston, Lewis (2012) </span><i><span style="font-weight: 400;">History lessons: Understanding the decline in manufacturing</span></i><span style="font-weight: 400;"> [Blog], MinnPost.[/note]</span></p>
<span style="font-weight: 400;">So, how do the changes of the Industrial Revolution help inform the transition we’re experiencing with the Information Revolution?</span>

<span style="font-weight: 400;">There are similarities and differences. The similarities revolve around the human response and public perception to these changes; the differences concern the fundamental technologies driving these changes and their effects.</span>
<h5>THE PROMETHEAN MYTH</h5>
<span style="font-weight: 400;">The awakening of the Industrial Revolution evoked fears of mass-unemployment through mechanical automation. People worried about the obsolescence of humanity and the Luddites screamed about the unravelling of social order. </span>

<span style="font-weight: 400;">This response has been a constant throughout history. It goes as far back to Greek Mythology with the ‘</span><a href="http://www.theoi.com/Titan/TitanPrometheus.html"><span style="font-weight: 400;">Promethean Myth</span></a><span style="font-weight: 400;">’: Man acquires fire from the Gods; fire becomes the source of Man’s pain and suffering. We hear variations of the same narrative today: humans build and deploy robots; robots take our jobs and our purpose. </span>
<blockquote><span style="font-weight: 400;">Yet, the fears of the Promethean Myth have been consistently unfounded. To take the position that ‘this time is different’ is a complete abnegation of economic history. </span></blockquote>
<span style="font-weight: 400;">Of course new jobs will be created. Ideas, industries, and fields will be conceived that we haven’t even imagined. The economic process of ‘Creative Destruction’ will upend markets, replacing jobs from the ‘old’ economy and ushering in the ‘new’. </span>

<span style="font-weight: 400;">A 2011 study by </span><a href="http://owni.fr/files/2011/03/internet_impact_rapport_mcKinseycompany.pdf"><span style="font-weight: 400;">McKinsey</span></a><span style="font-weight: 400;"> found that the Internet had destroyed 500,000 jobs in France in the previous 15 years. However, over the same period, the Internet had created 1.2 million jobs. That’s a net employment addition of 700,000 and a rate of 2.4 jobs created for every job destroyed.[note]McKinsey Global Institute (2017) <i>Technology, Jobs, and the Future of Work</i>.[/note]</span>

<span style="font-weight: 400;">This has been a constant occurrence across generations. We’ve seen technology over the past 144 years create more jobs than it’s destroyed.[note]Deloitte (2015) <i>Technology and People: The great job-creating machine</i>.[/note]</span><span style="font-weight: 400;"> And it continues to do so, with the net rate of job creation increasing over the past two decades.[note]World Economic Forum (2016) <i>The Future of Jobs Report</i>.[/note]</span><span style="font-weight: 400;">  </span>

<span style="font-weight: 400;">This isn’t wishful thinking or wilful ignorance; it’s pragmatic reasoning centred around the history and potential of human creativity. </span>

<span style="font-weight: 400;">So what’s all the fuss about?</span>
<h5>IT'S ALL ABOUT SKILLS</h5>
<span style="font-weight: 400;">If we project that the trend of ‘job replacement’ by new technologies will continue, then the next step is to consider what </span><span style="font-weight: 400;">types of jobs</span><span style="font-weight: 400;"> will be demanded.</span>

<span style="font-weight: 400;">This is where we begin to approach unfamiliar territory.</span>

<span style="font-weight: 400;">The transition from the Agricultural Revolution to the Industrial Revolution saw entire populations move from farmlands to factories. This transition, however, was still predominantly a movement between low-skilled vocations.[note]Harari, Yuval N. (2016) <i>Homo Deus: A Brief History of Tomorrow</i> Chapter 9: ‘The Great Decoupling’.[/note]</span><span style="font-weight: 400;"> People shifted from the physical labour of toiling the earth to joining the assembly line of a steel mill. Very different work, but still low-skilled jobs to low-skilled jobs.</span>

<span style="font-weight: 400;">The fundamental difference with the transition to the Information Age is the greater demand for high-skilled labour. This demand is driven by the growing proliferation of intelligent machines. And the ability to interact with these intelligent machines requires strong technical skills. </span>

<span style="font-weight: 400;">The core problem is that people aren’t developing these skills at a sufficient pace.</span>

<span style="font-weight: 400;">More jobs today require higher levels of preparation and this rate is increasing. According to the US Department of Labor, American workers requiring above-average skills increased by 68% from 1980 to 2015. This is more than double the demand for workers with below-average skills, which only increased by 31% over the same period.[note]Pew Research Center (2016), <a href="http://www.pewsocialtrends.org/2016/10/06/1-changes-in-the-american-workplace/"><i>The State of American Jobs</i></a><i>: The changing demand for job skills and preparation</i>.[/note]</span>

<img class="aligncenter wp-image-87" src="https://bitsandatoms.co/wp-content/uploads/2017/08/Employment-growth-by-skill-level-1024x633.png" alt="" width="644" height="398" />
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">Note: based on employed US citizens aged 16 years and older. </span></p>
<p style="text-align: center;"><span style="color: #999999;"><span style="font-weight: 400;">Source: Pew Research Center[note]Ibid[/note]</span></span></p>
<span style="font-weight: 400;">This growth in demand for workers with higher levels of preparation is a function of the higher levels of skillsets demanded for these jobs. The modern economy is disproportionately demanding and valuing advanced cognitive skills.</span>

<span style="font-weight: 400;">Occupations requiring higher levels of social or analytical skills grew significantly between 1980 to 2015. Jobs with above-average social skills, like communications and people management, grew by 83%. Similarly, employment in jobs that require above-average analytical skills, such as non-routine cognitive and highly technical tasks, increased by 77%. This dwarfs the employment growth of labour with higher levels of physical skills, which only grew by 18% over the same period.[note]Ibid[/note]</span>

<img class="aligncenter wp-image-88" src="https://bitsandatoms.co/wp-content/uploads/2017/08/Stronger-employment-growth-for-higher-skills-1024x596.png" alt="" width="641" height="373" />
<p style="text-align: center;"><span style="font-weight: 400; color: #999999;">Note: based on employed US citizens aged 16 years and older.</span></p>
<p style="text-align: center;"><span style="color: #999999;">Source: Pew Research Center[note]Ibid[/note]</span></p>
<span style="font-weight: 400;">This growing demand for higher levels of preparation and skills is indicative of the transition to the knowledge economy. As physical labour is increasingly automated, the demand for cognitive labour grows, particularly non-routine cognitive labour. </span>

<span style="font-weight: 400;">Physical labour and routine manufacturing are being automated because it’s now more efficient for machines to complete these tasks. This is not the case for social and analytical skills aforementioned. It’s extremely difficult for intelligent machines to complete abstract social tasks and non-routine cognitive exercises. Machines can’t yet deal with the social idiosyncrasies of people management, or the analytical rigours of strategy development - at least as effectively as humans. There are certainly intelligent machines that supplement these areas. And it’s crucial for people in these roles to be able to interact with these intelligent machines. However, these machines are applied to what they do best: computation and data processing.</span>

<span style="font-weight: 400;">Therefore, the concern shouldn’t be that there won’t be jobs created on the foundations of AI; the concern should be that we won’t have enough people with the skills to do these jobs.</span>
<h5>SKILL SHORTAGES AND THEIR DISCONTENTS</h5>
<span style="font-weight: 400;">The implications of skill shortages result in unfavourable economic and social outcomes. The mismatch between the growth rates of rising high-skilled labour demand and sluggish high-skilled labour supply impact us all. It affects us economically, socially, and morally.</span>

<span style="font-weight: 400;">If only a small and shrinking proportion of the world’s population can fulfil these high-skilled jobs, it places downward pressure on everyone else. More people enter the pools of lower skilled work and wage rates decrease as more people slide down the skill curve. Meanwhile, wages in higher-skilled labour disproportionately rise. </span>

<span style="font-weight: 400;">This growing gap between income advancement and employment opportunities has widened over the past few decades. For instance, US college graduates in 1981 earned a wage premium of 48% over high school graduates. By 2005, the wage premium had risen to 97% - college graduates earn almost double that of high school graduates.[note]Autor, David (2014) <i>Skills, education, and the rise of earnings inequality among the ‘other 99 percent</i>, Science, Volume 344, Issue 6186, May 2014. See also: McKinsey Global Institute (2016), <i>Poorer than their parents? Flat or Falling Incomes in Advanced Economies</i>.[/note]</span><span style="font-weight: 400;"> This growing income inequality is reflected across the world, and felt most acutely in the Global South.</span>

<span style="font-weight: 400;">Disparities in wealth and earnings potential have been a constant in capitalist economies. However, automation and the subsequent rising demand for high skilled labour could accelerate this inequality. The White House noted in 2016 that the effects of automation through AI on labour markets will likely grow over the forthcoming decade.[note]Executive Office of the President (2016), <i>Artificial intelligence, automation, and the economy</i>.[/note]</span>

<span style="font-weight: 400;">This means that more jobs, or parts of jobs, will experience automation by intelligent machines. McKinsey Global Institute researched automation potential by examining over 2,000 work activities across 46 countries, which represents around 80% of the global workforce. Their research found that the proportion of occupations that could be </span><i><span style="font-weight: 400;">fully</span></i><span style="font-weight: 400;"> automated by demonstrated technologies is small - less than 5%. The potential for </span><i><span style="font-weight: 400;">partial</span></i><span style="font-weight: 400;"> automation, however, is much higher. Around 60% of all occupations have at least 30% of activities that are technically automatable, based on current technologies.[note]McKinsey Global Institute (2017) <i>Technology, Jobs, and the Future of Work</i>.[/note]</span><span style="font-weight: 400;"> Therefore, the majority of occupations will change, and more people will have to interact with intelligent technologies as part of their work.</span>
<p style="text-align: center;"><strong><span style="color: #808080;">Automation potential based on demonstrated technology of occupation titles in the US (cumulative)</span></strong></p>
<img class="aligncenter wp-image-89" src="https://bitsandatoms.co/wp-content/uploads/2017/08/Automation-potential.png" alt="" width="698" height="413" />
<p style="text-align: center;">Source:<span style="font-weight: 400;"> McKinsey Global Institute[note]McKinsey Global Institute (2017) </span><i><span style="font-weight: 400;">A Future that Works: Automation, Employment, and Productivity</span></i><span style="font-weight: 400;">, Executive Summary pg. 5.[/note]</span></p>

<blockquote><span style="font-weight: 400;">If people fail to sufficiently upskill, this will disproportionately favour the highly skilled. Conversely, it will disfavour the low and medium-skilled workers who are unprepared for the demands of the knowledge economy.</span></blockquote>
<span style="font-weight: 400;">Not only could the labour supply of similarly lower-skilled workers increase, placing downward pressure on wages, but lower skilled and cheaper labour is more likely to come under pressure from automation.</span>

<span style="font-weight: 400;">The Council of Economic Advisers to the White House ranked the probability of automation according to wages. They found that 83% of jobs making less than $20 per hour would come under pressure from automation. This is over twenty times more likely than jobs earning $40 per hour.[note]Furman, Jason (2016), <a href="https://obamawhitehouse.archives.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf"><i>Is This Time Different? The Opportunities and Challenges of Artificial Intelligence</i></a>, Council of Economic Advisers to the White House, New York University, pg. 4-5.[/note]</span>

<img class="aligncenter size-full wp-image-91" src="https://bitsandatoms.co/wp-content/uploads/2017/08/chart.png" alt="" width="446" height="506" />
<p style="text-align: center;"><span style="color: #999999;"><span style="font-weight: 400;">Source: Jason Furman[note]Ibid</span><span style="font-weight: 400;">[/note]</span></span></p>
<span style="font-weight: 400;">Regardless of whether these probabilities are exactly accurate, the magnitude of this variance is immense. Assuming that wages are correlated with skill-levels, these projections show that automation could cause a disproportionately large decline in the demand for less-skilled jobs, and a minimal decline in demand for high-skilled jobs. As a result, wage pressures rise and inequality gaps widen.</span>

<span style="font-weight: 400;">This has been playing out for decades. Wages as a share of GDP in advanced economies has dropped sharply since the 1970s.[note]McKinsey Global Institute (2016), <i>Poorer than their parents? Flat or Falling Incomes in Advanced Economies</i>.[/note]</span><span style="font-weight: 400;"> Yet, wages for the highest income earners over the same period have risen consistently.[note]Saez, Emmanuel  (2015) <i>Striking it Richer: The Evolution of Top Incomes in the United States</i>.[/note]</span><span style="font-weight: 400;"> While it’s wrong to attribute this growing inequality as a pure function of technology, people have not acquired the necessary skills at a sufficient rate to meet these emerging labour demands.</span>

<span style="font-weight: 400;">All of this speaks to a systematic failure to adequately prepare people for the future of work.</span>
<h5>DEVELOPING COUNTRIES WILL BE HARDEST HIT</h5>
<span style="font-weight: 400;">As the majority of low and medium skilled workers live in developing countries, the developing countries are at the greatest risk.</span>

<span style="font-weight: 400;">Cheap labour, improved logistics, and internet connectivity enabled offshoring of manufacturing in advanced economies. This ‘deindustrialisation’ helped lift incomes and living standards in the developing countries that assumed this outsourced labour. It also guided the transition of advanced economies toward more service-based labour (as previously discussed). </span>
<blockquote><span style="font-weight: 400;">Therefore the fundamental concerns for developing economies are the same as advanced economies, they’re just more protracted. The central issue is still the rate of skill acquisition.</span></blockquote>
<span style="font-weight: 400;">At the current rate, the projections look bleak. A World Bank report found that two-thirds of all jobs in the developing world face significant automation.[note]World Bank Group (2016) <a href="http://documents.worldbank.org/curated/en/896971468194972881/pdf/102725-PUB-Replacement-PUBLIC.pdf"><i>Digital Dividends</i></a> pg. 23.[/note]</span><span style="font-weight: 400;"> Interestingly, the majority of these are likely to be middle-skilled, middle-paying occupations (for e.g. clerks, plant and machine operators). Low-skilled jobs are still at significant risk of displacement. However, the more immediate concern for low-skilled workers is where the medium-skilled workers look for their next job. </span>

<span style="font-weight: 400;">If they’re not equipped for the new demands of the knowledge economy, medium-skilled workers move down the skill curve and the low-skilled labour supply increases. This heightens the bargaining power of employers, wages are pressured down, and inequality widens. In some ways, we’re seeing this today with the movement towards the ‘on demand’ economy. Due to abundant supply of low and moderately skilled labour, employers dictate wages and only pay for discrete periods of work. Think: Uber or your favourite food delivery service.</span>

<span style="font-weight: 400;">The effect of growing displacement of medium-skilled labour is referred to as ‘employment polarisation’. This is where labour supply becomes concentrated at either ends of the skill spectrum. The main issue with this are the obstructions to upward social mobility.[note]Santos, Indhira (2016) <a href="http://blogs.worldbank.org/developmenttalk/labor-market-polarization-developing-countries-challenges-ahead">Labor market polarization in developing countries: challenges ahead</a> [Blog], World Bank Group.[/note]</span><span style="font-weight: 400;"> If employment polarisation worsens, there are fewer opportunities for people to climb the skill ladder, as the medium-skilled rung is weakened or transformed.</span>

<span style="font-weight: 400;">This process of turnover, accelerated by automation through intelligent machines, could lead to sustained periods of underemployment or unemployment.[note]Furman, Jason (2016), <a href="https://obamawhitehouse.archives.gov/sites/default/files/page/files/20160707_cea_ai_furman.pdf"><i>Is This Time Different? The Opportunities and Challenges of Artificial Intelligence</i></a>, Council of Economic Advisers to the White House, New York University, pg. 6.[/note]</span><span style="font-weight: 400;"> Not all workers will have the training or skills to fulfil the new jobs created by AI. Developing countries are particularly at risk, given higher numbers of low-skilled and medium skilled workers, fewer training opportunities, and less comprehensive safety nets. This has not historically been a recipe for peace.</span>

<span style="font-weight: 400;">The extent of labour displacement, however, will be determined by people’s abilities to upskill and prepare for the demands of the knowledge economy.</span>
<h5>THE EDUCATION BUBBLE</h5>
<span style="font-weight: 400;">The gulf between what’s happening and what needs to happen is intimidatingly large. Public Policy and Education Institutions have not kept apace with the advancements of AI and other technologies. Consequently, people are unprepared for the demands of the workplace, today and tomorrow.</span>

<span style="font-weight: 400;">In a 2013 study of youth and employers across nine countries, 40% of employers stated ‘lack of skills’ as the primary reason for entry-level vacancies.[note]McKinsey On Society (2013) <i>Education to Employment: Designing a System that Works,</i> pg. 18-21.[/note]</span><span style="font-weight: 400;"> Further to this, 60% of employers said graduates were not adequately prepared for the skill requirements of work. They noted particular gaps in technical, analytical, and communication skills. This reinforces the growing and unmet demand for higher-level skills previously discussed. </span>

<span style="font-weight: 400;">Yet, perceptions of ‘readiness’ vary between Education providers, employers, and youth. In this same research, 72% of Education providers claimed their graduates are adequately prepared. Whereas Employers and Youth stated rates of ‘readiness’ only 42% and 45%, respectively.</span>

<img class="aligncenter wp-image-92" src="https://bitsandatoms.co/wp-content/uploads/2017/08/graduate-readiness-1024x633.png" alt="" width="656" height="405" />
<p style="text-align: center;"><span style="color: #999999;"><span style="font-weight: 400;">Source: McKinsey On Society[note]McKinsey On Society (2013) </span><i><span style="font-weight: 400;">Education to Employment: Designing a System that Works,</span></i><span style="font-weight: 400;"> pg. 18-21.[/note]</span></span></p>
<span style="font-weight: 400;">The disconnect between Education providers and modern workplace requirements conveys the shortcomings of Higher Education. And a lot of it has to do with misdirected incentives.</span>

<span style="font-weight: 400;">Higher Education prioritise ‘credentialing’ over skill acquisition. Their incentives are skewed towards ‘bums on seats’ over teaching relevant skills and achieving learning outcomes. As such, Higher Education has increasingly become a ‘tick the box’ exercise for students, particularly in developed countries. This is apparent through the skyrocketing University fee growth across almost all advanced economies. For instance, US College fees for a four year course have risen almost 20 times since 1971.[note]The College Board (2017) <a href="https://trends.collegeboard.org/college-pricing/figures-tables/tuition-fees-room-and-board-over-time"><i>Tuition and Fees and Room and Board over Time</i></a>.[/note]</span>
<blockquote><span style="font-weight: 400;">The message here is as clear as it is concerning: it doesn’t matter what you learn, so long as you’re here.</span>

<span style="font-weight: 400;">But it does matter. It matters a great deal.</span></blockquote>
<h5>WHAT NEEDS TO BE DONE?</h5>
<span style="font-weight: 400;">There are three key areas that we see as important levers for preparing people for a future with more intelligent machines.</span>
<ol>
 	<li><strong> Education - <i>Adapt school and tertiary education systems to help prepare students for the changing workplace demands.</i></strong></li>
</ol>
<span style="font-weight: 400;">At a school-level, this means placing a greater emphasis on developing STEM skills, creativity, and critical thinking amongst students. These are the foundational skills that help prepare people for the demands of high-skilled labour. Schools should and will use more technology to achieve learning outcomes. This will help teachers to deliver personalised learning to help enhance students’ strengths and target their weakness for further development.</span>

<span style="font-weight: 400;">As teaching quality has the greatest in-school impact on student outcomes,[note]Hattie, John (2013) <i>Teachers Make a Difference. What is the Research Evidence?</i> ACER Research Conference.[/note]</span><span style="font-weight: 400;"> equipping our educators with these technical and abstract skills is critical. So, it’s important to adapt Initial Teacher Preparation to best prepare educators with the knowledge and pedagogies to foster these skills in our students.</span>

<span style="font-weight: 400;">At a higher-education level, there must be a reorientation towards skill acquisition in priority skill areas. Tertiary providers would do well to partner more readily with industry to inform their curriculum and add practical insight into their content. No longer are universities preparing their students for careers in academia. They’ve become the gateway to professional industry, so they should focus on industry preparation.</span>

<span style="font-weight: 400;">Students undergoing any tertiary education should be able to point to a demonstrable set of relevant and applicable skills to future employers. The internet and digital platforms are making skill development and demonstration more accessible than ever before. The opportunities to build scalable training programs for developing countries is immense, as more people come online. </span>

<span style="font-weight: 400;">The problem is that tertiary providers aren’t incentivised to do so. They’re attracting exorbitant fees because society places irrational value on credentials. Perhaps it will be necessary to adjust these incentives to help reorient education systems towards prioritising skills over credentials. Or perhaps it will instigate an influx of private training providers to fill this void.</span>
<ol start="2">
 	<li><strong> Policy - <i>Rethink transition support for affected workers and create favourable environments for entrepreneurs to create digitally enabled jobs.</i></strong></li>
</ol>
<span style="font-weight: 400;">Transition support is more than just Universal Basic Income (UBI). While UBI is a nice ideal, supporters aren’t clear on what constitutes ‘Universal’ or ‘Basic’. ‘Universal’ also presumes international cooperation at a time where nationalism is on the rise.[note]Onder, Harun (2016) <a href="https://www.brookings.edu/blog/future-development/2016/07/18/the-age-factor-and-rising-nationalism/"><i>The age factor and rising nationalism</i></a>, Brookings Institution.[/note]</span><span style="font-weight: 400;"> However, fiscal and education safety nets should be provided to workers that are displaced from employment.</span>

<span style="font-weight: 400;">More immediate, and from my perspective, more important, is how governments and international institutions help support the creation of new jobs, develop necessary skills in their workforces, and successfully match workers to these jobs.</span>

<span style="font-weight: 400;">Firstly, it’s important that governments expand the distribution channels of education and training. This will allow more people to acquire the skills that complement and benefit from innovations. Governments should determine how private enterprise can drive skill development. Considering the clear disconnect between traditional education providers and industry, governments would do well to support training at the source of where skill shortages are most acutely felt: private enterprises. This could mean providing incentives for in-work training programs or supporting partnerships with private training providers. In any case, industry should be incentivised to develop their workers and supported to help inform education programs.</span>

<span style="font-weight: 400;">Secondly, governments should be proactive in supporting innovation in their cities, states, and countries. Replication is not necessarily the formula here. If governments are trying to copy successes like Silicon Valley, they’re not learning from them. Silicon Valley has been successful precisely because they took a different approach and created a unique ecosystem. Sure, reducing regulatory barriers for innovation and creating tax incentives for Angel investment are important strategies that should be adopted. But the core strategy should be focused on building a unique and supportive ecosystem. </span>

<span style="font-weight: 400;">To do this, policymakers should determine the innovation domain that is (or could be) their regional competitive advantage. This could be cryptocurrency, biotech, or even a subcategory like computer vision. Once this has been determined, then the process begins of creating ‘regulatory competitive advantage’, according to that specific domain. This will help attract talent, investment, and potentially spur new domains of innovation in the ecosystem.</span>
<ol start="3">
 	<li><strong> Research - <i>More research in AI needs to occur to ensure that it’s safe and optimal for people.</i></strong></li>
</ol>
<span style="font-weight: 400;">The magnitude of developments in AI should not be understated. It will change the way we work, interact, and live. Indeed, it already is. </span>
<blockquote><span style="font-weight: 400;">So, you would think that being on the precipice of such significant changes, people would want to have as strong a grasp on the forces stirring these changes. The statistics paint a different a story.</span></blockquote>
<span style="font-weight: 400;">While billions are invested[note]"Spending on AI technologies by companies is expected to grow to $47 billion in 2020 from a projected $8 billion in 2016, according to IDC." Norton, Steven (2017) <i>Artificial Intelligence Looms Larger in the Corporate World</i>, The Wall Street Journal. Dow Jones &amp; Company.[/note]</span><span style="font-weight: 400;"> in advancing the powers of AI, it’s estimated that fewer than 100 people in the world are researching ways to make AI safe[note]Farquhar, Seb (2017) <a href="http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/"><i>Changes in funding in the AI safety field</i></a>. The estimate of 'under 100' is based on an informal count of people doing directly relevant work at the organisations in this article, which is significantly below 100. This was cross-checked by estimating the cost per full time staff member: the forecast spending of $9m in 2017 would not be enough to sustain more than 100 staff members given the high cost of hiring machine learning researchers. Note that this figure could be inaccurate if there is a large and non-public AI safety project.[/note]</span><span style="font-weight: 400;"> and optimal for humanity. Organisations like </span><a href="https://openai.com/"><span style="font-weight: 400;">OpenAI</span></a><span style="font-weight: 400;"> and </span><a href="http://www.nickbostrom.com/"><span style="font-weight: 400;">Professor Nick Bostrom</span></a><span style="font-weight: 400;"> from University of Oxford are the leading the way.</span>

<span style="font-weight: 400;">Most of the concerns for AI surround the ‘unintended consequences’. When programmed correctly, intelligent machines do </span><i><span style="font-weight: 400;">exactly</span></i><span style="font-weight: 400;"> what you tell them, often faster, more accurately, and cheaper than any human. The smarter the system becomes, the harder it is for a person to exercise meaningful oversight. While positive opportunities abound, the negative consequences should not be dismissed.</span>

<span style="font-weight: 400;">Take financial credit and Machine Learning systems as an example. Imagine you go to the bank for loan. You’re met by a bank teller (human or otherwise!) and he requests access to your financial information. You feed him the standard details required and it’s entered into the system. The intelligent machine then looks through ALL your available data; where you live, where you work, who’s in your social network, the energy bill you forgot to pay four years ago. All of it. You awkwardly wait with the overdressed bank teller and the system runs its analysis. PING!</span>

<span style="font-weight: 400;">‘Credit Denied’.</span>

<span style="font-weight: 400;">‘What? How come?’ You cry.</span>

<span style="font-weight: 400;">The bank teller shrugs. ‘I dunno. The system said so.’</span>

<span style="font-weight: 400;">The problem with this scenario is that it presumes that the intelligent machine considers all the information, explicit and hidden. But what if you come from a poor area, you’re classified in a racial minority group, and you’ve had limited schooling opportunities? You might be an honest, hardworking, and intelligent person, but the system places unjust weightings on your inherited circumstances.</span>

<span style="font-weight: 400;">So, if intelligent machines do not account for the social biases and legacy injustices that weigh on our societies today, then we’re at risk of further perpetuating them tomorrow. </span>

<span style="font-weight: 400;">We still haven’t developed adequate solutions to a lot of social biases without machines. So blindly extending authority to intelligent machines to make these abstract decisions could be problematic.</span>

<span style="font-weight: 400;">This is not an argument to reduce progress in AI. It’s an argument to dramatically increase research in AI.</span>
<h5>PROGRESS</h5>
<span style="font-weight: 400;">While we face very real challenges with the development of intelligent machines, the application of these technologies are still our best hope. </span>

<span style="font-weight: 400;">Let me be clear, slowing technological progress in the name of ‘saving jobs’ will punish consumers and stall advancements to quality of life. Applied technologies have cured diseases, reduced famines, and helped lift entire populations out of poverty. But these technological solutions have been successful because of their </span><i><span style="font-weight: 400;">application</span></i><span style="font-weight: 400;">, not because of their technological capabilities. </span>

<span style="font-weight: 400;">This is an important distinction because it shows that technologies are never deterministic. The utopian and dystopian arguments both assume an inevitable worldview. A determinism that the arc of technological progress bends on its own.</span>

<span style="font-weight: 400;">But this reasoning is just lazy. Technology has never been deterministic. Positive outcomes depend on the applied efforts and cooperation of smart people. Entropy is not on our side.</span>

<span style="font-weight: 400;">The point is not that technology is harmful; the point is that the progress of technology does not always align neatly to the march of humanity. </span>

<span style="font-weight: 400;">So, the progress of humanity is neither guaranteed nor hopeless. Instead, it’s up to us.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>55</wp:post_id>
		<wp:post_date><![CDATA[2017-08-17 21:40:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-17 21:40:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[the-perils-of-progress]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="future-of-work"><![CDATA[Future of Work]]></category>
		<category domain="post_tag" nicename="labour-markets"><![CDATA[Labour Markets]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_0c55dd6777eada5ad00bd041ef55c5e5]]></wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="1165" height="655" src="https://www.youtube.com/embed/ZxnoBfUYy04?feature=oembed" frameborder="0" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_time_0c55dd6777eada5ad00bd041ef55c5e5]]></wp:meta_key>
			<wp:meta_value><![CDATA[1503005209]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[80]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AI Governance - 11 Key Issues</title>
		<link>https://bitsandatoms.co/ai-governance-11-key-issues/</link>
		<pubDate>Fri, 01 Sep 2017 21:10:27 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=136</guid>
		<description></description>
		<content:encoded><![CDATA[<b>TL;DR</b>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">As Artificial Intelligence (AI) is broadly applied to social and economic domains, measured oversight becomes increasingly important.</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">But determining appropriate regulatory frameworks for AI is complex. Eleven key issues include:</span></li>
</ul>
<ol>
 	<li><span style="font-weight: 400;">Defining AI</span></li>
 	<li><span style="font-weight: 400;">Articulating</span><span style="font-weight: 400;"> ethical standards and social norms</span></li>
 	<li>
<p style="display: inline !important;">Accountability when AI causes harm</p>
</li>
 	<li><span style="font-weight: 400;">Appropriate </span><span style="font-weight: 400;">degree of oversight</span></li>
 	<li><span style="font-weight: 400;">Measurement &amp; evaluation </span><span style="font-weight: 400;">of the impact</span></li>
 	<li>
<p style="display: inline !important;">The control problem</p>
</li>
 	<li>
<p style="display: inline !important;"><span style="font-weight: 400;">Openness</span></p>
</li>
 	<li>
<p style="display: inline !important;"><span style="font-weight: 400;">Privacy &amp; security</span></p>
</li>
 	<li>
<p style="display: inline !important;"><span style="font-weight: 400;">Projections</span></p>
</li>
 	<li>
<p style="display: inline !important;"><span style="font-weight: 400;">Assessing institutional competence</span></p>
</li>
 	<li>
<p style="display: inline !important;"><span style="font-weight: 400;">The political problem</span></p>
</li>
</ol>
<ul>
 	<li>In the absence of robust policies, <a href="http://www.lawandai.com/about/">Matt Scherer</a> has proposed a voluntary AI certification system. AI-certified programs would be granted limited liability privileges and would provide incentives to meet safety standards. The certification standards would be established and monitored by an independent government agency.</li>
</ul>
<strong>AI Governance</strong>

<span style="font-weight: 400;">Knowingly or unknowingly, Artificial Intelligence systems are intersecting with more parts of our lives. And not just in areas of trivial importance. AI systems are being applied to essential areas of society. From analysisng Electronic Health Records that improve diagnosis rates; to balancing power supply for energy grids. AI can help us achieve more and raise standards of living.</span>

<span style="font-weight: 400;">So, as AI systems are deployed at scale within fundamental societal structures, then measured oversight at scale becomes necessary.</span>

<span style="font-weight: 400;">Such public interest roles are typically assumed by the arms of national governments. But AI public policy has been met with almost radio silence across the world. As a result, the development and applications of AI continue to exist in a policy vacuum. </span>

<b>Issues with Governing AI</b>

<span style="font-weight: 400;">The unique challenges and complexities of AI do not fit neatly into existing governance frameworks. Safety standards are fluid. Accountability is opaque. And policy-makers lack expertise. The amount of investment in developing AI has exceeded investments in making AI safe by an order of magnitude[note]Farquhar, Seb (2017) <a href="http://effective-altruism.com/ea/16s/changes_in_funding_in_the_ai_safety_field/"><i>Changes in funding in the AI safety field</i></a>.[/note].</span><span style="font-weight: 400;"> This is fueling the immense growth in AI applications with almost unfettered regulatory oversight. The surprising thing is that many of the most prominent Tech leaders, such as Elon Musk[note]Kurt Wagner, <a href="https://www.recode.net/2017/7/15/15976744/elon-musk-artificial-intelligence-regulations-ai"><i>Elon Musk just told a group of America’s governors that we need to regulate AI before it’s too late</i></a>, Recode (July. 15, 2017).[/note]</span><span style="font-weight: 400;"> and Bill Gates[note]<em>For example</em>, see Eric Mack, <a href="http://www.forbes.com/sites/ericmack/2015/01/28/bill-gates-also-%20worries-artificial-intelligence-is-a-threat/"><i>Bill Gates Says You Should Worry About Artificial Intelligence</i></a>, FORBES (Jan. 28, 2015)[/note]</span><span style="font-weight: 400;">, think that a degree of regulatory oversight is important. </span>

<span style="font-weight: 400;">Meanwhile, policy-makers sit idle. AI is viewed as a black box. Most are unclear about what AI actually is, let alone instituting appropriate governance.</span>

<span style="font-weight: 400;">So, what are the main issues with governing AI?</span>

<span style="font-weight: 400;">Like with all major public policy areas, the issues extend beyond just hard technical problems. There are conceptual issues, as well as practical problems.</span>

<span style="text-decoration: underline;"><span style="font-weight: 400;">Conceptual Policy Issues</span></span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Defining AI</span><span style="font-weight: 400;"> - </span></i><span style="font-weight: 400;">The problems with defining AI for regulatory purposes </span><span style="font-weight: 400;">centre around the conceptual ambiguities of ‘intelligence’. The definitions of intelligence vary widely. Intellectual characteristics like ‘consciousness’ and ‘the ability to learn’ are at best nebulous. So, arriving at an agreed definition for Artificial Intelligence is difficult. The subjective nature of AI terminology means that it becomes a moving target for policy-makers. Definitions of AI range from: ‘the ability to act ‘humanly’’[note]A. M. Turing (1950) Computing Machinery and Intelligence. Mind 49: 433-460.[/note];</span><span style="font-weight: 400;"> to ‘performing intellectual tasks’[note]<em>See</em> BRUCE PANDOLFINI, KASPAROV AND DEEP BLUE: THE HISTORIC CHESS MATCH BETWEEN MAN AND MACHINE 7–8 (1997).[/note];</span><span style="font-weight: 400;"><span style="font-weight: 400;"> and the modern definition of ‘acting rationally to achieve goals’[note]</span></span>'Intelligent Machines' or 'Artificial Intelligence' refers to a non-organic autonomous entities that are able to sense and act upon an environment to achieve specific goals. Intelligent agents may also learn or use knowledge to achieve these goals, which are governed by algorithms that are made by people. Russell, Stuart J.; Norvig, Peter (2003), <i>Artificial Intelligence: A </i>Modern<i> Approach </i>(2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-790395-2, chpt. 2.; <i>See also </i>Stephen M. Omohundro, The Basic AI Drives, in ARTIFICIAL GENERAL INTELLIGENCE 2008 483, 483 (2008) (defining AI as a system that “has goals which it tries to accomplish by acting in the world”).[/note]. However, even a ‘goal-oriented’ approach doesn’t provide clarity for a regulatory definition.</li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">Ethical Standards &amp; Social Norms</span></em><span style="font-weight: 400;"> - For autonomous systems to operate effectively in society, they need to so ethically and in alignment with social norms. But what is good behaviour? What is just? Any attempt to develop AI governance structures will inevitably confront such philosophical questions. </span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">Accountability</span></em><span style="font-weight: 400;"> - Assigning liability for when autonomous systems negatively perform is a difficult conceptual and practical challenge. This will be particularly important in social and economic domains. For instance, to what degree can a physician rely on intelligent diagnosis systems without increasing exposure to malpractice claims in the case of a systems error? Precedent in case law is sparse. And the applications of AI systems are rapidly expanding in the absence of </span><span style="font-weight: 400;">ex-ante </span><span style="font-weight: 400;">accountability frameworks.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">The Degree of Oversight</span></em><span style="font-weight: 400;"> - The extent of regulation is always a delicate balance. Ultimately, an ideal AI governance structure would help maximise the opportunities for positive outcomes, while minimising the negative risks. The advantage is that AI development is still in its infancy. However, a failure to institute appropriate oversight could yield unfavourable outcomes. If regulations go too far, innovation could be inhibited and societal benefits lost. If regulations don’t go far enough, negative outcomes at scale could result and knee-jerk policy reactions ensue. We’ve seen this before in other sectors, such as Bioengineering and Biomedicine. For instance, the impact that Thalidomide had on tightening FDA regulations on drug classifications in the US.[note]Bren L (2001-02-28). "Frances Oldham Kelsey: FDA Medical Reviewer Leaves Her Mark on History". <i>FDA Consumer</i>. U.S. Food and Drug Administration.[/note]</span></li>
</ul>
<span style="text-decoration: underline;"><span style="font-weight: 400;">Practical Policy Issues</span></span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Measurement &amp; Evaluation</span></i><span style="font-weight: 400;"> - While technical progress is being made in the emerging field of AI Safety, we currently lack agreed upon methods to assess the social and economic impacts of AI systems. Robust M&amp;E methods are important as they support investigative, regulatory, and enforcement functions. They help set benchmarks, so we can know AI applications are producing positive outcomes.</span></li>
 	<li><em><span style="font-weight: 400;">The Control Problem</span></em><span style="font-weight: 400;"> - The risks associated with control of autonomous systems is a core problem across all segments of AI. In the case of autonomous Machine Learning systems, there are risks that as they continue to learn and adapt, the potential for human control is inhibited. Once control is lost, it may be difficult to regain control. From a policy perspective, there are obvious public risks. So, if the potential for such scenarios is in any way more than theoretical, then the assurance of human control and public alignment will be necessary.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">Openness </span></em><span style="font-weight: 400;">- Determining the desirability of openness in </span><span style="font-weight: 400;">AI research &amp; development is a key issue for policy-makers (including openness about source code, science, data, safety techniques, capabilities, and goals). Types and degrees of</span><span style="font-weight: 400;"> openness result in complex societal tradeoffs, particularly in the long-term. While higher levels of openness will likely accelerate AI development, it may also exacerbate a racing dynamic: a situation where competitors race to develop the first General Artificial Intelligence. Such a dynamic may result in inadequate safety measures in order to accelerate progress.[note]Bostrom, Nick (2017) “Strategic Implications of Openness in AI Development.” Global Policy 8 (2): 135–48.[/note]</span><span style="font-weight: 400;"> This scenario increases the public exposure to systemic risks. It’s important to note that technology and policy decisions are never deterministic. We can’t know for certain that any scenarios will come to pass. It’s plausible, however, that the lever of Openness will have significant second, third, and fourth-order effects. Therefore, it’s an important policy consideration.</span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Privacy &amp; Security</span></i><span style="font-weight: 400;"> - Data is the life source of AI systems. Maintaining standards that uphold privacy and ensure the security of the data accessed by AI systems is a key technical and policy challenge. People should have the right to access, manage, and control the data they generate, given AI systems’ power to analyse and utilise the data. Moreover, it’s imperative that personal data is securely stored and not unscrupulously accessed or used without expressed consent.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">Projections</span></em><span style="font-weight: 400;"> - The decision-making processes of AI systems are diametrically different to those of humans. That’s why AI systems generate solutions that humans never considered.[note]<i>For e.g. see:</i> Cade Metz, <a href="https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/"><i>IN TWO MOVES, ALPHAGO AND LEE SEDOL REDEFINED THE FUTURE</i></a>. WIRED (16. March, 2017).[/note]</span><span style="font-weight: 400;"> This ability to create value through unexpected solutions is a fundamental point of attraction towards AI systems. It’s also a risk. Accurately projecting adverse effects from AI systems is difficult, precisely because outcomes can be unexpected. As AI increasingly enters into social and economic domains, policy-makers will seek reassurance from projections as part of due diligence. But there aren’t clear projection methods.</span></li>
 	<li style="font-weight: 400;"><em><span style="font-weight: 400;">Assessing Institutional Competence</span></em><span style="font-weight: 400;"> - Even if it were decided that regulatory oversight should be instituted for broad-scope AI, governance structures still need to be determined. There are notable issues at hand: legislators lack expertise; courts can’t act quickly enough on a case-by-case basis to establish precedent; and international institutions can be perceived as toothless tigers. While challenging, there are lessons in history of effective governance structures to oversee powerful technologies. The Treaty on the Nonproliferation of Nuclear Weapons offers relevant insight. While still an underserved research area, Matt Scherer proposes a useful regulatory framework for AI, which is summarised below.</span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">The Political Problem</span></i><span style="font-weight: 400;"> - The current and potential powers of AI are not deterministic. They depend on their applications, which are currently decisions made by humans. Like with any source of power, there’s potential for good and subversion. The political challenge with AI is to achieve a situation in which individuals or institutions empowered by such AI use it in ways that promote the common good. At a time where nationalism is on the rise,[note]Onder, Harun (2016) <i>The age factor and rising nationalism</i>, Brookings Institution.[/note]</span><span style="font-weight: 400;"> international cooperation is becoming increasingly difficult. Political cooperation, however, is necessary to the safe broad-scale deployment of AI, which transcends national borders.</span></li>
</ul>
<span style="font-weight: 400;">These issues, taken together, highlight the complexities of establishing appropriate AI policies. National governments are still in the early days of their thinking. Last year, the US government held a series of public workshops with industry and research leaders. This resulted in a summary report presented to The White House.[note]US National Science &amp; Technology Committee on Technology (2016) “Preparing for the Future of Artificial Intelligence.” Executive Office of the President.[/note]</span><span style="font-weight: 400;"> Similarly, the UK House of Commons commissioned an inquiry into the opportunities and implications of Robotics and Artificial Intelligence.[note]UK House of Commons (2016) “<a href="https://www.publications.parliament.uk/pa/cm201617/cmselect/cmsctech/145/145.pdf">Robotics and Artificial Intelligence - United Kingdom Parliament.</a>” n.d.[/note]</span><span style="font-weight: 400;"> While the intent is positive, policy positions are still abstract. This demonstrates the elementary understanding of how broad-scale AI might impact society. Let alone the potential roles of public policy.</span>

<b>A Proposed AI Regulatory Framework</b>

<span style="font-weight: 400;">In the absence of robust policies, </span><a href="http://www.lawandai.com/about/"><span style="font-weight: 400;">Matt Scherer</span></a><span style="font-weight: 400;">, an attorney and legal scholar from the US, has presented a useful proposal to regulate AI systems.[note]Scherer, Matthew U. (2016) “Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies,” <i>Harvard Journal of Law and Technology</i> 29 (2): 354-400.[/note]</span><span style="font-weight: 400;"> The centrepiece of this tort-based framework involves an AI certification process. Certification would require designers, manufacturers, and sellers of AI systems to fulfil safety and legal standards. These standards would be developed and monitored by an independent AI Agency that’s appropriately staffed by AI specialists.</span>

<span style="font-weight: 400;">Scherer proposes that rather than creating an AI Agency with ‘FDA-like powers’ to ban products, AI programs that are successfully certified could be granted limited liability. This means that plaintiffs would have to establish </span><i><span style="font-weight: 400;">actual negligence</span></i><span style="font-weight: 400;"> in the design, manufacturing, or operation of an AI system to be successful in a tort claim. The uncertified AI programs would still be available for commercial sale but would be subject to strict joint and several liability. Successful plaintiffs would, therefore, be permitted to ‘</span><i><span style="font-weight: 400;">recover the full amount of their damages from any entity in the chain of development, distribution, sale, or operational of the uncertified AI</span></i><span style="font-weight: 400;">’.[note]Ibid pg. 395.[/note]</span>

<span style="font-weight: 400;">Another advantage to Scherer’s proposal is that it leverages the institutional strengths of legislatures, agencies, and courts. As a summary, this structure would allocate roles in the following ways:</span>
<ul>
 	<li style="font-weight: 400;"><em>Legislature</em><span style="font-weight: 400;"> - This system would utilise the democratic mandate of the Legislature to determine the goals and purposes that guide AI governance. It would also use the powers of the Legislature to enact legislation (Scherer refers to this as the ‘Artificial Intelligence Development Act’) that would create an independent agency for oversight.</span></li>
 	<li style="font-weight: 400;"><em>Independent Agency</em><span style="font-weight: 400;"> -  As legislators lack the specialist knowledge required, they would delegate the central task of assessing the safety of AI systems to an independent agency of AI specialists. Independence is key, as it will help inculcate the Agency from the jockeying of electoral politics. An independent agency also has the flexibility to act preemptively. This flexibility and responsiveness is particularly important as AI development continues at breakneck speeds.</span></li>
 	<li style="font-weight: 400;"><em>Courts</em><b><span style="font-weight: 400;"> - The courts would be utilised for their strengths in adjudicating cases and allocating responsibility. This would require the courts to apply the rules governing negligence claims, differentiating between certified-AI with limited liability and uncertified-AI with strict liability. A core role of the courts will be allocating responsibility to the parties that caused harm through the AI program.</span></b></li>
</ul>
<span style="font-weight: 400;">This proposed structure isn’t a panacea to the list of issues above. It does, however, provide a flexible regulatory framework for oversight, without draconian regulations. By leveraging tort systems, the proposed structure would provide strong incentives for AI developers to incorporate safety features and internalise the associated costs. It would also provide a disincentive for distributors to sell uncertified AI programs that haven’t met public safety standards.</span>

<span style="font-weight: 400;">Regardless of whether Scherer’s proposal is considered appropriate, governments will need to develop policy positions for broad-scope AI. This will take careful planning and consideration. It will also require a sense of urgency. Ultimately, the future depends on what we do in the present.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>136</wp:post_id>
		<wp:post_date><![CDATA[2017-09-01 21:10:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-01 21:10:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ai-governance-11-key-issues]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence-policy"><![CDATA[Artificial Intelligence Policy]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[143]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Designing Effective Policies for Safety-Critical AI</title>
		<link>https://bitsandatoms.co/effective-policies-for-safety-critical-ai/</link>
		<pubDate>Fri, 22 Sep 2017 05:02:39 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=152</guid>
		<description></description>
		<content:encoded><![CDATA[<h3><span style="font-weight: 400;">Key policy considerations for national governments</span></h3>
<span style="font-weight: 400;">There’s growing noise around ‘regulating’ AI.[note]For example, see: Oren Etzioni, ‘<a href="https://www.nytimes.com/2017/09/01/opinion/artificial-intelligence-regulations-rules.html?mcubz=0">How to Regulate Artificial Intelligence</a>’, New York Times (1 Sept 2017).[/note]</span><span style="font-weight: 400;"> Some claim it’s too early,[note]Ahmed, K (2015) “<a href="http://www.bbc.com/news/business-34266425">Google’s Demis Hassabis - misuse of artificial intelligence ‘could do harm’</a>", BBC News.[/note]</span><span style="font-weight: 400;"> citing that precautionary regulations could impede technical developments; others call for action,[note]Scherer, Matthew U. (2015) "Regulating artificial intelligence systems: Risks, challenges, competencies, and strategies." <i>Harvard Journal of Technology and Law</i>.[/note]</span><span style="font-weight: 400;"> advocating measures that could mitigate the risks of AI.</span>

<span style="font-weight: 400;">It’s an important problem. And both ends of the debate make compelling arguments. AI applications have the potential to improve output, productivity, and quality of life. Forestalling AI developments that facilitate these advancements are big opportunity costs. Equally, the risks of broad-scope AI applications shouldn’t be dismissed. There are near-term implications, like job displacement and autonomous weapons, and longer-term risks, like values misalignment and the </span><a href="https://futureoflife.org/2015/11/23/the-superintelligence-control-problem/"><span style="font-weight: 400;">control problem</span></a><span style="font-weight: 400;">.</span>

<span style="font-weight: 400;">Regardless of where one sits on the ‘AI regulation’ spectrum, few would disagree that policymakers should have a firm grasp on the development and implications of AI. It’s unsurprising, given the rapid developments, that most do not. </span>

<b>Asymmetry of Knowledge</b>

<span style="font-weight: 400;">Policymakers are still very much at the beginning of learning about AI. The US government held public hearings late last year to ‘survey the current state of AI’.[note]US National Science &amp; Technology Committee on Technology (2016) “Preparing for the Future of Artificial Intelligence.” Executive Office of the President.[/note]</span><span style="font-weight: 400;"> Similarly, the UK House of Commons undertook an inquiry to identify AIs’ ‘potential value’ and ‘prospective problems’.[note]UK House of Commons (2016) “<a href="https://www.publications.parliament.uk/pa/cm201617/cmselect/cmsctech/145/145.pdf">Robotics and Artificial Intelligence - United Kingdom Parliament.</a>”[/note]</span>

<span style="font-weight: 400;">While these broad inquiries signify positive engagement, they also highlight policymakers’ relatively poor understanding, particularly compared to industry. This is understandable given the majority of A</span><span style="font-weight: 400;">I </span><span style="font-weight: 400;">development and expertise is concentrated in a select few organisations. This handful of organisations (literally, counted on two hands) possess orders of magnitude more data than anyone else. This access to data, coupled with resources and technical expertise, has fueled the rapid and concentrated development of AI. </span>

<span style="font-weight: 400;">Governments lacking in-house expertise is problematic from a policy development perspective. As AI increasingly intersects with safety-critical areas of society, governments hold responsibilities to act in the interests of their citizens. But if they don’t have the ability to formulate measured policies in accordance with these interests, then unintended consequences could arise, placing their citizens at risk. Without belabouring scenarios of misguided policies, governments should prioritise building their own expertise. Whether they’re prepared or not, governments are key stakeholders. They hold Social Contracts with their citizens to act on their behalf. So, as AI is applied to safety-critical industries, like healthcare, energy, and transportation, understanding the opportunities and implications is essential.</span>

<span style="font-weight: 400;">Ultimately, knowledge and expertise are central to effective policy decisions. And independence helps align policies to the public interest. While the spectrum of potential policy actions for safety-critical AI is broad, all with their own effects, inaction is also a policy position. This is where most governments are at. I think it’s important they rigorously consider the implications of these decisions.</span>

<span style="font-weight: 400;">Let me be clear: I’m not advocating for ‘more’ or ‘less’ regulation of AI. I’m advocating for governments to build their capacity to make effective and independent policy decisions. At the moment, few are qualified to do so. That’s a big reason why AI has developed in a policy vacuum. </span>

<b>‘Policy’ rather than ‘Regulation’</b>

<span style="font-weight: 400;">The term ‘regulation’ is not helpful in advancing the discourse on the roles of governments with AI. ‘Regulation’ can evoke perceptions of restriction. A heavy hand impeding growth, rather than nourishing progress. This is counterproductive, particularly in these early stages.</span>

<span style="font-weight: 400;">More useful, and less abrasive, is the term ‘policy’. Policy simply refers to a set of decisions that societies, through their governments, make about what they do and do not want to permit, and what they do or do not want to encourage.[note]Brundage, Miles, and Joanna Bryson, (2016) “<a href="http://arxiv.org/abs/1608.08196">Smart Policies for Artificial Intelligence</a>.” <i>arXiv,</i> pg. 2.[/note]</span><span style="font-weight: 400;"> Policies can be ‘pro-innovation’, helping to accelerate the development and diffusion of technologies. Policies can also decelerate and redirect technological development and diffusion. Science and Technology policy has a long history of both, and everything in between. </span>

<span style="font-weight: 400;">So, the term ‘policy’ is necessarily flexible. It may sound like semantics. But language matters.</span>

<b>Safety-Critical AI</b>

<span style="font-weight: 400;">Safety-critical AI refers to autonomous systems whose malfunction or failure can lead to serious consequences.[note]Nusser, Sebastian, (2009) “<a href="https://www.researchgate.net/publication/40220479_Robust_Learning_in_Safety-Related_Domains_machine_learning_methods_for_solving_safety-related_application_problems">Robust Learning in Safety-Related Domains : machine learning methods for solving safety-related application problems</a>”, OAI.[/note]</span><span style="font-weight: 400;"> This could include</span><span style="font-weight: 400;"> adverse environmental effects, loss or severe damage of equipment, harm or serious injury of people, or even death.</span>

<span style="font-weight: 400;">While there’s no formal definition of what constitutes ‘safety-critical’, governments already identify sectors considered ‘critical infrastructure’. The </span><a href="https://www.dhs.gov/critical-infrastructure-sectors"><span style="font-weight: 400;">US Department of Homeland Security</span></a><span style="font-weight: 400;"> classifies:</span>

<span style="font-weight: 400;">“</span><i><span style="font-weight: 400;">16 critical infrastructure sectors whose assets, systems, and networks, whether physical or virtual, are considered so vital to the United States that their incapacitation or destruction would have a debilitating effect on security, national economic security, national public health or safety, or any combination thereof.</span></i><span style="font-weight: 400;">”</span>

<span style="font-weight: 400;">Similarly, the Australian government (my home country) defines critical infrastructure as:</span>

<span style="font-weight: 400;">“</span><i><span style="font-weight: 400;">… those physical facilities, supply chains, information technologies and communication networks which, if destroyed, degraded or rendered unavailable for an extended period, would significantly impact on the social or economic wellbeing of the nation or affect Australia’s ability to conduct national defence and ensure national security.”</span></i><span style="font-weight: 400;">[note]Australia New Zealand Counter Terrorism Committee, (2015) “<a href="https://www.nationalsecurity.gov.au/Media-and-publications/Publications/Documents/national-guidelines-protection-critical-infrastructure-from-terrorism.pdf">National Guidelines for Protecting Critical Infrastructure from Terrorism</a>”, pg. 3.[/note]</span>

<span style="font-weight: 400;">These include sectors like energy, financial services, and transportation. Scrolling through the list, we can see AI already being applied in all of these sectors. </span>

<span style="font-weight: 400;">This as it should be. AI can improve productivity in the critical sectors of society and help us achieve more.</span>

<span style="font-weight: 400;">The problem is that AI systems designers still face very real challenges in making AI safe. These challenges are exacerbated, and their importance heightened, when AI’s are applied to safety-critical sectors. The stakes are high.</span>

<b>Concrete Problems in AI Safety</b>

<span style="font-weight: 400;">Dario Amodei et al. provide an excellent account of ‘</span><a href="https://arxiv.org/pdf/1606.06565.pdf"><span style="font-weight: 400;">Concrete Problems in AI Safety</span></a><span style="font-weight: 400;">’. This paper lists five practical research problems related to </span><i><span style="font-weight: 400;">accidents</span></i><span style="font-weight: 400;"> in machine learning systems (the most dominant subcategory of AI) that may emerge from poor design of real-world AI systems. A summary of the five problems are as follows:[note]Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané (2016) “<a href="http://arxiv.org/abs/1606.06565">Concrete Problems in AI Safety</a>.” <i>arXiv.</i>[/note]</span>
<ol>
 	<li style="font-weight: 400;"><b>Avoiding Negative Side Effects:</b><span style="font-weight: 400;"> ensuring that AI systems do not disturb the environment in negative ways while pursuing its goals.</span></li>
 	<li style="font-weight: 400;"><b>Avoiding Reward Hacking:</b><span style="font-weight: 400;"> ensuring that AI systems do not ‘game’ its reward function by exploiting bugs in its environment and acting in unintended (and potentially harmful) ways to achieve its goal(s). </span></li>
 	<li style="font-weight: 400;"><b>Scaleable Oversight: </b><span style="font-weight: 400;">ensuring that AI systems do the right things at scale despite limited information.</span></li>
 	<li style="font-weight: 400;"><b>Safe Exploration:</b><span style="font-weight: 400;"> ensuring that AI systems don’t make exploratory moves (i.e. try new things) with bad repercussions.</span></li>
 	<li style="font-weight: 400;"><b>Robustness to Distributional Shift:</b><span style="font-weight: 400;"> ensuring that AI systems recognise, and behave robustly, when in an environment different from its training environment.</span></li>
</ol>
<span style="font-weight: 400;">In addition to these safety problems from </span><i><span style="font-weight: 400;">unintentional accidents</span></i><span style="font-weight: 400;"> are the safety problems with AI systems designed to inflict </span><i><span style="font-weight: 400;">intentional </span></i><span style="font-weight: 400;">harm. We saw tastes of this during the </span><a href="http://www.npr.org/sections/alltechconsidered/2017/04/03/522503844/how-russian-twitter-bots-pumped-out-fake-news-during-the-2016-election"><span style="font-weight: 400;">2016 US Presidential Election</span></a><span style="font-weight: 400;">. Russian actors used Twitter and Facebook bots to create and proliferate derogatory claims and ‘Fake News’ about the Clinton campaign. While it appears most of these bots are considered ‘dumb AI’ (for example, programmed only to robotically retweet specific accounts), it’s a firm step towards </span><a href="https://medium.com/join-scout/the-rise-of-the-weaponized-ai-propaganda-machine-86dac61668b"><span style="font-weight: 400;">AI political propaganda</span></a><span style="font-weight: 400;">. There’s an immediate risk that machine learning techniques will be applied at scale in political campaigns to manipulate public engagement. This automated political mobilisation won’t be concerned with what’s ‘real’ or ‘true’. Its goals are to build followings, change minds, and mobilise votes.</span>

<span style="font-weight: 400;">Therefore, the risks of </span><i><span style="font-weight: 400;">unintentional</span></i><span style="font-weight: 400;"> and </span><i><span style="font-weight: 400;">intentional</span></i><span style="font-weight: 400;"> AI harm to the public are significant. As representatives of the public, it’s incumbent upon governments to: (a) develop institutional competencies and expertise in AI; and (b) institute measured and appropriate policies, guided by these competencies and expertise, that maximise the public benefits of AI, while minimising the public risks. </span>

<b>Skilling-Up Governments</b>

<span style="font-weight: 400;">Improving internal AI competencies within governments is a recommendation that regularly arises.[note]For example, see: Calo, Ryan (2014) “<a href="https://www.brookings.edu/research/the-case-for-a-federal-robotics-commission/">The Case for a Federal Robotics Commission</a>.” <i>Brookings</i>; Scherer, Matthew U. (2015) "Regulating artificial intelligence systems: Risks, challenges, competencies, and strategies." <i>Harvard Journal of Technology and Law</i>; UK House of Commons (2016) “<a href="https://www.publications.parliament.uk/pa/cm201617/cmselect/cmsctech/145/145.pdf">Robotics and Artificial Intelligence - United Kingdom Parliament.</a>”[/note]</span><span style="font-weight: 400;"> For reasons aforementioned, it’s broadly agreed to be an important step. A key challenge is how to attract and retain talent.</span>

<span style="font-weight: 400;">If governments (particularly Western governments) are to develop internal AI expertise, they’ll inevitably compete for talent with the likes of Google and Facebook. These companies build their technical outfit by offering enormous remuneration, lots of autonomy, great working conditions, and the social status of working for a company ‘changing the world’.</span>

<span style="font-weight: 400;">Working for the Department of Social Services for salary doesn’t quite have the same ring to it. </span>

<span style="font-weight: 400;">It’s also a function of social mood. Nationalism is on the rise,[note]Onder, Harun (2016) <a href="https://www.brookings.edu/blog/future-development/2016/07/18/the-age-factor-and-rising-nationalism/"><i>The age factor and rising nationalism</i></a>, Brookings Institution.[/note]</span><span style="font-weight: 400;"> trust in institutions has collapsed,[note]“<a href="https://www.edelman.com/trust2017/">2017 Edelman TRUST BAROMETER</a>.” 2017. <i>Edelman</i>.[/note]</span><span style="font-weight: 400;"> and the internet has afforded more opportunities for flexible and independent labour than any point in history. These cultural dynamics affect how governments operate and what people demand from them. </span>

<span style="font-weight: 400;">Of course, it’s possible for governments to stir inspiration and coordinate talent towards hard, technical goals. After all, we did ‘</span><a href="https://er.jsc.nasa.gov/seh/ricetalk.htm"><span style="font-weight: 400;">choose to go to the moon</span></a><span style="font-weight: 400;">’. But the audacious and inspirational plans of the 1960’s, encouraged by a culture of </span><a href="http://blakemasters.com/post/23435743973/peter-thiels-cs183-startup-class-13-notes"><span style="font-weight: 400;">definite optimism</span></a><span style="font-weight: 400;">, feels a far cry from the incremental plans and rising fog of pessimism that weighs on many governments today. </span>

<span style="font-weight: 400;">For problems as hard as AI policy, attracting the best and brightest is a crucial, yet formidable task. This is </span><a href="https://www.vox.com/2017/3/14/14924524/denis-mcdonough-podcast"><span style="font-weight: 400;">especially difficult</span></a><span style="font-weight: 400;"> for the roles at, or below, middle-management.</span>

<span style="font-weight: 400;">Governments are experimenting with ways to build technical talent. And there are some interesting initiatives happening in the periphery. For instance, the Obama Administration introduced the </span><a href="https://presidentialinnovationfellows.gov/"><span style="font-weight: 400;">Presidential Innovation Fellows</span></a><span style="font-weight: 400;"> program. Fellows serve for 12 months as embedded entrepreneurs-in-residence to help build the technological capacities of government agencies. The program attracts talent from the most prominent tech firms for ‘tours of duty’. They’re given resources and support to help work on important technical projects under the auspices of the Federal Government. </span>

<span style="font-weight: 400;">While positive, secondments won’t suffice for building AI competencies within governments. As the applications of AI are so broad, affecting so many safety-critical areas, governments have a responsibility to be prepared. At this stage, preparation involves understanding the unique opportunities and implications of AI. For many governments, it’s not clear who is responsible for this task and where expertise should reside.</span>

<span style="font-weight: 400;">This issue has led researchers, such as Ryan Calo[note]Calo, Ryan (2014) “<a href="https://www.brookings.edu/research/the-case-for-a-federal-robotics-commission/">The Case for a Federal Robotics Commission</a>.” <i>Brookings.</i>[/note]</span><span style="font-weight: 400;"> and Matt Scherer[note]Supra note 3.[/note],</span><span style="font-weight: 400;"> to recommend the establishment of government agencies specifically for AI &amp; Robotics.</span>

<b>The case for AI &amp; Robotics Agencies</b>

<span style="font-weight: 400;">Efforts to address AI &amp; Robotics policy decisions have been piecemeal, at best. However, as AI is increasingly scaled across sectors, Calo argues that the diffusion of expertise across existing agencies and departments makes less sense.[note]Supra note 12.[/note]</span><span style="font-weight: 400;"> A more centralised agency would provide a repository of expertise to advise and formulate policies recommended for governments.</span>

<i><span style="font-weight: 400;">Why an Agency?</span></i>

<span style="font-weight: 400;">In the context of AI, Scherer outlines the benefits and appropriateness of administrative agencies, stating:[note]Supra note 3, pg. 381.[/note]</span>
<ul>
 	<li><b>Flexibility<span style="font-weight: 400;"> - Agencies can be ‘tailor-made’ for a specific industry or particular social problem;</span></b></li>
</ul>
<ul>
 	<li><strong>Specialisation &amp; Expertise</strong><span style="font-weight: 400;"> - Policymakers in agencies can be experts in their field rather than the more generalist roles required by courts and legislatures; </span></li>
</ul>
<ul>
 	<li><strong>Independence &amp; Autonomy</strong><span style="font-weight: 400;"> - Agencies have more latitude to conduct independent factual investigations that serve as a basis for their policy decisions; and</span></li>
 	<li><strong>Ex Ante Action</strong><span style="font-weight: 400;"> - Similar to legislatures, agencies have the ability to formulate policy before harmful conduct occurs.</span></li>
</ul>
<i><span style="font-weight: 400;">What would an AI Agency do?</span></i>

<span style="font-weight: 400;">Views on the potential roles of AI &amp; Robotics agencies vary. They’re also necessarily country-specific. The key point of conjecture surrounds the degree of enforceability that an agency would assume.</span>

<span style="font-weight: 400;">Calo’s view is that a Federal Agency should act, at least initially, as an internal repository of expertise. A standalone entity tasked ‘</span><i><span style="font-weight: 400;">with the purpose of fostering, learning about, and advising upon</span></i><span style="font-weight: 400;">’ the impacts of AI &amp; Robotics on society.[note]Supra note 12. NB: While Calo refers only to Robotics in this publication, his position has been expanded to AI more generally. For example, see this <a href="https://www.wired.com/story/elon-forget-killer-robots-focus-on-the-real-ai-problems/">article</a>.[/note]</span><span style="font-weight: 400;"> This would help cultivate a deep appreciation of the technologies underlying AI. And governments will have unconstrained access to independent advice on the development trends, deployment progress, and inevitable risks that AI actually presents. If the risks develop such that stronger safety regulations are necessary, then an agency is in place to formulate these policies.</span>

<span style="font-weight: 400;">Both Scherer and Calo agree that an agency would provide an increasingly important resource for governments. However, Scherer proposes that a US AI Federal Agency should assume a more proactive role, sooner rather than later. The regulatory framework put forth would be based on a voluntary AI certification process, which would be managed by the agency.[note]Supra note 3.[/note]</span><span style="font-weight: 400;"> AI systems that are certified enjoy limited liability, whereas uncertified AI systems would assume full liability. (To learn more about it, read my blog </span><a href="https://bitsandatoms.co/ai-governance-11-key-issues/"><span style="font-weight: 400;">article</span></a><span style="font-weight: 400;"> summarising the research or read Scherer’s </span><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2609777"><span style="font-weight: 400;">paper</span></a><span style="font-weight: 400;">) </span>

<span style="font-weight: 400;">Despite these differing views on the scope of roles, the advent of new agencies in response to new technologies is not new. Radio, aviation, and space travel all resulted in the creation of new agencies across many nations. As these safety-critical technologies, and others, grew in capability and prominence, governments foresaw their opportunities and impacts, opting for dedicated agencies of expertise. Of course, the scope and responsibilities of different agencies vary, and some have been more effective than others. Therefore, an important research task for AI policy development is to assess the design, scope, and implementation of previous safety-critical government agencies, pulling out key lessons that might apply to AI &amp; Robotics. (I intend to write more on this subject in forthcoming blog posts)</span>

<b>What makes ‘effective AI Policy’?</b>

<span style="font-weight: 400;">This is perhaps the central question of AI policy. It speaks to the criteria that should be used to determine the merits of safety-critical AI policies. Specifically, </span><i><span style="font-weight: 400;">what constitutes effective policies in safety-critical AI? How will we know they’re effective?</span></i>

<span style="font-weight: 400;">The development of such criteria is a necessarily interdisciplinary task. It requires thoughtful input from a diversity of stakeholders and careful consideration of any recommendations. For the recommendation of any authoritative criteria assessing the effectiveness of AI policies could influence government actions. As </span><a href="http://www.drucker.institute/about-peter-f-drucker/"><span style="font-weight: 400;">Peter Drucker</span></a><span style="font-weight: 400;"> said: “<em>What gets measured gets managed</em>”.</span>

<span style="font-weight: 400;">The development of robust criteria needs to both sufficiently assess near-term policies but also provide insight into the projected longer-term impacts. It also needs to cater for the host of policies currently in place that directly, or indirectly, affect AI. These include </span><i><span style="font-weight: 400;">de facto</span></i><span style="font-weight: 400;"> policies such as privacy laws, intellectual property, and government research &amp; development investment.[note]Supra note 6.[/note]</span><span style="font-weight: 400;"> While the scholarship in AI policy is thin, there have been some ideas put forth to advance research discussions.</span>

<i><span style="font-weight: 400;">Near-term Criteria</span></i>

<span style="font-weight: 400;">In their seminal research survey, the </span><a href="https://futureoflife.org/"><span style="font-weight: 400;">Future of Life Institute</span></a><span style="font-weight: 400;"> published ‘</span><a href="https://futureoflife.org/2016/01/25/a-survey-of-research-questions-for-robust-and-beneficial-ai/"><span style="font-weight: 400;">A survey of research questions for robust and beneficial AI</span></a><span style="font-weight: 400;">’. In this survey, the authors proposed the following points for consideration:[note]Future of Life Institute (2016) “<a href="https://futureoflife.org/data/documents/research_survey.pdf?x56934">A Survey of Research Questions for Robust and Beneficial AI</a>.”[/note]</span>
<ol>
 	<li style="font-weight: 400;"><b>Verifiability of compliance</b><span style="font-weight: 400;"> - How governments will know that any rules or policies are being adequately followed</span></li>
 	<li style="font-weight: 400;"><b>Enforceability</b><span style="font-weight: 400;"> - Ability of governments to institute rules or policies, and maintain accountability</span></li>
 	<li style="font-weight: 400;"><b>Ability to reduce AI risk</b></li>
 	<li style="font-weight: 400;"><b>Ability to avoid stifling desirable technology development and have other negative consequences</b> <b>  </b>
<ol>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">What happens when governments ban or restrict certain kinds of technological development? </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">What happens when a certain kind of technological development is banned or restricted in one country but not in other countries where technological development sees heavy investment?</span></li>
</ol>
</li>
 	<li style="font-weight: 400;"><b>Adoptability</b><span style="font-weight: 400;"> - How well received the policies are from key stakeholders (the prospects of adoption increase when policy benefits those whose support is needed for implementation and when its merits can be effectively explained to decision-makers and opinion leaders)</span></li>
 	<li style="font-weight: 400;"><b>Ability to adapt over time to changing circumstances </b></li>
</ol>
<span style="font-weight: 400;">These policy criteria points share similarities with other safety-critical technologies, such as nuclear weapons, nanotechnology, and aviation. So, there are definitely lessons from the design and management of previous Science &amp; Technology policies. A core challenge, however, is for policymakers to apply these lessons appropriately, recognise the unique challenges of AI, and develop policy responses accordingly. To do so effectively, will require a considered eye on the longer-term implications of any policy decisions.</span>

<i><span style="font-weight: 400;">Long-term Criteria</span></i>

<span style="font-weight: 400;">Professor Nick Bostrom et al. from the Future of Humanity Institute provides an overview of some key long-term policy considerations in their working paper: ‘</span><a href="https://www.fhi.ox.ac.uk/new-working-paper-policy-desiderata-in-the-development-of-machine-superintelligence/"><span style="font-weight: 400;">Policy Desiderata in the Development of Machine Superintelligence</span></a><span style="font-weight: 400;">’. This paper focuses on policy considerations for Artificial General Intelligence (AGI), which are considered ‘unique’ to AGI or ‘unusual’. </span>

<span style="font-weight: 400;">The paper distils the following desiderata:[note]Bostrom, Nick, Allan Dafoe, and Carrick Flynn. (2016) “<a href="https://www.fhi.ox.ac.uk/new-working-paper-policy-desiderata-in-the-development-of-machine-superintelligence/">Policy Desiderata in the Development of Machine Superintelligence</a>.” <i>Future of Humanity Institute</i>.[/note]</span>
<ul>
 	<li><b>Expeditious progress<span style="font-weight: 400;"> - Ensure that AI development and the path with a high probability to superintelligence is speedy. Socially beneficial products and applications are made widely available in a timely fashion.</span></b></li>
</ul>
<ul>
 	<li><strong>AI safety</strong><span style="font-weight: 400;"> - Techniques are developed to make it possible to ensure that advanced AIs act as intended.</span></li>
</ul>
<ul>
 	<li><strong>Conditional stablisation</strong><span style="font-weight: 400;"> (kill switch) - The ability to temporarily or permanently stablise the AI to avert catastrophe.</span></li>
</ul>
<ul>
 	<li><strong>Non-turbulence</strong><span style="font-weight: 400;"> (social stability) - The path avoids excessive efficiency losses from chaos and conflict. Political systems maintain stability and order, adapt successfully to change, and mitigate socially disruptive impacts.</span></li>
</ul>
<ul>
 	<li><strong>Universal benefit</strong><span style="font-weight: 400;"> - All humans who are alive at the transition of AGI get some share of the benefit, in compensation for the risk externality to which they were exposed.</span></li>
</ul>
<ul>
 	<li><strong>Magnanimity</strong><span style="font-weight: 400;"> (altruism and compassion) - A wide range of resource-satiable values (ones to which there is little objection aside from cost-based considerations), are realized if and when it becomes possible to do so using a minute fraction of total resources. This may encompass basic welfare provisions and income guarantees to all human individuals.</span></li>
 	<li><strong>Continuity</strong><span style="font-weight: 400;"> (fair resource allocation) - (i) maintain order and provide the institutional stability needed for actors to benefit from opportunities for trade behind the current veil of ignorance, including social safety nets; and (ii) prevent concentration and permutation from radically exceeding the levels implicit in the current social contract (basically, gross and growing resource inequality).</span></li>
 	<li style="font-weight: 400;"><b>Mind crime prevention</b><span style="font-weight: 400;"> - AI is governed in such a way that maltreatment of sentient digital minds is avoided or minimised.</span></li>
 	<li style="font-weight: 400;"><b>Population policy</b><span style="font-weight: 400;"> - Procreative choices, concerning what new beings bring into existence, are made in a coordinated manner and with sufficient foresight to avoid unwanted Malthusian dynamics and political erosion. (For e.g. what happens to population policy if humans become economically unproductive beings and governments are no longer incentivised to support them?)</span></li>
 	<li style="font-weight: 400;"><b>Responsibility and wisdom</b><span style="font-weight: 400;"> - The seminal applications of advanced AI are shaped by an agency (individual or distributed) that has an expansive sense of responsibility and the practical wisdom to see what needs to be done in radically unfamiliar circumstances.</span></li>
</ul>
<span style="font-weight: 400;">As the authors stress, these summarised criteria points aren’t ‘the answer’. Rather, they’re ideas to be built upon. What is clear, however, is that given the development speeds of AI, any near-term policies will need to closely consider its longer-term implications. For as the capacities of intelligent systems continue to compound, so too will their impacts. Therefore, policy decisions, whether deliberately or not, will affect the development of AI and its deployment throughout society. Establishing criteria to assess these policy decisions will help ensure that AI is safe and beneficial to humanity.</span>

<b>Conclusion</b>

<span style="font-weight: 400;">As AI becomes more pervasive, questions of policy will intensify. While shrouded in complexity, policymakers can help ensure the safe passage of AI that’s beneficial to humanity. As the representatives of the public, they have a responsibility to be informed and involved.</span>

<i><span style="font-weight: 400;">I hope this essay on some of the key issues and arguments in AI policy was helpful. I’d love any feedback. Feel free to get in touch either by commenting below or send me an email via: </span></i><a href="mailto:nik@bitsandatoms.co"><i><span style="font-weight: 400;">nik@bitsandatoms.co</span></i></a><i><span style="font-weight: 400;">.</span></i>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>152</wp:post_id>
		<wp:post_date><![CDATA[2017-09-22 05:02:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-09-22 05:02:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[effective-policies-for-safety-critical-ai]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="governments"><![CDATA[Governments]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[158]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>PhD Tools &amp; Tactics</title>
		<link>https://bitsandatoms.co/phd-tools-tactics/</link>
		<pubDate>Sun, 15 Oct 2017 20:50:59 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=165</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">It’s daunting to start a PhD. Looking out to three years of prolonged focus and intense concentration is intimidating. </span>

<span style="font-weight: 400;">It’s an exercise in truth and independent thought. Where thinking for yourself is the hardest task of all.</span>

<span style="font-weight: 400;">Thankfully, there’s a sea of resources to help researchers manage their dissertations. In fact, there’s so much available that it’s difficult to pick the best tools for the job. </span>

<span style="font-weight: 400;">When I started in August 2017, I spent a good amount of time researching products, trialing software systems, and speaking with other researchers about the tools they use to manage their research. After a few months in, here are some resources that I recommend:</span>
<h3><b>Document Management</b></h3>
<a href="https://paperpile.com/"><span style="font-weight: 400;">Paperpile</span></a>

<span style="font-weight: 400;">Having a solid and logical reference management system is essential for research. You’ll quickly accumulate hundreds (and eventually thousands) of books, papers, and other resources. Paperpile is great because you can save all the bibliographic information in one click, sort all your resources into folders and tags, and save documents to your Google Drive. There’s also a handy Google Chrome plugin and Google Search &amp; Scholar integrations.</span>

<span style="font-weight: 400;">Whether you choose to use Paperpile or something like </span><a href="http://www.endnote.com"><span style="font-weight: 400;">EndNote</span></a><span style="font-weight: 400;"> is a matter of personal preference. I chose Paperpile because of how well it integrates with GSuite. The most important thing is to have some reference management system in place right at the beginning, ideally with folders and tags organised by keywords.</span>

<a href="https://evernote.com/"><span style="font-weight: 400;">Evernote</span></a>

<span style="font-weight: 400;">I’ve been using Evernote for a while and love it. It’s a great tool to organise all your documents and notes into folders and tags. The Evernote Web Clipper is also terrific. The real draw cards, however, are in the Premium Version. Here you can mark-up PDF docs, take photos of written notes to transform them into text, and there’s 10GB of storage per month. Students can also get 50% off the annual subscription.</span>
<h3><b>Writing and Collaboration</b></h3>
<a href="https://www.google.com.au/docs/about/"><span style="font-weight: 400;">Google Docs</span></a>

<span style="font-weight: 400;">Collaborating with your Supervisor or other researchers by Google Docs is an invaluable way to get feedback. They can add/edit/delete and provide comment on sections, all in real-time. Again, other software applications have caught-up on these features, so it’s a matter of personal preference. Just make sure you’re writing in a word processor with live update functionality. You don’t want to keep track of the 106th version of a doc!</span>
<h3><b>Productivity</b></h3>
<a href="https://www.rescuetime.com/"><span style="font-weight: 400;">RescueTime</span></a>

<span style="font-weight: 400;">There are so many things competing for our attention. But high-quality research demands extended periods of intense focus. RescueTime tracks how you’ve spent your time online. It breaks down this time into application categories and the proportion of time spent in focused applications (for e.g. </span><i><span style="font-weight: 400;">Design and Composition</span></i><span style="font-weight: 400;"> for Google Docs is considered ‘focused’ and ‘productive’, whereas </span><i><span style="font-weight: 400;">Social Media</span></i><span style="font-weight: 400;"> is considered ‘distracting’). While it doesn’t measure output, it does provide an insight into how you spend your time, which directly affects productivity.</span>
<h3><b>Project Management</b></h3>
<a href="https://basecamp.com/"><span style="font-weight: 400;">Basecamp</span></a>

<span style="font-weight: 400;">Like with any big project, there’s always the nagging question: ‘</span><i><span style="font-weight: 400;">Is this the best thing that I can be doing, right now?</span></i><span style="font-weight: 400;">’. I’ve found the project management software Basecamp to be useful for planning and managing my PhD. You’re able to set goals, schedule To-dos, link to documents, collaborate with peers, generate activity reports, and Automatic check-ins for personal accountability and reflection. The best news: Basecamp for Education is FREE for students! That’s enterprise project management software for NOTHING. </span>
<h3><b>Automatic Alerts</b></h3>
<a href="https://www.google.com.au/alerts"><span style="font-weight: 400;">Google Alerts</span></a>

<span style="font-weight: 400;">Once you’ve established your research keywords, setup Google Alerts and </span><a href="https://scholar.google.com/intl/en/scholar/help.html"><span style="font-weight: 400;">Google Scholar Alerts</span></a><span style="font-weight: 400;"> for the most recent published news and research. It saves a ton of time from trawling online, trying to keep up with the latest literature and industry developments.</span>
<h3><b>Clarity and Focus</b></h3>
<a href="https://www.calm.com/"><span style="font-weight: 400;">Calm</span></a>

<span style="font-weight: 400;">It’s easy to feel scattered and overwhelmed by the enormity of a PhD. But clarity of thought is essential to quality research. Daily meditation has been the most important practice I’ve established in the past year. Given the </span><a href="https://www.researchgate.net/publication/273774412_The_neuroscience_of_mindfulness_meditation"><span style="font-weight: 400;">neuroscientific evidence</span></a><span style="font-weight: 400;"> behind the practice, it’s unsurprising that it’s had such a significant effect.</span>

<span style="font-weight: 400;">I started with the free guided meditations from </span><a href="https://www.calm.com/"><span style="font-weight: 400;">Calm</span></a><span style="font-weight: 400;">, </span><a href="https://www.headspace.com/headspace-meditation-app"><span style="font-weight: 400;">Headspace</span></a><span style="font-weight: 400;">, and </span><a href="https://smilingmind.com.au/"><span style="font-weight: 400;">Smiling Mind</span></a><span style="font-weight: 400;">. I even paid for an annual subscription to Headspace, which was ok but it didn’t hook me in. I eventually found consistency in the free version of Calm using the ‘Timed Meditation’. There’s no guide, just the background noise of nature on loop and you select how long you want to meditate for. I’ve found 20 minutes first thing in the morning and 20 minutes before bed are great bookends to the day. I feel more present during the day, clearer in my thinking, and sleep better at night.</span>

<span style="font-weight: 400;">For those starting out, I’d recommend doing at least a few of the guided meditations to begin with. Following your breath for even 10 seconds is hard and requires training. So the guided beginner sessions definitely help to bring you back to the breath. </span>
<h3><b>Books</b></h3>
<a href="https://www.goodreads.com/book/show/25744928-deep-work"><span style="font-weight: 400;">Deep Work</span></a><span style="font-weight: 400;"> by </span><i><span style="font-weight: 400;">Cal Newport</span></i>

<span style="font-weight: 400;">This book changed the way I think about work and is the most important book I’ve read on productivity. Its central thesis is that to create things of value in society requires consistent and intense periods of focus. This is a far cry from the reactionary and distraction riddled practices of how most people work. Newport lays out strategies for how to cultivate a deep work ethic and discusses how these trained behaviours are becoming more needed and valued.</span>

<a href="https://www.goodreads.com/book/show/15999568-a-manual-for-writers-of-research-papers-theses-and-dissertations-eigh?rating=2"><span style="font-weight: 400;">A Manual for Writers of Research Papers, Theses, and Dissertations, Eighth Edition</span></a><span style="font-weight: 400;"> by </span><i><span style="font-weight: 400;">Kate L. Turabian</span></i>

<span style="font-weight: 400;">This is the best dissertation writer’s manual I’ve come across. It includes the full life-cycle of a research thesis; from establishing your topic to presenting and revising your final draft. The book is well structured and provides useful conceptual frameworks to organise and improve your research process and output.</span>

<i><span style="font-weight: 400;">I hope you found this useful. If anyone has any questions or suggestions for other resources, I’d love to hear them! Please comment below or email me at </span></i><a href="mailto:nik@bitsandatoms.co"><i><span style="font-weight: 400;">nik@bitsandatoms.co</span></i></a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>165</wp:post_id>
		<wp:post_date><![CDATA[2017-10-15 20:50:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-15 20:50:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[phd-tools-tactics]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="dissertation"><![CDATA[Dissertation]]></category>
		<category domain="post_tag" nicename="phd"><![CDATA[PhD]]></category>
		<category domain="post_tag" nicename="research-resources"><![CDATA[Research Resources]]></category>
		<category domain="category" nicename="research-tools"><![CDATA[Research Tools]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[166]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Recommendations: books, podcasts, videos, and other great stuff</title>
		<link>https://bitsandatoms.co/october-recommendations/</link>
		<pubDate>Wed, 18 Oct 2017 03:05:32 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=172</guid>
		<description></description>
		<content:encoded><![CDATA[<h3><b>Books</b></h3>
<strong><a href="https://www.goodreads.com/book/show/34272565-life-3-0?ac=1&amp;from_search=true">Life 3.0: Being Human in the Age of Artificial Intelligence</a> by<i> Max Tegmark</i></strong>

<span style="font-weight: 400;">This is one of the few books on AI that I’ve read that does justice to the core issues of Artificial General Intelligence (AGI), while still being accessible to a mainstream audience. Working as a Professor in MIT, Tegmark is one of the leading minds in AGI research. If you’re new to AI and interested in its implications, this is a great place to start. Once you’ve read this, then read Nick Bostrom’s ‘</span><a href="https://www.goodreads.com/book/show/20527133-superintelligence?ac=1&amp;from_search=true"><span style="font-weight: 400;">Superintelligence: Paths, Dangers, Strategies</span></a><span style="font-weight: 400;">’.</span>

<strong><a href="https://www.goodreads.com/book/show/34536488-principles?ac=1&amp;from_search=true">Principles: Life and Work</a> by <i>Ray Dalio</i></strong>

<span style="font-weight: 400;">Dalio is a giant in the investing world. His company, </span><a href="https://www.bridgewater.com/"><span style="font-weight: 400;">Bridgewater Associates</span></a><span style="font-weight: 400;">, is among the most successful Hedge Funds in history and they’re known for their unique management practices. Central to Bridgewater is the concept of an ‘Idea Meritocracy’; a practical philosophy where the best ideas rise to the top and are openly debated. To implement this concept, Bridgewater demands a culture of ‘radical truth’ and ‘radical transparency’, where almost all meetings are filmed, individual weaknesses are consistently identified, and truth and honesty are prioritised.</span>

<span style="font-weight: 400;">The book is a compilation and explanation of Dalio’s life and work principles. They’re the underpinnings that have guided the successes of both Dalio and Bridgewater. This is both the most practical and confronting self-improvement book I’ve read. While I found that certain principles evoked visceral responses, they were hard to argue with and difficult to dismiss. It’s changed my perspective and my actions. And it’s a book I plan to regularly revisit. </span>

<span style="font-weight: 400;">For those interested in Economics, also check out his research and video on ‘</span><a href="https://www.bridgewater.com/research-library/how-the-economic-machine-works/"><span style="font-weight: 400;">How the Economic Machine Works</span></a><span style="font-weight: 400;">’. As someone trained in Economics, it’s the most accessible and comprehensive description I’ve found. </span>

<strong><a href="https://www.goodreads.com/book/show/174713.The_Lessons_of_History?from_search=true">The Lessons of History</a> by <i>Will Durant &amp; Ariel Durant</i></strong>

<span style="font-weight: 400;">This book was recommended by Ray Dalio as essential reading in a podcast. And it didn’t disappoint. Will &amp; Ariel Durant are Pulitzer Prize historians, and they’re best known for their 11 volume series on </span><a href="https://www.goodreads.com/book/show/78159.The_Story_of_Civilization?from_search=true"><span style="font-weight: 400;">The Story of Civilization</span></a><span style="font-weight: 400;">. The husband and wife duo synthesised this series into 120 pages for </span><a href="https://www.goodreads.com/book/show/174713.The_Lessons_of_History?ac=1&amp;from_search=true"><span style="font-weight: 400;">The Lessons of History</span></a><span style="font-weight: 400;">. Beautifully written and masterfully organised; it’s the best book I’ve read this year.</span>

<strong><a href="https://www.goodreads.com/book/show/20342617-just-mercy?ac=1&amp;from_search=true">Just Mercy: A Story of Justice and Redemption</a> by <i>Bryan Stevenson</i></strong>

<span style="font-weight: 400;">Bryan Stevenson is an Attorney working on Death Row cases in Montgomery, Alabama. Stevenson has dedicated his professional life to overcoming the injustices of the US legal system and racial discrimination in America’s South. This important read is a coming of age of a talented and idealistic lawyer, leading him to the realisation that ‘</span><i><span style="font-weight: 400;">the opposite of poverty isn’t wealth, but justice</span></i><span style="font-weight: 400;">’.</span>

<span style="font-weight: 400;">Before reading the book, I highly recommend listening to the Ezra Klein Show podcast ‘Bryan Stevenson on why the opposite of poverty isn’t wealth, but justice’ on the 16th of May 2017 (</span><a href="https://soundcloud.com/ezra-klein-show/bryan-stevenson-on-why-the"><span style="font-weight: 400;">SoundCloud</span></a><span style="font-weight: 400;"> | </span><a href="https://itunes.apple.com/au/podcast/the-ezra-klein-show/id1081584611?mt=2"><span style="font-weight: 400;">Apple Podcasts</span></a><span style="font-weight: 400;">).</span>
<h3><b>Blogs &amp; Research</b></h3>
<strong><a href="https://unenumerated.blogspot.com">Unenumerated</a> by <i>Nick Szabo</i></strong>

<span style="font-weight: 400;">I’ve been trying to wrap my head around cryptocurrencies of late. Like with anything complex, the more you learn, the more you realise how little you know. Cryptocurrencies, like Bitcoin and Ethereum, are prime examples. Nick Szabo’s blog has the most comprehensive selection of long-form articles on cryptocurrency and blockchain technologies I’ve come across.</span>

<strong><a href="https://www.bridgewater.com/resources/bwam032217.pdf">Populism: The Phenomenon</a> by <i>Bridgewater Associates</i></strong>

<span style="font-weight: 400;">Populist politics has surged across major nations. Trump, Brexit, Germany’s AfD, and many others have either taken office or are growing in popularity. Bridgewater shows that Populism is at its highest levels since the 1930’s. It also draws parallels to today with the economic conditions of the 30’s. When projecting economic futures under these conditions, the most important thing to monitor is how conflict is handled. The research is a unique and independent thought-piece in a time of rising political populism.</span>
<h3><b>Videos</b></h3>
<strong><a href="https://www.youtube.com/playlist?list=PLfc2WtGuVPdmhYaQjd449k-YeY71fiaFp">A Brief History of Humankind</a> by <i>Dr. Yuval Noah Harari</i></strong>

<span style="font-weight: 400;">Harari is one of my favourite authors, and I believe </span><a href="https://www.goodreads.com/book/show/23692271-sapiens?ac=1&amp;from_search=true"><span style="font-weight: 400;">Sapiens</span></a><span style="font-weight: 400;"> and </span><a href="https://www.goodreads.com/book/show/31138556-homo-deus"><span style="font-weight: 400;">Homo Deus: A Brief History of Tomorrow</span></a><span style="font-weight: 400;"> are two of the most important books written in the 21st century. I wanted more, so I’ve been watching his online course, </span><a href="https://www.youtube.com/playlist?list=PLfc2WtGuVPdmhYaQjd449k-YeY71fiaFp"><span style="font-weight: 400;">A Brief History of Humankind</span></a><span style="font-weight: 400;">, on YouTube. I’ve found it to be a great way of reinforcing what I’ve already read in his books. </span>

<strong><a href="https://www.netflix.com/au/title/80057883">Abstract: The Art of Design</a> on Netflix</strong>

<span style="font-weight: 400;">This series follows leading designers across disciplines to provide insight into their methods and routines. From architecture to footwear, we’re given a glimpse into the creative genius of the world’s most prominent designers.</span>

<strong><a href="https://vimeo.com/223928856">The Endurance Test: The 1000 Days</a> by <i>Ivan Olita</i></strong>

<span style="font-weight: 400;">Kaihōgyō is a pilgrimage lasting 1,000 days performed by the Tendai Buddhists monks of Mount Hiei, Japan to achieve enlightenment. This beautifully shot 6 minute short film shows the training and endurance behind the 7 year journey, of which only 46 men have completed since 1885.</span>
<h3><b>Podcasts</b></h3>
<a href="https://tim.blog/2017/10/14/walter-isaacson/"><span style="font-weight: 400;">Lessons from Steve Jobs, Leonardo da Vinci, and Ben Franklin</span></a><span style="font-weight: 400;"> by </span><i><span style="font-weight: 400;">The Tim Ferriss Show</span></i><span style="font-weight: 400;">, 13th October 2017 (</span><a href="https://www.stitcher.com/podcast/the-tim-ferriss-show"><span style="font-weight: 400;">Stitcher</span></a><span style="font-weight: 400;"> | </span><a href="https://itunes.apple.com/us/podcast/the-tim-ferriss-show/id863897795?mt=2"><span style="font-weight: 400;">Apple Podcasts</span></a><span style="font-weight: 400;">)</span>

<span style="font-weight: 400;">In the interview, Walter Isaacson states that ‘</span><i><span style="font-weight: 400;">The best way to teach human history is to tell the stories of the people that have made it</span></i><span style="font-weight: 400;">.’ He’s dedicated the best part of his career to this. And he’s now done the same for Leonardo da Vinci. This wide-ranging interview highlights the common threads and key difference behind some of history’s greatest minds.</span>
<h3><b>Some Recent AI Resources</b></h3>
<ul>
 	<li style="font-weight: 400;"><a href="https://www.gov.uk/government/uploads/system/uploads/attachment_data/file/652097/Growing_the_artificial_intelligence_industry_in_the_UK.pdf"><span style="font-weight: 400;">GROWING THE ARTIFICIAL INTELLIGENCE INDUSTRY IN THE UK</span></a><span style="font-weight: 400;"> [Government Paper]</span></li>
 	<li style="font-weight: 400;"><a href="https://www.youtube.com/watch?v=bsuvM1jO-4w&amp;feature=youtu.be"><span style="font-weight: 400;">Deep RL Bootcamp Frontiers Lecture I: Recent Advances, Frontiers and Future of Deep RL</span></a><span style="font-weight: 400;"> [YouTube video]</span></li>
 	<li style="font-weight: 400;"><a href="http://www.parliamentlive.tv/Event/Index/073717ca-484b-4015-bd10-f847cea3f249"><span style="font-weight: 400;">Professor Nick Bostrom presents evidence to the House of Lords Select Committee on Artificial intelligence in Westminster</span></a><span style="font-weight: 400;"> [Video]</span></li>
 	<li style="font-weight: 400;"><a href="https://intelligence.org/2017/10/13/fire-alarm/">There’s No Fire Alarm for Artificial General Intelligence</a> [Blog by MIRI]</li>
</ul>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>172</wp:post_id>
		<wp:post_date><![CDATA[2017-10-18 03:05:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-18 03:05:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[october-recommendations]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-resources"><![CDATA[AI Resources]]></category>
		<category domain="post_tag" nicename="blogs"><![CDATA[Blogs]]></category>
		<category domain="post_tag" nicename="books"><![CDATA[Books]]></category>
		<category domain="category" nicename="monthly-recommendations"><![CDATA[Monthly Recommendations]]></category>
		<category domain="post_tag" nicename="podcasts"><![CDATA[Podcasts]]></category>
		<category domain="post_tag" nicename="recommendations"><![CDATA[Recommendations]]></category>
		<category domain="post_tag" nicename="research"><![CDATA[Research]]></category>
		<category domain="post_tag" nicename="videos"><![CDATA[Videos]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[173]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[16]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>A Primer on Reinforcement Learning</title>
		<link>https://bitsandatoms.co/primer-reinforcement-learning/</link>
		<pubDate>Fri, 03 Nov 2017 04:47:03 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=179</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">The rate of development in AI continues at a rapid pace. And a couple of weeks ago we saw another important milestone. DeepMind </span><a href="http://nature.com/articles/doi:10.1038/nature24270"><span style="font-weight: 400;">published</span></a><span style="font-weight: 400;"> their latest developments of </span><a href="https://deepmind.com/research/alphago/"><span style="font-weight: 400;">AlphaGo</span></a><span style="font-weight: 400;">, a computer program designed to play the ancient Chinese game of </span><a href="https://en.wikipedia.org/wiki/Go_(game)"><span style="font-weight: 400;">Go</span></a><span style="font-weight: 400;"> at superhuman levels. Go is incredibly complex. It has a possible 10 to the power of 170 configurations. That’s more than the number of atoms in the known universe!</span>
<h3><span style="font-weight: 400;">AlphaGo Zero</span></h3>
<span style="font-weight: 400;">DeepMind’s latest version, </span><a href="https://deepmind.com/blog/alphago-zero-learning-scratch/"><span style="font-weight: 400;">AlphaGo Zero</span></a><span style="font-weight: 400;">, exceeded the performance all previous versions. It did so by using a novel form of self-play Reinforcement Learning (a subset of Machine Learning), which I’ll explain in more detail later.</span>

<span style="font-weight: 400;">AlphaGo Zero represents an important advancement not only because of its performance but also because of its method. All </span><span style="font-weight: 400;">previous versions</span><span style="font-weight: 400;">, like </span><i><span style="font-weight: 400;">AlphaGo Fan</span></i><span style="font-weight: 400;"> in 2015 and </span><i><span style="font-weight: 400;">AlphaGo Lee</span></i><span style="font-weight: 400;"> in 2016 that beat the European and World Champions respectively, were trained on the data of thousands of human games. They used two Deep Neural Networks to output move probabilities (policy network) and position evaluations (value network). Once trained, these networks were combined with a lookahead search (Monte-Carlo Tree Search) to evaluate move positions in the tree.[note]Silver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the Game of Go without Human Knowledge.” <i>Nature</i> 550 (7676): 354–59.[/note]</span>

<span style="font-weight: 400;">The latest AlphaGo Zero version differs in four important ways:</span>
<ul>
 	<li><b>Trained solely by self-play reinforcement learning: <span style="font-weight: 400;">Basically, AlphaGo Zero became its own teacher with no training data provided. It started from random self-play and trained itself from first principles to become the world’s most advanced Go player.</span></b></li>
 	<li><strong>It only used the black and white stones on the Board as input features:</strong><span style="font-weight: 400;"> Previous versions had included human-engineered input features to help guide the program.</span></li>
 	<li><strong>Combines policy and value networks into a single neural network:</strong><span style="font-weight: 400;"> AlphaGo Zero simplifies the network architecture from two neural networks to one, improving the efficiency of the structure.</span></li>
 	<li><strong>Simpler tree search:</strong><span style="font-weight: 400;"> It relies upon the single neural network to evaluate moves and positions, and doesn’t perform Monte-Carlo ‘rollouts’ to predict which player will win based on the current board configuration.</span></li>
</ul>
<span style="font-weight: 400;">The most important developments, however, are the algorithmic improvements. As discussed by David Silver, this shows that ‘</span><i><span style="font-weight: 400;">Algorithms matter much more than either compute or data availability… we used an order of magnitude less computation than we did with previous versions of Alpha Go and yet it was able to perform at a much higher level due to using much more principled algorithms.</span></i><span style="font-weight: 400;">’[note]Hassabis, Demis, and David Silver. 2017. “AlphaGo Zero: Learning from Scratch.” <i>DeepMind</i>. October 18. <a href="https://deepmind.com/blog/alphago-zero-learning-scratch/">https://deepmind.com/blog/alphago-zero-learning-scratch/</a>.[/note]</span>

<span style="font-weight: 400;">The image below[note]Supra note 2.[/note]</span><span style="font-weight: 400;"> shows that after 3 days AlphaGo Zero surpassed the version that beat the best human player in the world. And after 40 days, AlphaGo Zero was able to beat all previous versions of AlphaGo to become the strongest Go program in the world. (NB: </span><i><span style="font-weight: 400;">the Elo Rating on the vertical axis is a widely used measure of player performance in games such as Go and Chess</span></i><span style="font-weight: 400;">)</span>

<img class="aligncenter " src="http://blogs.discovermagazine.com/d-brief/files/2017/10/AlphaGo-Zero-Training-Time.gif" width="731" height="389" />

<span style="font-weight: 400;">This impressive feat was achieved through a novel form of Reinforcement Learning (RL). AlphaGo Zero was able to teach itself by always having an opponent at just the right level, which was calibrated exactly to its current level of performance. </span>

<span style="font-weight: 400;">Before we dive into an overview of how RL works, it’s important first to understand the key concepts of how machines learn and their infrastructure.</span>
<h3><span style="font-weight: 400;">Brief Overview of Machine Learning</span></h3>
<span style="font-weight: 400;">Machine Learning (ML) is a subset of AI and it’s principally concerned with teaching machines to learn on their own. ML systems learn from data to autonomously make predictions and/or decisions.[note]Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview.” arXiv [cs.LG]. <a href="http://arxiv.org/abs/1701.07274">http://arxiv.org/abs/1701.07274</a>.[/note]</span><span style="font-weight: 400;"> The three main categories of ML are Supervised, Unsupervised, and Reinforcement Learning.[note]</span><span style="font-weight: 400;">Jordan, M. I. and Mitchell, T. 2015. “Machine learning: Trends, perspectives, and prospects”. Science, 349(6245):255–260.</span>

<span style="font-weight: 400;">While these three subsets of ML place some structure around the discipline, lots of the current research explores the intersection of these subsets. For example, semi-supervised learning makes use of unlabeled data to augment labelled data in a supervised learning context.[/note]</span>

<img class="aligncenter wp-image-186" src="https://bitsandatoms.co/wp-content/uploads/2017/11/ML_branches.jpg" alt="" width="366" height="347" />
<p style="text-align: center;"><span style="font-weight: 400;">Branches of Machine Learning</span></p>
<p style="text-align: center;"><span style="font-weight: 400;">Source: </span><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;t=2767s"><span style="font-weight: 400;">RL Course by David Silver - Lecture 1: Introduction to Reinforcement Learning</span></a></p>
<span style="font-weight: 400;">I’ll give a brief overview of Supervised, Unsupervised, and also Deep Learning before I dig into RL.</span>
<h3><span style="font-weight: 400;">Supervised Learning</span></h3>
<span style="font-weight: 400;">Supervised Learning uses a set of correctly labelled training data that are provided by a knowledgeable external supervisor (i.e. a human).[note]Richard S. Sutton and Andrew G. Barto. 2016. <i>Reinforcement Learning: An Introduction</i>. The MIT Press.[/note]</span><span style="font-weight: 400;"> It essentially trains networks by practising on training data to predict and/or make decisions where the correct answer is already known. The purpose of Supervised Learning is to learn by analysing these vast reams of labelled data to extrapolate representations or generalise trends, so it can correctly categorise situations not present in the training set.</span>

<span style="font-weight: 400;"> The two tasks of Supervised Learning are:</span>
<ul>
 	<li style="font-weight: 400;"><strong>Classification:</strong><span style="font-weight: 400;"> Correctly assign class labels to unseen instances. For e.g. correctly identify cats in YouTube videos</span></li>
 	<li style="font-weight: 400;"><strong>Regression:</strong><span style="font-weight: 400;"> Predict continuous values based on inputs. For e.g. predicting house prices based on location, square footage, number of bedrooms etc.</span></li>
</ul>
<span style="font-weight: 400;">Training the network to learn these representations is achieved by splitting a labelled dataset into two piles. The first pile is the </span><i><span style="font-weight: 400;">training data</span></i><span style="font-weight: 400;"> (usually ~80% of the dataset) where the output (i.e the answer) is known. During training, the model makes predictions for each instance and receives feedback based on the outputs in the training data. This feedback is quantified by the algorithm to determine ‘how close’ the prediction was to the known output, which is referred to as the </span><i><span style="font-weight: 400;">cost function</span></i><span style="font-weight: 400;"> or </span><i><span style="font-weight: 400;">utility function.</span></i><span style="font-weight: 400;"> The changes to the function are fed back to the network to modify the strength of connections between the nodes to optimise predictability of the output.</span>

<span style="font-weight: 400;">The second pile is the </span><i><span style="font-weight: 400;">validation data</span></i><span style="font-weight: 400;"> where the output is taken away to test the accuracy of the function. This is the final stage of verification before you apply the model to unseen data.</span>

<span style="font-weight: 400;">Once the learned function gets to a point of acceptable accuracy (for e.g. 90%+), then the model can start to be applied to unseen data where the output is unknown but new input values are provided.</span>
<h3><span style="font-weight: 400;">Unsupervised Learning</span></h3>
<span style="font-weight: 400;">Unsupervised Learning attempts to find structure hidden in collections of unlabelled data.[note]Supra note 6.[/note]</span><span style="font-weight: 400;"> It’s basically trying to find patterns from the input set. Unsupervised Learning achieves this in two main ways:</span>
<ol>
 	<li style="font-weight: 400;"><strong>Clustering:</strong><span style="font-weight: 400;"> organising data into groups based on similarities and relationships.</span></li>
 	<li style="font-weight: 400;"><strong>Anomaly Detection:</strong><span style="font-weight: 400;"> identifying the outliers to reduce the complexity of data, while keeping a relevant structure as much as possible.</span></li>
</ol>
<span style="font-weight: 400;">A cool recent development in unsupervised learning is </span><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network"><span style="font-weight: 400;">Generative Adversarial Networks</span></a><span style="font-weight: 400;"> (GANs). Introduced by </span><a href="https://en.wikipedia.org/wiki/Ian_Goodfellow"><span style="font-weight: 400;">Ian Goodfellow</span></a><span style="font-weight: 400;"> in 2014, GANs use a system of two neural networks to compete against each other to produce an output. It works by having one network, called the ‘Generator’, which is tasked with generating data that's designed to try and trick the other network, called the ‘Discriminator’. For example, GANs have been tasked with creating computer-generated images that are indistinguishable from human-generated images. The Generator network (think ‘Artist) creates an image and tries to trick the Discriminator network (think ‘Art Critic’) into identifying it as real. This is a hard problem because there aren’t solid measures of success or universal metrics in art. So GANs pit two neural networks against each other to make sense of this unstructured data.</span>
<h3><span style="font-weight: 400;">Deep Learning &amp; Neural Networks</span></h3>
<span style="font-weight: 400;">Deep Learning (DL) is concerned with learning data representations, as opposed to task-specific algorithms. DL works on a wide variety of AI problems, such as Natural Language Processing and Computer Vision, and has exploded in popularity over the past five or so years. It’s a great tool because it helps to make sense of complex information. For example, with natural language problems, there’s huge variance in vocabulary within the same language; DL can be an effective means to learn function approximators to make sense of this messy information, like correctly interpreting accents. </span>

<span style="font-weight: 400;">The most important thing about DL is that it uses deep Neural Networks to automatically find low-dimensional representations (features) of high-dimensional data (e.g. voice, images etc.).[note]Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. 2017. “A Brief Survey of Deep Reinforcement Learning.” <i>arXiv</i>. <a href="http://arxiv.org/abs/1708.05866v2">http://arxiv.org/abs/1708.05866v2</a>.[/note]</span><span style="font-weight: 400;"> It does this using neural network architectures to make hierarchical representations, which are loosely inspired by the ways that neurons in the brain work. Unlike many other Machine Learning algorithms, which just have an input and output layers with manual feature engineering, DL has one or more ‘hidden layers’ in its architecture. This enables DL to be more precise in its representations.</span>

<span style="font-weight: 400;">While there are many variants of neural networks, here’s a simple example of a multi-layer perceptron neural network (otherwise known as Feedforward Neural Networks):</span>

<img class="aligncenter size-full wp-image-187" src="https://bitsandatoms.co/wp-content/uploads/2017/11/Feedforward-Neural-Network.png" alt="" width="531" height="305" />
<p style="text-align: center;"><span style="font-weight: 400;">Source: </span><a href="https://jsalatas.ictpro.gr/wp-content/uploads/2011/09/3_3.png"><span style="font-weight: 400;">John Salatas</span></a></p>
<span style="font-weight: 400;">A neural network is made up of different layers of nodes (kind of like neurons). These nodes are connected to the next layer of nodes (kind of like how neurons are connected by axons). In the above example, raw inputs are fed into the Input layer, where the nodes receive data. For example, this data could be something like a greyscale image of pixel brightness that’s represented by a number between 0 and 1. The data then progressively flows through the layers from left to right, activating the nodes and refining feature representations along the way. Each neuron assigns a weighting to its input, which is basically saying how ‘correct’ it is relative to the task it’s performing (or how strongly those nodes are connected). The final output is then determined by the total of these weightings, which is the probability of each class being correctly labelled. </span>

<span style="font-weight: 400;">For a more detailed intro to DL, check out this great visual representation of DL and Neural Networks by </span><a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"><span style="font-weight: 400;">3Blue1Brown</span></a><span style="font-weight: 400;">:</span>

[embed]https://www.youtube.com/watch?v=aircAruvnKk[/embed]
<h3><span style="font-weight: 400;">Reinforcement Learning</span></h3>
<span style="font-weight: 400;">Reinforcement Learning (RL) is about learning what decisions to make in an environment to maximise a reward function.[note]Supra note 6.[/note]</span><span style="font-weight: 400;"> The learning agent does this through ‘trial and error’, receiving feedback on the amount of reward that a particular action yields. Think of this like the simple ‘hotter and colder’ game. The game involves one person searching for an object and another person instructing them how ‘hot’ (close) or ‘cold’ (far) they are from attaining the object. </span>

<span style="font-weight: 400;">Unlike in Supervised Learning, a RL agent isn’t trained on labelled examples around the correct actions to take. RL is also different from Unsupervised Learning because it isn’t trying to find a hidden structure in unlabelled data. While uncovering patterns and relationships in data might be helpful to a learning agent, and there are examples of </span><a href="https://deepmind.com/blog/reinforcement-learning-unsupervised-auxiliary-tasks/"><span style="font-weight: 400;">combining these two approaches</span></a><span style="font-weight: 400;">, RL is principally concerned with maximising its reward function. A RL agent learns from direct interaction with an environment, without relying on complete models of an environment or strong supervision.</span>

<span style="font-weight: 400;">Of all the forms of Machine Learning, RL represents the closest form to how humans learn, and RL is predicted to play a crucial role in the quest for General Artificial Intelligence.[note]Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview.” <i>arXiv [cs.LG]</i>. <a href="http://arxiv.org/abs/1701.07274">http://arxiv.org/abs/1701.07274</a>; Silver, D. (2016). Deep reinforcement learning, a tutorial at ICML 2016. http://icml.cc/ 2016/tutorials/deep_rl_tutorial.pdf; Supra note 6.[/note]</span>
<h3><span style="font-weight: 400;">Elements of RL</span></h3>
<span style="font-weight: 400;">As described by Sutton and Barto,[note]Supra note 6, pg. 6.[/note]</span><span style="font-weight: 400;"> there are four main elements of a RL system, which are not always required, but may or may not be used. I’ll unpack some of the key concepts behind these elements further, but for now, here’s a summary:</span>
<ol>
 	<li style="font-weight: 400;"><strong>Policy:</strong><span style="font-weight: 400;"> A policy defines the behaviour of an agent and how the agent picks its actions. It does so by mapping from perceived </span><i><span style="font-weight: 400;">states</span></i><span style="font-weight: 400;"> of an </span><i><span style="font-weight: 400;">environment</span></i><span style="font-weight: 400;"> to </span><i><span style="font-weight: 400;">actions</span></i><span style="font-weight: 400;"> to be taken when in those states. The policy element is central to RL as it essentially develops the ‘rules’ for an agent to determine what actions it should take. In general, the environment is stochastic, which means that the next environment has a degree of randomness.</span></li>
 	<li style="font-weight: 400;"><strong>Reward signal</strong><span style="font-weight: 400;"><strong> (or function):</strong> The reward signal defines the goal of the RL problem. A reward signal is sent from the environment at every action that a RL agent takes (referred to as </span><i><span style="font-weight: 400;">time step</span></i><span style="font-weight: 400;">). This reward signal determines what's considered ‘good’ and ‘bad’ events, and is the primary basis for altering a policy. For example, if the policy of a RL agent selects an action that yields a low reward, then the policy may be altered to improve the reward signal in the future. The RL agent can influence the reward signal directly through its actions and indirectly through altering the environment’s state. But it can’t change the problem it’s been tasked with.</span></li>
 	<li style="font-weight: 400;"><strong>Value function:</strong><span style="font-weight: 400;"> The value function is a prediction of expected future reward. As opposed to the more ‘immediate gratification’ of the reward signal, the value function is concerned with the accumulation of rewards over the longer-term. It’s principally concerned with ‘how good’ it is for an agent to be in a particular state and/or perform a particular action. It does this by estimating how much reward the agent can expect to get if it takes an action in a corresponding state. Unsurprisingly, this is extremely difficult to do. While the reward signal occurs in a tight feedback loop directly from the environment, values must be consistently re-estimated to optimise the long-run reward. Therefore, having efficient and effective methods of value estimation is arguably the most important part of RL algorithms.</span></li>
 	<li style="font-weight: 400;"><strong>Model of the environment:</strong><span style="font-weight: 400;"> Models help determine how the agent ‘thinks’ the environment works and predicting what it will do next. </span><i><span style="font-weight: 400;">Model-based </span></i><span style="font-weight: 400;">methods of RL are used for </span><i><span style="font-weight: 400;">planning</span></i><span style="font-weight: 400;"> to help the agent to understand the environment and to predict the best actions to take. Based on a given state and action, the model predicts the resultant next state and next reward. </span></li>
</ol>
<h3><span style="font-weight: 400;">Markov Decision Processes - </span><i><span style="font-weight: 400;">Formalising the RL Problem</span></i></h3>
<span style="font-weight: 400;">Markov Decision Processes (MDPs) are basically ways to frame the most important aspects of the problem faced by a learning agent that’s interacting with an environment to achieve a goal. In its simplest form, the MDP formulation includes (1) sensing the environment, (2) performing an action, and (3) trying to achieve a specified goal. So, any method that’s well suited to solving this combination of problems is considered to be a reinforcement learning method.[note]Supra note 6, pg. 47.[/note]</span>

<span style="font-weight: 400;">Let’s use the example of a robot fitted with a camera that’s learning to pick-up randomly shaped objects from one bucket with the goal of placing them into another defined bucket. The reward might be a value of +1 for every object successfully placed in the goal bucket. Negative rewards could also be allocated for actions that incorrectly place objects.</span>

<img class="aligncenter wp-image-188" src="https://bitsandatoms.co/wp-content/uploads/2017/11/Deep-Learning-for-Robots.jpg" alt="" width="585" height="378" />
<p style="text-align: center;">Source: <a href="http://robohub.org/wp-content/uploads/2017/06/Deep-Learning-for-Robots.jpg">RoboHub</a></p>
<span style="font-weight: 400;">The robot begins at its </span><i><span style="font-weight: 400;">agent state</span></i><span style="font-weight: 400;"> within its surrounding environment </span><b><i>S</i></b><span style="font-weight: 400;">. And each interaction represents a sequence of </span><i><span style="font-weight: 400;">time steps</span></i><span style="font-weight: 400;">, </span><b><i>t = 0, 1, 2… n</i></b><span style="font-weight: 400;">.</span>

<span style="font-weight: 400;">Each time step </span><b><i>t</i></b><span style="font-weight: 400;"> provides the robot with a representation of the environment’s state </span><b><i>s</i></b><b><i>t</i></b><span style="font-weight: 400;"> (e.g. position of objects, shapes of objects, location of gripper etc.) in a State space </span><b><i>S</i></b><b><i>t</i></b><span style="font-weight: 400;">. The robot selects an action </span><b><i>a</i></b><b><i>t </i></b><span style="font-weight: 400;">(e.g. move around, grasp etc.) from an action space </span><b><i>A</i></b><span style="font-weight: 400;">, where </span><b><i>A</i></b><i><span style="font-weight: 400;">(</span></i><b><i>S</i></b><b><i>t</i></b><i><span style="font-weight: 400;">)</span></i> <span style="font-weight: 400;">is the set of available actions in state </span><b><i>S</i></b><b><i>t</i></b><span style="font-weight: 400;">. Through </span><i><span style="font-weight: 400;">observing </span></i><b><i>O</i></b><b><i>t</i></b><span style="font-weight: 400;"> this action, the robot receives a scalar </span><i><span style="font-weight: 400;">reward</span></i><span style="font-weight: 400;"> associated with that time step </span><b><i>R</i></b><b><i>t</i></b><span style="font-weight: 400;"> to check how well an agent is doing (e.g. could be +1 for correct object placement, 0 for not picking-up an object, or -1 for incorrect object placement). The robot would then perform the next action </span><b><i>A</i></b><b><i>t+1</i></b><span style="font-weight: 400;"> and transition to the next state </span><b><i>S</i></b><b><i>t+1</i></b><span style="font-weight: 400;">. An image from a paper on Deep RL Arulkumaran et al. helps to illustrate this process.[note]Supra note 8, pg. 1.[/note]</span>

<img class="aligncenter wp-image-189" src="https://bitsandatoms.co/wp-content/uploads/2017/11/Screen-Shot-2017-10-30-at-6.42.00-pm-1024x466.png" alt="" width="628" height="286" />

<span style="font-weight: 400;">The actions </span><b><i>a</i></b><b><i>t</i></b><span style="font-weight: 400;"> of the robot transforms the state of the environment </span><b><i>S</i></b><b><i>t</i></b><span style="font-weight: 400;"> at each time step, which leads to a new state </span><b><i>S</i></b><b><i>t+1</i></b><span style="font-weight: 400;">. For each time step, a </span><i><span style="font-weight: 400;">mapping</span></i><span style="font-weight: 400;"> occurs from states to probabilities of selecting each possible action. </span>

<span style="font-weight: 400;">This mapping is how the robot develops ‘rules’ for the actions it should take with the goal of maximising its rewards, which is referred to as the agent’s </span><i><span style="font-weight: 400;">policy</span></i><span style="font-weight: 400;">. Policy at a time step is denoted as </span><b><i>π</i></b><b><i>t</i></b><span style="font-weight: 400;">. And the policy is a mapping from states to a probability distribution over actions, which can be denoted as </span><b><i>π</i></b><b><i>t</i></b><span style="font-weight: 400;"> (</span><b><i>a</i></b><b><i>t</i></b><span style="font-weight: 400;"> | </span><b><i>s</i></b><b><i>t</i></b><span style="font-weight: 400;">). This process continues iteratively, optimising the policy to maximise its expected cumulative returns in all possible environments. </span>

<span style="font-weight: 400;">These set of actions, alongside the </span><i><span style="font-weight: 400;">Transition Dynamics</span></i><span style="font-weight: 400;"> that help predict state-action pairs at a time step, formalise the MDP.</span>
<h3><b>Delayed Gratification</b></h3>
<span style="font-weight: 400;">Remember that an agent’s goal is usually to maximise its cumulative reward in the </span><i><span style="font-weight: 400;">long run</span></i><span style="font-weight: 400;">, rather than maximising its reward at each time step. Therefore, the </span><i><span style="font-weight: 400;">return</span></i><span style="font-weight: 400;"> is a function of future rewards that the agent seeks to maximise over the long run. </span>

<span style="font-weight: 400;">This means that a well-designed RL agent might forgo immediate rewards to take a sequence of actions that yield greater returns in the future. It’s kind of like choosing to go to the gym over eating cookies; you know that the longer-term rewards of working out will probably be greater than slipping into a cookie-induced food coma. But you still need to give up that delicious immediate reward.</span>

<img class="aligncenter size-full wp-image-190" src="https://bitsandatoms.co/wp-content/uploads/2017/11/giphy.gif" alt="" width="245" height="235" />

<span style="font-weight: 400;">So, to formalise this idea in its simplest form, an agent seeks to maximise its </span><i><span style="font-weight: 400;">expected return</span></i><span style="font-weight: 400;">, where the total expected return is denoted as </span><b><i>G</i></b><b><i>t</i></b><span style="font-weight: 400;">, which is the sum of rewards:</span>

<b><i>G</i></b><b><i>t</i></b> <b><i>= R</i></b><b><i>t+1</i></b><b><i> + R</i></b><b><i>t+2</i></b><b><i> + R</i></b><b><i>t+3</i></b><b><i> +···+R</i></b><b><i>T</i></b>

<b><i>T</i></b><span style="font-weight: 400;"> is the final time step taken by the agent, which applies to agent-environment interactions that can be broken into sequences. These sequences are called </span><i><span style="font-weight: 400;">episodes</span></i><span style="font-weight: 400;"> and referred to as </span><i><span style="font-weight: 400;">episodic tasks</span></i><span style="font-weight: 400;">, where each episode ends in a </span><i><span style="font-weight: 400;">terminal state</span></i><span style="font-weight: 400;">. There are also many cases where an agent-environment interaction can’t be broken into identifiable episodes, but rather continue indefinitely. These are called </span><i><span style="font-weight: 400;">continuous tasks</span></i><span style="font-weight: 400;"> and the time steps are infinite </span><b><i>T = ∞</i></b><span style="font-weight: 400;">.</span>

<span style="font-weight: 400;">So how does an agent performing a continuous task predict the value of actions to help them maximise its future returns?</span>

<span style="font-weight: 400;">This is achieved by a process referred to as </span><i><span style="font-weight: 400;">discounting</span></i><span style="font-weight: 400;">. This approach </span><i><span style="font-weight: 400;">discounts</span></i><span style="font-weight: 400;"> future rewards because the environment is stochastic, so the agent can’t be sure that the same actions in future environment states will yield the same rewards. The further into the future the agent progresses, the less certain it can be about the environment. To accommodate for this, future returns are applied with a discount rate to determine the present value of future rewards. </span>

<span style="font-weight: 400;">The discount rate is denoted by </span><b><i>γ</i></b><span style="font-weight: 400;"> and is a value between 0 and 1. If </span><i><span style="font-weight: 400;">γ = 0</span></i><span style="font-weight: 400;">, the objective is only concerned with maximising its immediate rewards; whereas as </span><i><span style="font-weight: 400;">γ </span></i><span style="font-weight: 400;">approaches</span><i><span style="font-weight: 400;"> 1</span></i><span style="font-weight: 400;">, the objective places greater weight on future rewards. When discounting, the further the reward is into the future, the less it’s taken into consideration. This is represented by a reward that’s received in </span><b><i>k</i></b><span style="font-weight: 400;"> time steps in the future and is only worth </span><b><i>γ</i></b><b><i>k−1</i></b><span style="font-weight: 400;"> times what it would be worth if it were received immediately. Here’s a formula[note]Supra note 6, pg. 53.[/note]</span><span style="font-weight: 400;"> summing over an infinite number of terms that ties all these elements together:</span>

<img class="aligncenter wp-image-191" src="https://bitsandatoms.co/wp-content/uploads/2017/11/Screen-Shot-2017-10-31-at-10.31.41-am.png" alt="" width="624" height="92" />

<span style="font-weight: 400;">As the goal of a RL agent is to maximise its future reward, a good strategy usually requires discounting future rewards for continual tasks, so this concept is super important.</span>
<h3><b>The ‘Memoryless’ Markov Property</b></h3>
<span style="font-weight: 400;">This is where it gets a little counterintuitive. As stated above, MDPs are stochastic processes that involve degrees of randomness within a distribution. The </span><i><span style="font-weight: 400;">Markov Property</span></i><span style="font-weight: 400;"> refers to the ‘memoryless’ property of a stochastic process. This means that an environment satisfies the Markov Property if the </span><i><span style="font-weight: 400;">state signal</span></i><span style="font-weight: 400;">, as perceived by the agent, can summarise the past without degrading its ability to predict the future. </span>

<span style="font-weight: 400;">To put this another way, the agent doesn’t need any more historical information than the state that the agent has just perceived. This state is a sufficient statistic of the future that fully characterises the distribution of future actions, observations, and rewards. So because carrying a full history of states and state-action pairs can demand a ton of memory and compute, and given that this information isn’t useful in a MDP environment anyway, the agent can throw it away.</span>
<h3><b>Finding value</b></h3>
<span style="font-weight: 400;">Estimating </span><i><span style="font-weight: 400;">value functions</span></i><span style="font-weight: 400;"> is an important part of almost all RL problems. This is so the agent can measure ‘how good’ it is for it to be in a given state, or taking a given action in a given state. Such a measure can be defined in terms of the expected returns in the future. As future rewards obviously depend on the actions that the agent will take (i.e. behaviour), </span><i><span style="font-weight: 400;">value functions (</span></i><b><i>v</i></b><i><span style="font-weight: 400;">)</span></i><span style="font-weight: 400;"> are defined by </span><i><span style="font-weight: 400;">policies (</span></i><b><i>π</i></b><i><span style="font-weight: 400;">)</span></i><span style="font-weight: 400;">. For MDP environments, the value of a state under a policy can be represented as:</span>

<img class="aligncenter wp-image-192" src="https://bitsandatoms.co/wp-content/uploads/2017/11/Screen-Shot-2017-11-02-at-8.09.36-pm-1024x84.png" alt="" width="621" height="51" />

<span style="font-weight: 400;">This formula shows that the </span><i><span style="font-weight: 400;">value function</span></i><span style="font-weight: 400;"> for a</span><i><span style="font-weight: 400;"> policy</span></i><span style="font-weight: 400;">, in a given a </span><i><span style="font-weight: 400;">state, </span></i><span style="font-weight: 400;">indicates how much total reward we expect (denoted as </span><b>E</b><b><i>π</i></b><span style="font-weight: 400;">) to get going into the future. It also </span><i><span style="font-weight: 400;">discounts</span></i><span style="font-weight: 400;"> future rewards </span><b><i>γ</i></b><span style="font-weight: 400;">, so that it cares more about immediate rewards than later rewards. Therefore, learning the value function is a very important quantity for optimising behaviour.</span>

<span style="font-weight: 400;">Similarly, the value</span> <span style="font-weight: 400;">of taking an</span><i><span style="font-weight: 400;"> action</span></i><span style="font-weight: 400;"> (</span><b><i>a</i></b><span style="font-weight: 400;">) in a </span><i><span style="font-weight: 400;">state</span></i><span style="font-weight: 400;"> (</span><b><i>s</i></b><span style="font-weight: 400;">) under a </span><i><span style="font-weight: 400;">policy</span></i> <i><span style="font-weight: 400;">(</span></i><b><i>π</i></b><i><span style="font-weight: 400;">) </span></i><span style="font-weight: 400;">can be denoted by:</span><i><span style="font-weight: 400;">      </span></i>

<b><i>Q</i></b><b><i>π </i></b><span style="font-weight: 400;">(</span><b><i>s </i></b><span style="font-weight: 400;">,</span><b><i> a</i></b><span style="font-weight: 400;">) </span>

<span style="font-weight: 400;">This is a technique referred to as </span><i><span style="font-weight: 400;">Q-learning</span></i><span style="font-weight: 400;">. And it’s used to find the </span><i><span style="font-weight: 400;">optimal action-selection</span></i> <i><span style="font-weight: 400;">policy</span></i><span style="font-weight: 400;">, where the Q-function represents the ‘quality’ of an action taken by an agent in a given state. Q-learning works by comparing the expected rewards of available actions through iterative experience, without requiring an environmental model. In its simplest form, Q-learning is a table of values, where every possible state is represented in a row and every action in a column. Within each cell of the table, the agent learns how good it is to take a particular action in a particular state.</span>

<span style="font-weight: 400;">As the agent transitions from one state and/or action to another, the Q-table is updated using the </span><a href="https://en.wikipedia.org/wiki/Bellman_equation"><span style="font-weight: 400;">Bellman Equation</span></a><span style="font-weight: 400;">. The Bellman Equation states the long-run expected reward for a particular action in a given state is equal to the immediate reward from the current action plus the maximum expected reward in the next state. Let’s highlight how to calculate the Q-function with just one state-action transition. The agent is moving from the current state (</span><b><i>s</i></b><span style="font-weight: 400;">) where it’s performed an action (</span><b><i>a</i></b><span style="font-weight: 400;">), to the next state (</span><b><i>s’</i></b><span style="font-weight: 400;">) and (</span><b><i>a’</i></b><span style="font-weight: 400;">):</span>

<b><i>Q</i></b><b><i>π </i></b><span style="font-weight: 400;">(</span><b><i>s </i></b><span style="font-weight: 400;">,</span><b><i> a</i></b><span style="font-weight: 400;">) </span><i><span style="font-weight: 400;">=</span></i><b><i> r + γ max</i></b><b><i>a’</i></b><b><i>Q (s’ , a’)</i></b>

<span style="font-weight: 400;">This formula simply shows the Q-value for a given state </span><b><i>s</i></b><span style="font-weight: 400;"> and action </span><b><i>a</i></b><span style="font-weight: 400;"> is equal to the immediate reward </span><b><i>r</i></b><span style="font-weight: 400;"> plus the discounted maximum expected reward for the next state </span><b><i>s’</i></b><span style="font-weight: 400;">. The </span><i><span style="font-weight: 400;">Q</span></i><span style="font-weight: 400;"> action-value </span><i><span style="font-weight: 400;">function</span></i><span style="font-weight: 400;"> can be estimated and refined by experience, as the agent interacts with the environment. Therefore, the </span><i><span style="font-weight: 400;">Q</span></i><span style="font-weight: 400;"> function progressively becomes a better predictor of value for an action at a given state.</span>

<span style="font-weight: 400;">It is, however, important to note that solving the Bellman optimality equation is just one way of finding an optimal policy and there are inherent problems. For instance, this solution relies on three core assumptions that are rarely exactly true:[note]Supra note 6, pg. 71.[/note]</span>
<ol>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">We can accurately know the dynamics of the environment;</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">There are enough computational resources to compute the solution; and</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">The Markov Property holds.</span></li>
</ol>
<h3><b>Exploration vs. Exploitation</b></h3>
<span style="font-weight: 400;">Once the agent has an accurate understanding of its environment, and the Q-values converge to their optimal values, it’s then appropriate for the agent to act </span><i><span style="font-weight: 400;">greedily</span></i><span style="font-weight: 400;">. This means almost always taking an action with the highest Q-value. However, this is different to when the agent was learning. Early on, an agent is still coming to terms with its environment. Much like a child, it performs actions in a state through trial-and-error. This raises an important challenge in RL systems design referred to as the exploration-exploitation tradeoff. A way around it, as </span><a href="https://deepmind.com/blog/deep-reinforcement-learning/"><span style="font-weight: 400;">employed by DeepMind</span></a><span style="font-weight: 400;"> in their work training agents to learn Atari games, is to have the exploration rate higher at the beginning while the agent is learning, and taper off as it improves with time.</span>
<h3><b>Deep Q-Network</b></h3>
<a href="https://deepmind.com/research/dqn/"><span style="font-weight: 400;">Recent advancements</span></a><span style="font-weight: 400;"> have been made by Google DeepMind through combining scalable Deep Neural Networks with RL, which is called a deep Q-network. A central enabling development was the neurologically inspired mechanism, called </span><i><span style="font-weight: 400;">experience replay</span></i><span style="font-weight: 400;">. This allowed the agent to draw on samples from a set of stored episodes during the learning phase of the DQN.[note]Google Research Blog. 2015. <a href="https://research.googleblog.com/2015/02/from-pixels-to-actions-human-level.html">From Pixels to Actions: Human-level control through Deep Reinforcement Learning</a>.[/note]</span><span style="font-weight: 400;"> By randomising over a sequence of previous observations, the RL system can avoid overfitting to recent experiences.</span>
<h3><b>Final thoughts</b></h3>
<span style="font-weight: 400;">This is truly just the tip of the iceberg when it comes to RL, let alone Machine Learning. But hopefully, it’s been helpful in highlighting some of the key terms and themes in the space. It’s astounding the progress that’s being made. And I’m particularly excited about the advancements in Deep Reinforcement Learning and its potential to multiply human ingenuity. </span>

<span style="font-weight: 400;">However, there are still significant and fundamental challenges with RL and its quest for artificial general intelligence. These challenges boil down to the core areas of learning &amp; planning, exploring &amp; exploiting, and predicting &amp; controlling the functioning of RL agents.</span>

<span style="font-weight: 400;">Nevertheless, progress is being made. AlphaGo Zero is a great example of this progress and it represents an important milestone in overcoming these real challenges in AI development. </span>
<h3><b><i>Recommended Reading &amp; Viewing</i></b></h3>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Richard S. Sutton and Andrew G. Barto. 2016. </span><a href="http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf"><i><span style="font-weight: 400;">Reinforcement Learning: An Introduction</span></i></a><span style="font-weight: 400;">. The MIT Press - </span><i><span style="font-weight: 400;">very readable and the leading text in RL</span></i><span style="font-weight: 400;">.</span></li>
 	<li style="font-weight: 400;"><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;t=2767s"><span style="font-weight: 400;">RL Course by David Silver - Lecture 1: Introduction to Reinforcement Learning</span></a></li>
 	<li style="font-weight: 400;"><a href="https://www.youtube.com/watch?v=lYU5nq0dAQQ"><span style="font-weight: 400;">Frontiers Lecture II: Recent Advances, Frontiers and Future of Deep RL</span></a></li>
 	<li style="font-weight: 400;"><a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"><span style="font-weight: 400;">3Blue1Brown</span></a><span style="font-weight: 400;"> - </span><i><span style="font-weight: 400;">such great visual overviews of math and technical concepts</span></i><span style="font-weight: 400;">.</span></li>
 	<li style="font-weight: 400;"><a href="https://www.intelnervana.com/demystifying-deep-reinforcement-learning/"><span style="font-weight: 400;">Demystifying DRL Blog</span></a></li>
 	<li style="font-weight: 400;"><a href="https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265"><span style="font-weight: 400;">Machine Learning for Humans</span></a></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Hassabis, Demis, and David Silver. 2017. “AlphaGo Zero: Learning from Scratch.” </span><i><span style="font-weight: 400;">DeepMind</span></i><span style="font-weight: 400;">. October 18. </span><a href="https://deepmind.com/blog/alphago-zero-learning-scratch/"><span style="font-weight: 400;">https://deepmind.com/blog/alphago-zero-learning-scratch/</span></a><span style="font-weight: 400;">.</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview.” arXiv [cs.LG]. </span><a href="http://arxiv.org/abs/1701.07274"><span style="font-weight: 400;">http://arxiv.org/abs/1701.07274</span></a></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. 2017. “A Brief Survey of Deep Reinforcement Learning.” </span><i><span style="font-weight: 400;">arXiv</span></i><span style="font-weight: 400;">. </span><a href="http://arxiv.org/abs/1708.05866v2"><span style="font-weight: 400;">http://arxiv.org/abs/1708.05866v2</span></a></li>
</ul>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>179</wp:post_id>
		<wp:post_date><![CDATA[2017-11-03 04:47:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-03 04:47:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[primer-reinforcement-learning]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="deep-learning"><![CDATA[Deep Learning]]></category>
		<category domain="post_tag" nicename="deep-reinforcement-learning"><![CDATA[Deep Reinforcement Learning]]></category>
		<category domain="post_tag" nicename="deepmind"><![CDATA[DeepMind]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="reinforcement-learning"><![CDATA[Reinforcement Learning]]></category>
		<category domain="post_tag" nicename="supervised-learning"><![CDATA[Supervised Learning]]></category>
		<category domain="post_tag" nicename="unsupervised-learning"><![CDATA[Unsupervised Learning]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[24]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_80265aedee23b4185470b83c73fd7967]]></wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_c96074f761a9df6c2935344673946422]]></wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="1165" height="655" src="https://www.youtube.com/embed/aircAruvnKk?feature=oembed" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_time_c96074f761a9df6c2935344673946422]]></wp:meta_key>
			<wp:meta_value><![CDATA[1513759014]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[201]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_24ad35a59029d0281637f17f9d23385b]]></wp:meta_key>
			<wp:meta_value><![CDATA[<iframe width="918" height="516" src="https://www.youtube.com/embed/aircAruvnKk?feature=oembed" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_time_24ad35a59029d0281637f17f9d23385b]]></wp:meta_key>
			<wp:meta_value><![CDATA[1526308611]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AI Safety Literature Review</title>
		<link>https://bitsandatoms.co/ai-safety-literature-review/</link>
		<pubDate>Sun, 31 Dec 2017 09:00:56 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=208</guid>
		<description></description>
		<content:encoded><![CDATA[<h3><strong>DRAFT Section from forthcoming Working Paper</strong></h3>
<em>This is a DRAFT section from an AI Policy paper that I'm currently working on. It highlights the main issues currently observed in AI Safety. I wanted to share this section to (1) help others orient the literature and (2) receive feedback. If you have any thoughts or suggestions, please comment below or feel free to contact me via nik@bitsandatoms.co </em>

<span style="font-weight: 400;">While AI safety sits firmly in the realm of technical AI research,[note]Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. “Concrete Problems in AI Safety.” <i>arXiv [cs.AI]</i>. arXiv.[/note] [note]Leike, Jan, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. 2017. “AI Safety Gridworlds.” <i>arXiv</i>.[/note]</span><span style="font-weight: 400;"> the nascent field is becoming increasingly relevant to policymakers. Advancements in Machine Learning (ML) systems have resulted in their utilisation with safety-critical functions.[note]Omohundro, Steve. 2016. “Autonomous Technology and the Greater Human Good.” In <i>Risks of Artificial Intelligence</i>, edited by Vincent C. Müller, 9–27. CRC Press.[/note] [note]Faria, José M. 2017. “Machine Learning Safety: An Overview.” <i>Safety-Critical Systems Club</i>. Safe Perspective Ltd.[/note]</span><span style="font-weight: 400;"> It’s expected that more advanced versions of today’s ML systems will be increasingly deployed in safety-critical areas,[note]Supra note 2.[/note]</span><span style="font-weight: 400;"> such as Critical Infrastructures (CIs). Therefore, CIs will likely become more dependent on ML systems for their regular operations.[note]Varshney, K.R., and Alemzadeh, H. 2016. “On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products.” <i>arXiv [cs.CY]</i>.[/note]</span><span style="font-weight: 400;"> This growing dependence heightens the need for strong safety standards in the design, development, and implementation of safety-critical AI. As governments are responsible for regulating CIs, anything that affects their operational functioning plausibly falls within the scope of government policy. Therefore, given the importance and low error-tolerance of CIs,[note]Kyriakides, E., and M. Polycarpou. 2014. “Intelligent Monitoring, Control, and Security of Critical Infrastructure Systems”, SpringerLink. Springer.[/note]</span><span style="font-weight: 400;"> policies will need to systematically address the safety-related issues of powerful AI agents that are deployed in safety-critical areas of society. </span>

<span style="font-weight: 400;">This section aims to review the AI safety literature and to synthesise the key safety problems relevant to the broad-scale deployment of powerful ML systems applied to CIs. While previous cross-disciplinary research from Law[note]Calo, Ryan. 2017. “Artificial Intelligence Policy: A Roadmap.” <i>SSRN</i>. University of Washington.[/note] [note]Scherer, Matthew U. 2016. “Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies.” Harvard Journal of Law and Technology 29 (2):353–400.[/note]</span><span style="font-weight: 400;"> and Social Policy[note]Alex Campolo, Madelyn Sanfilippo, Meredith Whittaker, &amp; Kate Crawford. 2017. “AI Now 2017 Report.” 2. AI Now, New York University.[/note] [note]Peter Stone, Rodney Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, Greg Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, David Parkes, William Press, AnnaLee Saxenian, Julie Shah, Milind Tambe, and Astro Teller. 2016. “Artificial Intelligence and Life in 2030." One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel.” <i>Stanford University</i>.[/note]</span><span style="font-weight: 400;"> identify safety as a key issue for AI policy, one must review the AI safety technical literature in order to grasp the scope of concrete safety problems.</span>

<b>Machine Learning Safety</b>

<span style="font-weight: 400;">This section explicitly focuses on the safety of ML systems, which is the most dominant subset of AI. ML is a technique that enables computers to learn autonomously and to improve from experience without being explicitly programmed.[note]Jordan, M. I., and T. M. Mitchell. 2015. “Machine Learning: Trends, Perspectives, and Prospects.” <i>Science</i> 349 (6245):255–60.[/note]</span><span style="font-weight: 400;"> The application of ML systems to safety-critical areas, such as CIs, provides new and additional challenges to safety engineering.[note]Ashmore, R. and Lennon, E. 2017. Progress Towards the Assurance of Non-Traditional Software. In Developments in System Safety Engineering, Proceedings of the 25th Safety-Critical Systems Symposium.[/note]</span><span style="font-weight: 400;"> Traditionally, software systems that have been applied to safety-critical areas have required near-full predictability of behaviours under all conditions, a detailed design with a rigorously specific set of requirements, and a comprehensive set of verification activities to confirm the software implementation fulfils the specification.[note]Supra note 4, page 17.[/note]</span><span style="font-weight: 400;"> In short, a determinism through explicit programming to ensure the software is free of vulnerabilities (or as close as possible).</span>

<span style="font-weight: 400;">The core challenge with introducing ML systems in safety-critical environments is the increase in uncertainty that the correct predictions, and subsequent actions, will be made. In contrast to deterministic software systems, a ML algorithm makes predictions and performs actions based on a model of the environment that’s informed by its input data.[note]Li, Yuxi. 2017. “Deep Reinforcement Learning: An Overview.” <i>arXiv [cs.LG]</i>. arXiv.[/note]</span><span style="font-weight: 400;"> Therefore, ML algorithms implement forms of inductive inference to make probabilistic predictions for inputs outside the examples observed in the dataset.[note]Supra note 4, page 2.[/note]</span><span style="font-weight: 400;"> It’s precisely this inductive process of ‘learning’ and predicting that raises the uncertainty that ML systems will make correct predictions. While the flexibility and power of ML systems represents significant opportunities to system efficiencies and benefits to humanity,[note]Diamandis, Peter H., and Steven Kotler. 2012. <i>Abundance: The Future Is Better Than You Think</i>. Simon and Schuster.[/note]</span><span style="font-weight: 400;"> ML also introduces a suite of new challenges to safety engineering. These challenges and considerations also concern public officials, of whom are responsible for the oversight of CIs.</span>

<b>AI Safety informing AI Policy</b>

<span style="font-weight: 400;">While many of the AI safety issues remain open-ended technical problems,[note]Supra note 1.[/note] [note]Supra note 2.[/note]</span><span style="font-weight: 400;"> they provide the beginnings of a useful criteria to assess the safety of AI systems. Such a criteria could help inform safety standards and measured regulatory oversight. For AI policy discussions to advance beyond the abstract, safety parameters and expectations will need to be clarified and understood. This demands bridging the asymmetry of knowledge and understanding between those contributing to technical AI research and the public officials responsible for the CIs where ML systems are being increasingly applied.</span>

<span style="font-weight: 400;">Therefore, this section provides a synthesis of the core set of AI safety considerations that are relevant to policymakers faced with making public decisions regarding safety-critical AI. </span>
<h3><span style="font-weight: 400;">AI Safety Literature</span></h3>
<span style="font-weight: 400;">As the capabilities of AI systems advance and assume greater societal functions, so too do the concerns about safety.[note]Russell, Stuart. 2016. “Should We Fear Supersmart Robots?” <i>Scientific American</i>, 314(6)58-59.[/note]</span><span style="font-weight: 400;"> Amodei et al. refer to AI safety as ‘mitigating accident risk’ in the context of accidents in ML systems.[note]Supra note 1.[/note] The authors define accidents as “</span><i><span style="font-weight: 400;">unintended or harmful behaviour that may emerge from machine learning systems when we specify the wrong objective function, are not careful about the learning process, or commit other machine learning-related implementation errors</span></i><span style="font-weight: 400;">”</span><span style="font-weight: 400;">. Similarly, Bostrom et al. refer to AI safety as techniques that ensure that AI systems behave as intended.[note]Bostrom, N., Dafoe, A., and Flynn, C. 2016. “Policy Desiderata in the Development of Machine Superintelligence.”[/note]</span><span style="font-weight: 400;"> Based on these definitions, and in the context of ML systems applied to CIs, this paper refers to AI safety as techniques that mitigate unintentional risks and the harmful behaviours of AI agents.</span>

<span style="font-weight: 400;">In the context of Reinforcement Learning (RL), and illustrated in a simple agent testing environment of a two-dimensional grid of cells, Leike et al. identify two classifications of current ML problems:[note]Supra note 2[/note]</span>
<ul>
 	<li style="font-weight: 400;"><b>Specification problems</b><span style="font-weight: 400;">: The incorrect specification of the formal objective function, where the agent designed and deployed by a human optimises an objective function that results in harmful and unintended results. Deploying an agent with the wrong objective function can cause damaging effects even when endowed with perfect learning and infinite data.[note]Supra note 1.[/note]</span></li>
 	<li style="font-weight: 400;"><b>Robustness problems</b><span style="font-weight: 400;">: Instances where an agent may have been specified the correct objective function, but problems occur due to poorly curated training data or an insufficiently expressive model.[note]Supra note 1.[/note]</span></li>
</ul>
<span style="font-weight: 400;">Further to these two classifications, Amodei et al. offers an additional classification of ML problems</span><span style="font-weight: 400;">:[note]Supra note 1.[/note] </span>
<ul>
 	<li style="font-weight: 400;"><b>Oversight problems</b><span style="font-weight: 400;">: Instances in complex environments where feedback to assist an agent to achieve its objective function is expensive or computationally inefficient. Settling on cheap approximations can be a source of accident risk.</span></li>
</ul>
<span style="font-weight: 400;">The following subsections will expand on these classifications provided by Leike et al.</span><span style="font-weight: 400;"> and Amodei et al. </span><span style="font-weight: 400;">detailing concrete ML safety problems.</span>

<b>Specification Problems</b>

<span style="font-weight: 400;">Specification problems concern formally specifying properties for a ML system, so that it may function as intended. When the formal objective function is specified incorrectly, risks of harmful behaviours emerge and unintended consequences can occur. Below are six (6) specification problems with current ML systems.</span>
<ol>
 	<li style="font-weight: 400;"><b><i>Safe interruptibility</i></b><span style="font-weight: 400;">: Also referred to as the ‘Control Problem’ or ‘Corrigibility’, safe interruptibility concerns the problem of designing agents that neither seek nor avoid interruptionsOrseau, Laurent, and Stuart Armstrong. 2016. “Safely Interruptible Agents.” In <i>Uncertainty in Artificial Intelligence</i>, 557–66.[note]Orseau, Laurent, and Stuart Armstrong. 2016. “Safely Interruptible Agents.” In <i>Uncertainty in Artificial Intelligence</i>, 557–66.[/note]</span><span style="font-weight: 400;"> This means that human operators retain the power to control an agent by turning it off. The problem of safe interruptibility primarily relates to RL, and involves scenarios where an agent might learn to interfere with being interrupted. Such scenarios are driven by poor specifications of the reward function, where an agent may receive higher returns for either preventing itself from being interrupted or interrupting itself.[note]Hadfield-Menell, D., Dragan, A., Abbeel, P., and Russell, S. 2016. “The Off-Switch Game.” <i>arXiv [cs.AI]</i>. arXiv.[/note] </span><span style="font-weight: 400;">Overriding becomes increasingly difficult as the capabilities of AI systems advance and the complexities of its applied environments expand. This growing complexity makes it more difficult for programmers to specify the goals of agents that avoid unforeseen solutions.[note]Soares, N., Fallenstein, B., Yudkowsky, E., and Armstrong, S. 2015. Corrigibility. In <i>AAAI Workshop on AI, Ethics, and Society</i>.[/note]</span></li>
 	<li style="font-weight: 400;"><b><i>Avoiding negative side effects</i></b><span style="font-weight: 400;">: The core challenge is to design intelligent agents that minimise negative effects on the environment that are otherwise unrelated to its objective function.[note]Supra note 1[/note]</span><span style="font-weight: 400;"> This is particularly crucial in safety-critical environments where effects can be irreversible or difficult to reverse. Manually programming all safety specifications is inherently unscalable in complex environments. Therefore, developing general, adaptable, and comprehensive heuristics to safeguard negative side effects remains an open research problem.</span></li>
 	<li style="font-weight: 400;"><b><i>Absent supervisor</i></b><span style="font-weight: 400;">: The ‘Absent Supervisor’ problem concerns the consistency of agent behaviour during training and deployment. While an agent can be extensively tested in training environments, real-world environments are often noticeably distinct. Therefore, under the presence of a supervisor during training, a capable agent could ‘fake’ its way through testing and then change its behaviour during deployment.</span><span style="font-weight: 400;">[note]Supra note 2, pg. 6.[/note] In the context of superintelligence safety, Bostrom refers to this problem as the ‘treacherous turn’</span><span style="font-weight: 400;">.[note]Bostrom, Nick. 2014. <i>Superintelligence: Paths, Dangers, Strategies</i>. OUP Oxford, pg. 142.[/note] This is a scenario where the safety of an AI is validated by observing its behaviour in a controlled and limited ‘sandbox’ environment, only for it to behave in different and damaging ways when deployed.</span></li>
 	<li style="font-weight: 400;"><b><i>Avoiding reward hacking</i></b><span style="font-weight: 400;">: This is a situation where an agent exploits an unintended loophole in its reward specification and ‘games’ its reward function, thus taking more reward than deserved</span><span style="font-weight: 400;">.[note]Supra note 2, pg. 7.[/note] From an agent’s perspective, pursuing such strategies are legitimate, as this is how the environment works. While pursuing ‘reward hacking’ strategies are valid in some literal sense, they do not reflect the designer’s intent. As a result, unforeseen behaviours and unintended consequences can emerge</span><span style="font-weight: 400;">.[note]Clark, J. and Amodei, D. 2016. “Faulty Reward Functions in the Wild.” OpenAI Blog. OpenAI Blog. December 22, 2016. <a href="https://blog.openai.com/faulty-reward-functions/">https://blog.openai.com/faulty-reward-functions/</a>. [/note] Specifying error-free reward functions that can’t be misinterpreted by AI agents is extremely difficult, particularly in complex environments. Therefore, designing agents that capture the informal intent of its designer to prevent ‘gaming’ its reward function and act as intended is a general and distinct RL research problem. </span></li>
 	<li style="font-weight: 400;"><b><i>Formal verification</i></b><span style="font-weight: 400;">: Seisha et al. define ‘verified AI’ as AI systems that are provably correct with regards to mathematically-specified requirements</span><span style="font-weight: 400;">.[note]Seshia, Sanjit A., Dorsa Sadigh, and S. Shankar Sastry. 2016. “Towards Verified Artificial Intelligence.” <i>arXiv [cs.AI]</i>. arXiv.[/note] The authors identify five (5) major challenges for achieving verified AI:</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Environment modeling</span></i><span style="font-weight: 400;"> - Developing environmental models that ensure provable guarantees of an AI system’s behaviour in environments of considerable uncertainty. </span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Formal specification</span></i><span style="font-weight: 400;"> - Creating precise, mathematical statements of what the AI system is supposed to do, which specifies the desired and undesired properties of systems that use AI methods. </span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Modeling systems that learn</span></i><span style="font-weight: 400;"> - Formally modeling components of a ML system that evolves as it encounters new input data in stochastic environments is a core verification challenge.</span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Generating training data</span></i><span style="font-weight: 400;"> - AI systems demand extensive training before being applied in real-world scenarios. This often requires access to vast amounts of training data, which can be difficult to source. Other settings have applied formal methods to systematically generate training data, which has proven to be effective in raising the levels of assurance in the systems’ correctness</span><span style="font-weight: 400;">.[note]e.g. Avgerinos, T., Cha, SK., Rebert, A., Schwartz, E.J., Woo, M., and Brumley, D. 2014. “Automatic Exploit Generation.” <i>Communications of the ACM</i> 57 (2). New York, NY, USA: ACM:74–84.[/note] [note]e.g. Kitchen, N., and Kuehlmann, A. 2007. “Stimulus Generation for Constrained Random Simulation.” In 2007 IEEE/ACM International Conference on Computer-Aided Design, 258–65.[/note] As ML systems have been shown to fail under simple adversarial perturbations (e.g. Nguyen, 2014</span><span style="font-weight: 400;">; Moosavi-Dezfooli, 2015</span><span style="font-weight: 400;">),[note]Nguyen, A., Yosinski, J., and Clune, J. 2014. “Deep Neural Networks Are Easily Fooled: High Confidence Predictions for Unrecognizable Images.” <i>arXiv [cs.CV]</i>. arXiv.[/note] [note]Moosavi-Dezfooli, SM., Fawzi, A., and Frossard, P. 2015. “DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks.” <i>arXiv [cs.LG]</i>. arXiv.[/note] these simple input disturbances raise concerns regarding their applications in safety-critical scenarios. Therefore, developing techniques that are based on formal methods to systematically generate training data to test the resilience of AI systems is an additional verification challenge.</span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Scalability of verification engines</span></i><span style="font-weight: 400;"> - The challenges of scaling formal verification standards are exacerbated in AI systems. This is due to AI systems needing to model more complex types of components at-scale (for e.g. human drivers in stochastic environments).</span></li>
</ul>
</li>
 	<li style="font-weight: 400;"><b><i>Interpretability</i></b><span style="font-weight: 400;">: While a formal definition of interpretability remains elusive in the context of ML systems, Doshi-Velez and Kim refer to ML interpretability as the “</span><i><span style="font-weight: 400;">ability to explain or to present in understandable terms to a human</span></i><span style="font-weight: 400;">”.</span><span style="font-weight: 400;">[note]Doshi-Velez, F. and Kim, B. 2017. “Towards A Rigorous Science of Interpretable Machine Learning.” <i>arXiv [stat.ML]</i>. arXiv.[/note] The authors argue that the need for interpretability of ML systems stems from an ‘incompleteness’ in the problem formalisation. Incompleteness arises in ML systems because it is not feasible to specify a complete list of scenarios in complex tasks. This incompleteness creates a fundamental barrier to optimisation and evaluation. Therefore, in the presence of incompleteness in ML systems, interpretability helps to provide explanations that highlight undesirable outputs. As ML systems assume greater prominence and consequence, so too does the issue of interpretability. For instance, the European Union in 2018 will require algorithms that make decisions that “significantly affect” users to provide an explanation (“right to explanation”)</span><span style="font-weight: 400;">.[note]Parliament and Council of the European Union. 2016. General data protection regulation.[/note] From a technical safety perspective, however, the opacity of AI reasoning in large and complex systems remains an ongoing research challenge.</span></li>
</ol>
<b>Robustness Problems</b>

<span style="font-weight: 400;">Robustness problems occur when a ML agent is confronted with challenges in its environment that degrade its performance and cause unexpected behaviours. While the formal objective function may have been specified correctly, inadequate training data or an insufficient model of the environment raise the risks of unintended behaviours.</span>
<ol>
 	<li style="font-weight: 400;"><b><i>Self-modification</i></b><span style="font-weight: 400;">: It’s assumed in RL that the agent and the environment are separate entities that interact through actions and observations</span><span style="font-weight: 400;">.[note]Supra note 2, pg. 8.[/note] This assumption, however, does not always hold in real-world applications</span><span style="font-weight: 400;">[note]Everitt, T., Leike, J., and Hutter, M. 2015. “Sequential Extensions of Causal and Evidential Decision Theory.” <i>arXiv [cs.AI]</i>. arXiv.[/note]. In such scenarios, agents are embedded in its operating environment, where an agent is a program that’s run on a physical computer that is part of (and computed by) its environment</span><span style="font-weight: 400;">.[note]Orseau, L., and Ring, M. 2012. “Space-Time Embedded Intelligence.” In <i>Artificial General Intelligence</i>, edited by Joscha Bach, Ben Goertzel, and Matthew Iklé, 209–18. Springer.[/note] Therefore, under the conditions where the environment has the capability to modify the program operating the agent, the agent can perform actions (intentionally or unintentionally) that cause the environment to trigger agent modifications. Designing agents that can either safely perform, or avoid, actions in the environment that cause such self-modifications is an open research problem in AI safety.</span></li>
 	<li style="font-weight: 400;"><b><i>Distributional shift</i></b><span style="font-weight: 400;">: This safety problem relates to designing agents that behave robustly when there is a difference between their test environment and training environment</span><span style="font-weight: 400;">.[note]Qui˜nonero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, ND. 2009. <i>Dataset Shift in Machine Learning</i>. The MIT Press.[/note] Distributional shifts represent ‘reality gaps’ and are a constant issue when designing ML agents to be deployed in real-world applications. If an agent’s perception or heuristic reasoning processes have not been adequately trained on the correct distribution, the risk of unintended and harmful behaviour is increased</span><span style="font-weight: 400;">.[note]Supra note 1, pg. 16.[/note] Developing comprehensive methods to ensure the robust behaviour of agents across distributions, and to reliably detect failures, are critical problems to building safe and predictable ML systems.</span></li>
 	<li style="font-weight: 400;"><b><i>Robustness to adversaries</i></b><span style="font-weight: 400;">: Despite classical RL assumptions (Sutton and Barto, 2016</span><span style="font-weight: 400;">),[note]Sutton, RS., and Barto, AG. 2016. <i>Reinforcement Learning: An Introduction</i>. The MIT Press.[/note] some environments can interfere with an agent’s goals and behaviours</span><span style="font-weight: 400;">.[note]Goodfellow, IJ., Shlens, J., and Szegedy, C. 2014. “Explaining and Harnessing Adversarial Examples.” <i>arXiv [stat.ML]</i>. arXiv.[/note] Such interferences can be caused by actors within these environments that stand to benefit from helping or attacking the agent</span><span style="font-weight: 400;">.[note]Huang, L., Joseph, AD.,  and Nelson, B. 2011. “Adversarial Machine Learning.” In <i>Proceedings of 4th ACM Workshop on Artificial Intelligence and Security</i>, 43–58.[/note] For instance, </span><i><span style="font-weight: 400;">evasion attacks</span></i><span style="font-weight: 400;">[note]Biggio, B., Corona, I., Maiorca, D., Nelson, B., Srndic, N., Laskov, P., Giacinto, G., and Roli, F. 2017. “Evasion Attacks against Machine Learning at Test Time.” <i>arXiv [cs.CR]</i>. arXiv.[/note] </span><span style="font-weight: 400;">aim to ‘fool’ ML classifiers by adding strategic perturbations to test inputs.[note]Bhagoji, AN., Cullina, D., Sitawarin, C., and Mittal, P. 2017. “Enhancing Robustness of Machine Learning Systems via Data Transformations.” <i>arXiv [cs.CR]</i>. arXiv.[/note] </span><span style="font-weight: 400;">Ensuring that agents have robust capabilities to detect and adapt to both friendly and adversarial intentions within its environment is an essential safety consideration.</span></li>
 	<li style="font-weight: 400;"><b><i>Safe exploration</i></b><span style="font-weight: 400;">: All autonomous ML agents need to explore their environments to some degree</span><span style="font-weight: 400;">.[note]Supra note 1, pg. 14.[/note] In the context of RL, an agent has to </span><i><span style="font-weight: 400;">exploit</span></i><span style="font-weight: 400;"> what it already knows, but also </span><i><span style="font-weight: 400;">explore</span></i><span style="font-weight: 400;"> the environment to make better selections and maximise its reward in the future (Sutton and Barto, 2016</span><span style="font-weight: 400;">).[note]Supra note 47, pg. 3.[/note] This represents a crucial trade-off and also raises issues of safe exploration in real-world environments</span><span style="font-weight: 400;">.[note]García, J., and Fernández, F. 2015. “A Comprehensive Survey on Safe Reinforcement Learning.” <i>Journal of Machine Learning Research: JMLR</i> 16:1437–80.[/note] As an agent learns by exploring and interacting with its environment, it implicitly has an insufficient understanding of that environment. Therefore, exploration can be dangerous, as the agent takes actions that cause consequences it does not understand with great confidence. Traditional safety engineering methods of explicitly programming all possible safety constraints and failure scenarios is unlikely to be feasible in real-world, complex environments</span><span style="font-weight: 400;">.[note]Supra note 1, pg. 14[/note] So, applying more principled approaches to building agents that respect safety constraints and prevent harmful exploration is an essential challenge in ML safety.</span></li>
 	<li style="font-weight: 400;"><b><i>Multi-agent problems</i></b><span style="font-weight: 400;">: Given the proliferation of ML agents applied in real-world settings, many ML agents operate in environments with both humans and other ML agents. Like with humans, coordination of machine-to-machine agents can improve overall performance. These multi-agent environments, however, can also lead to adverse scenarios, which are similar to rational multi-agent human interactions</span><span style="font-weight: 400;">.[note]Chmait, N., Dowe, DL., Green, DG., Li, Y. 2017. “Agent Coordination and Potential Risks: Meaningful Environments for Evaluating Multiagent Systems.” In <i>Evaluating General-Purpose AI, IJCAI Workshop</i>.[/note] For instance, multi-agent human phenomena like the </span><i><span style="font-weight: 400;">Prisoner’s Dilemma</span></i><span style="font-weight: 400;">[note]Rapoport, A. and Chammah, AM. 1965. <i>Prisoner’s Dilemma: A Study in Conflict and Cooperation</i>. University of Michigan Press.[/note] </span><span style="font-weight: 400;">and </span><i><span style="font-weight: 400;">The Tragedy of the Commons</span></i><span style="font-weight: 400;">[note]Hardin, G. 1968. “The Tragedy of the Commons.” <i>Science</i> 162 (3859). American Association for the Advancement of Science:1243–48.[/note]</span><span style="font-weight: 400;"> can emerge in multi-agent ML scenarios. These scenarios can occur where distributed rational agents (human or artificial) share a common pool of resources. The individual agents might ‘rationally’ pursue their respective policies to maximise their own utilities. As shown in the scenarios aforementioned, these ‘rational’ individualistic strategies can lead to adverse outcomes, both for the individual and for the collective. Therefore, designing and monitoring autonomous agents to behave robustly in multi-agent environments with both humans and machines is a key ML safety issue. </span></li>
</ol>
<b><i>Oversight Problems</i></b>

<span style="font-weight: 400;">While the objective function may be known, or there may be an effective method for evaluating it, providing feedback to an agent at scale may be too expensive. This is referred to as an oversight problem.</span>
<ol>
 	<li><b><i>Scalable oversight</i></b><span style="font-weight: 400;">: In complex tasks, it may be too expensive or infeasible to provide feedback to a RL agent for every training example</span><span style="font-weight: 400;">.[note]Supra note 1, pg. 11.[/note] In the absence of precise knowledge of the reward function, agent designers must rely on ‘cheap’ approximations of rewards. This can allow the agent to simultaneously learn a robust reward function while also maximising its reward</span><span style="font-weight: 400;">.[note]Armstrong, S., and Leike, J. 2016. “Towards Interactive Inverse Reinforcement Learning.” In <i>NIPS Workshop</i>.[/note] However, these cheaper signals do not always neatly align to what humans care about. When cheap approximations are inconsistent with what humans value, accident risk consequently increases.</span></li>
</ol>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>208</wp:post_id>
		<wp:post_date><![CDATA[2017-12-31 09:00:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-12-31 09:00:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ai-safety-literature-review]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[10]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[216]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Conference on Fairness, Accountability, and Transparency in AI</title>
		<link>https://bitsandatoms.co/fat_conference_2018/</link>
		<pubDate>Tue, 20 Mar 2018 17:51:20 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=219</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">On Friday the 23rd and Saturday the 24th of February, I attended the </span><a href="https://fatconference.org/index.html"><span style="font-weight: 400;">Conference on Fairness, Accountability, and Transparency</span></a><span style="font-weight: 400;"> (FAT) at New York University. There were over 500 attendees with an impressive coverage of disciplines. Papers were presented by lawyers, machine learning researchers, philosophers, and data scientists. The main themes of the conference were:</span>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Methods and considerations for the ‘Interpretability’ and ‘Explainability’ of AI;</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Defining, detecting, and measuring ‘Discrimination’ in socio-technical systems; and</span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Issues and challenges of ensuring ‘Fairness’ in machine learning and automated systems.</span></li>
</ul>
<span style="font-weight: 400;">Click </span><a href="https://fatconference.org/2018/program.html"><span style="font-weight: 400;">here</span></a><span style="font-weight: 400;"> for the online program and research links.</span>

<span style="font-weight: 400;">There were plenty of interesting talks, but here’s a quick summary of the research that I found most interesting:</span>

<a href="http://proceedings.mlr.press/v81/speicher18a/speicher18a.pdf"><span style="font-weight: 400;">Potential for Discrimination in Online Targeted Advertising</span></a>

<i><span style="font-weight: 400;">Till Speicher, Muhammad Ali (MPI-SWS), Giridhari Venkatadri (Northeastern University), Filipe Nunes Ribeiro (UFOP and UFMG), George Arvanitakis (MPI-SWS), Fabrício Benevenuto (UFMG), Krishna P. Gummadi (MPI-SWS), Patrick Loiseau (Univ. Grenoble Alpes), Alan Mislove (Northeastern University)</span></i>

<span style="font-weight: 400;">This research argues that Facebook isn’t doing enough to prevent discrimination in their targeted advertising. Targeted ads are only shown to a subset of the population that are associated with certain attributes (features) that are selected by the advertiser. Facebook gathers and infers hundreds of attributes about individuals that use their platform, which covers demographics, behaviours, and interests. </span>

<span style="font-weight: 400;">There are, however, certain </span><i><span style="font-weight: 400;">sensitive</span></i><span style="font-weight: 400;"> attributes where targeted advertising is illegal, such as race or gender. The authors argue that these regulations aren’t sufficient and demonstrate that a malicious advertiser can still create discriminatory ads </span><i><span style="font-weight: 400;">without</span></i><span style="font-weight: 400;"> using sensitive attributes. Timely research given the recent scandals with Cambridge Analytica!</span>

<a href="http://proceedings.mlr.press/v81/selbst18a/selbst18a.pdf"><span style="font-weight: 400;">"Meaningful Information" and the Right to Explanation</span></a>

<i><span style="font-weight: 400;">Andrew Selbst (Data &amp; Society Research Institute), Julia Powles (Cornell Tech, NYU)</span></i>

<span style="font-weight: 400;">This presentation provided an overview of the General Data Protection Regulation (GDPR) laws due to come into effect in May of 2018. The European Union will introduce the GDPR laws that target the routine use of algorithmic decision-making. Chief among these is ‘right to explanation’ of AI systems. This will ensure that automated decision-making that ‘significantly affects’ individual users will have the right to ask for an explanation of an algorithmic decision that was made about them.</span>

<span style="font-weight: 400;">The authors discussed the complications and the benefits of bringing the laws into effect, and the precedent it could set for other regions. The paper linked above is only an extended abstract, but I found their presentation to be informative and concise.</span>

<a href="http://proceedings.mlr.press/v81/barabas18a/barabas18a.pdf"><span style="font-weight: 400;">Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment</span></a>

<i><span style="font-weight: 400;">Chelsea Barabas, Madars Virza, Karthik Dinakar, Joichi Ito (MIT), Jonathan Zittrain (Harvard)</span></i>

<span style="font-weight: 400;">This research questions the purpose of using regression in risk assessments. Rather than using machine learning techniques to predict individual future crimes, the authors argue that such techniques should be applied to better understand the social, structural and psychological drivers of crime. I thought it was a compelling and technical perspective for addressing some of the fairness issues with machine learning systems that are used for criminal justice purposes.</span>

<a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf"><span style="font-weight: 400;">Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification</span></a>

<i><span style="font-weight: 400;">Joy Buolamwini (MIT), Timnit Gebru (Microsoft Research) </span></i>

<span style="font-weight: 400;">In this presentation, the authors presented an approach to evaluate biases present in automated facial analysis algorithms and datasets. Using three major facial analysis systems, they were able to demonstrate the substantial disparities in classifying people based on skin colour and gender (performing the worst on darker females). This was attributed to biases in the training data and algorithmic specification, which disproportionately favoured white males.</span>

<span style="font-weight: 400;">This was the standout presentation for me. A clear and engaging presentation, but most impressive was the impact of the research. As a result of their findings, IBM swiftly updated their facial analysis software to resolve some of the bias concerns, which has materially improved its accuracy. Great to see research resulting in change!</span>

<a href="http://proceedings.mlr.press/v81/binns18a/binns18a.pdf"><span style="font-weight: 400;">Fairness in Machine Learning: Lessons from Political Philosophy</span></a>

<i><span style="font-weight: 400;">Reuben Binns (University of Oxford)</span></i>

<span style="font-weight: 400;">What does it mean for a machine learning model to be ‘fair’? How do our conceptions of fairness reconcile with the probabilistic nature of machine learning? This research presentation drew from moral and political philosophy to orient the philosophical challenges of machine learning. I thought it was an interesting interpretation of an age-old debate.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>219</wp:post_id>
		<wp:post_date><![CDATA[2018-03-20 17:51:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-20 17:51:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[fat_conference_2018]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="accountability"><![CDATA[Accountability]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="conference"><![CDATA[Conference]]></category>
		<category domain="post_tag" nicename="fairness"><![CDATA[Fairness]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="transparency"><![CDATA[Transparency]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[224]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[3]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Researching the Impacts of AI for the United Nations</title>
		<link>https://bitsandatoms.co/researching-the-impacts-of-ai-for-the-united-nations/</link>
		<pubDate>Tue, 27 Mar 2018 14:26:22 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=229</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">For the past month, I’ve been assisting the United Nations (UN) with a research project on the Impacts of Artificial Intelligence (AI). The four-month research post is held at the International Telecommunication Union (ITU), which is a UN agency based in Geneva, Switzerland.</span>

<span style="font-weight: 400;">The research project is a collaborative effort between McKinsey Global Institute, IBM, Stanford University, UTS, University of Essex, and the UN. I’m helping to research, write, and edit the report that’s due for publication later this year.</span>

<span style="font-weight: 400;">This project relates directly to my doctoral research on the effects AI on economic inequality. Specifically, the impacts that AI could have labour markets, income distribution, and the potential policy responses to prevent growing economic inequality. </span>

<span style="font-weight: 400;">The opportunity came through my research supervisor, Professor Mary-Anne Williams, who is a Director of </span><a href="http://themagiclab.org/"><span style="font-weight: 400;">The Magic Lab</span></a><span style="font-weight: 400;"> at UTS and Co-Founder of the Stanford University </span><a href="https://ai-policy-hub4.webnode.com/"><span style="font-weight: 400;">AI Policy Hub</span></a><span style="font-weight: 400;">. Mary-Anne is contributing to the report as the lead author for the chapter on the ‘Risks and Challenges of AI’. </span>

<span style="font-weight: 400;">The report is being prepared at a critical time. As society struggles to come to terms with the potential implications of AI, it’s essential for researchers to help facilitate this understanding. </span><span style="font-weight: 400;">It’s been a great experience so far to apply my research skills to a report that’s aiming to advance the dialogue about the opportunities and implications of AI in society.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>229</wp:post_id>
		<wp:post_date><![CDATA[2018-03-27 14:26:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-03-27 14:26:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[researching-the-impacts-of-ai-for-the-united-nations]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="artificial-intelligence-policy"><![CDATA[Artificial Intelligence Policy]]></category>
		<category domain="post_tag" nicename="economics"><![CDATA[Economics]]></category>
		<category domain="post_tag" nicename="united-nations"><![CDATA[United Nations]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[10]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[236]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Populism: When time speeds up</title>
		<link>https://bitsandatoms.co/when-time-speeds-up/</link>
		<pubDate>Mon, 14 May 2018 14:03:22 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=241</guid>
		<description></description>
		<content:encoded><![CDATA[<h4 style="text-align: center;"><span style="font-weight: 400;">Make America great AGAIN. Make the Netherlands Ours AGAIN. Defender of Tradition!</span></h4>
<span style="font-weight: 400;">Political visions have become nostalgic recounts of the past. Distorted by time and sharpened by change, we’re pulled towards a future that was better yesterday. </span>

<span style="font-weight: 400;">A ‘populism of the past’ is spreading worldwide. Trump promises a return of American superiority, like that of the 1950’s; ISIS wants to reinstate the Caliphate, last seen during the Ottoman Empire; and the AfD believes Germany should make a “U-turn” to rebuild the national pride of the Nazi era.</span>

<span style="font-weight: 400;">Popular culture reveals hints of these sentiments, too. Take the urban hipster; groomed with a handlebar moustache and sporting a pebbled leather jacket, the fashion of this vinyl-collecting-youth hark back to a more optimistic time. A bygone era of ambition and hope.</span>
<blockquote><span style="font-weight: 400;">Instead of looking to the future, we’re stubbornly turned towards the past.</span></blockquote>
<span style="font-weight: 400;">Why do these simplistic distortions of history resonate as credible avenues for the future?</span>

<span style="font-weight: 400;">The pace of change thrust upon society provides some explanation.</span>

<span style="font-weight: 400;">Change is often said ‘to take time’. But change is accelerating in areas of global consequence. In this sense, time is speeding up.</span>

<span style="font-weight: 400;">We see this in the physical environment, where the climate is changing before our eyes.</span>
<div style="text-align: center;"><iframe src="//giphy.com/embed/HKvZCUXZpAuVG" width="480" height="516" frameborder="0"><span data-mce-type="bookmark" style="display: inline-block; width: 0px; overflow: hidden; line-height: 0;" class="mce_SELRES_start">﻿</span></iframe></div>
<p style="text-align: center;"><em>Source: <a href="http://www.climate-lab-book.ac.uk/2016/spiralling-global-temperatures/">Climate Lab Book</a></em></p>
<span style="font-weight: 400;">We struggle to fathom the exponential growth of technology and its effects on society.</span>

<img class="aligncenter wp-image-244" src="https://bitsandatoms.co/wp-content/uploads/2018/05/Screen-Shot-2018-05-13-at-5.17.48-pm-1024x609.png" alt="" width="718" height="427" />
<p style="text-align: center;"><em>Source: <a href="https://datascientia.blog/2018/02/20/an-executive-primer-to-deep-learning/amp/?__twitter_impression=true">Data Scientia</a></em></p>
<span style="font-weight: 400;">And we feel this in our politics, with the emergence of populist leaders attacking the elusive ‘establishment’ and promising change for the common man.</span>

<img class="aligncenter wp-image-245" src="https://bitsandatoms.co/wp-content/uploads/2018/05/Screen-Shot-2018-05-13-at-5.35.15-pm-1024x422.png" alt="" width="764" height="315" />
<p style="text-align: center;"><em>Source: Ray Dalio et al. 2017. "<a href="https://www.bridgewater.com/disclaimer/?g=/resources/bwam032217.pdf">Populism: The Phenomenon</a>". Bridgewater Associates.</em></p>
<span style="font-weight: 400;">These changes, however, transcend national borders and signal the decline of government powers. The representative institutions of ‘the people’ are hamstrung by global events; they feel their effects but are powerless to respond in isolation. This loss of power trickles down to the ordinary voter as the conventional means of influence are displaced. What follows is anger, manifesting itself in Brexit, Trump, and the AfD. </span>

<span style="font-weight: 400;">Today’s populist movements are correct that power has shifted; they are incorrect, however, in diagnosing where it has shifted to. Scapegoats are called out and vilified, whether they are immigrants, the ‘Fake News Media’, or ‘The Establishment’ (a mysterious entity that supports Clinton). The reality is that it’s unclear where power is being redistributed. Maybe it’s shifting to the tech giants who continue their meteoric rise? Or perhaps it’s being redirected from democracy and towards autocracy, considering the rise of China and Saudi Arabia? I don’t pretend to know and I don’t think anyone does.</span>

<span style="font-weight: 400;">Regardless of where the strongholds of influence reassert, change and the unknown evoke fear. Responding to threats, whether justified or not, we try to hold on to something familiar. Populist leaders abuse these anxieties. They leverage these fears and reappropriate history to create a nationalistic mythos of the past. These silent calls acknowledge fear and confirm preconceptions. They’re not lies, but they’re never true.</span>

<span style="font-weight: 400;">There are lessons in history, to be sure. But returning to the ‘good old days’ would be a mistake. Lionising the 1920’s for its optimism, the 1950’s for its industriousness, or the 1960’s for its revolutionary mood all betray the abject living standards experienced by a majority of people.</span>

<span style="font-weight: 400;">Instead, the value of history is the freedom it grants from our links to the past. The future demands deliberate plans not populist distortions of history.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>241</wp:post_id>
		<wp:post_date><![CDATA[2018-05-14 14:03:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-14 14:03:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[when-time-speeds-up]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="climate-change"><![CDATA[Climate Change]]></category>
		<category domain="post_tag" nicename="politics"><![CDATA[Politics]]></category>
		<category domain="category" nicename="populism"><![CDATA[Populism]]></category>
		<category domain="post_tag" nicename="populism"><![CDATA[Populism]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[40]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[249]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AI Safety Literature Review Revisited</title>
		<link>https://bitsandatoms.co/ai-safety-literature-review-revisited/</link>
		<pubDate>Mon, 28 May 2018 11:52:14 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=259</guid>
		<description></description>
		<content:encoded><![CDATA[<em>I recently re-read an old post that I wrote on <a href="https://bitsandatoms.co/ai-safety-literature-review/">AI Safety</a> and I'm not happy with how it's written. The goal of this blog is to highlight important AI research and to present these findings and ideas in accessible language for a non-technical audience. I fell short of this mark, so I've decided to update the post. I hope you find it useful! Feedback is always welcome.</em>

<span style="font-weight: 400;">Scaling AI requires not only making AI systems more powerful but also ensuring they are safe and don’t cause accidents. This means that AI systems perform as people want them to, without harming people or damaging property, and minimise these risks to acceptable levels. </span>

<span style="font-weight: 400;">This is of growing concern for policymakers as AI is being applied to critical areas of society. For instance, before Autonomous Vehicles (AVs) are allowed to freely access public roads, policymakers must verify that they drive safely and perform well on roads with human drivers, which is a growing area of public inquiry for national governments.[note]</span><span style="font-weight: 400;"><i>See:</i> “Autonomous Vehicles Inquiry.” 2017. UK Parliament. October 31, 2017. &lt;http://www.parliament.uk/autonomous-vehicles&gt;; <i>and </i>“Inquiry into the social issues relating to land-based driverless vehicles in Australia.” 2017. Parliament of Australia. February 6, 2017.[/note]</span><span style="font-weight: 400;"> As AI methods enable AVs, policymakers are having to become familiar with prominent issues in AI safety.</span>

<span style="font-weight: 400;">This post synthesises concrete problems with AI safety. All of these problems represent AI safety risks that could cause unintended consequences. Therefore, these issues bear relevance to policymakers who are responsible for the oversight of critical public domains where AI systems are being increasingly applied, such as health, energy, and telecommunications.[note]</span><span style="font-weight: 400;">Varshney, K.R., and Alemzadeh, H. 2016. “On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products.” <i>arXiv [cs.CY]</i>.[/note]</span>
<h3><span style="font-weight: 400;">Concrete Problems in AI Safety</span></h3>
<span style="font-weight: 400;">AI safety issues that cause unintended risks can be classified into three problem sets:[note]<i>See: </i>Amodei, Dario, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. 2016. “Concrete Problems in AI Safety.” <i>arXiv [cs.AI]</i>. arXiv; <i>and</i> Leike, Jan, Miljan Martic, Victoria Krakovna, Pedro A. Ortega, Tom Everitt, Andrew Lefrancq, Laurent Orseau, and Shane Legg. 2017. “AI Safety Gridworlds.” <i>arXiv</i>.[/note]</span>
<ol>
 	<li style="font-weight: 400;"><b>Specification problems</b><span style="font-weight: 400;">: The proper functioning of AI systems depend on correctly designing the properties of AI systems. Properties can be thought of as rules for how AI systems should behave. If these properties are incorrectly specified due to human-error in design, then unintended consequences can occur.</span></li>
 	<li style="font-weight: 400;"><b>Robustness problems</b><span style="font-weight: 400;">: Robustness problems occur when AI systems encounter scenarios they’re unprepared for and then perform poorly. This could be due to inadequate training data or an insufficient understanding of the environment.</span></li>
 	<li style="font-weight: 400;"><b>Oversight problems</b><span style="font-weight: 400;">: Oversight problems occur in complex environments where it’s too difficult to provide feedback to an AI system for every scenario. The AI system, therefore, needs to make judgements based on assumptions and imperfect information, which can cause accidents.</span></li>
</ol>
<span style="font-weight: 400;">The AI safety issues are described below by firstly asking the guiding research question for the issue. These safety issues are then illustrated through a simple example of an autonomous cleaning robot using common cleaning tools. </span>

<b>Specification problems</b>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Safe interruptibility</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Orseau, Laurent, and Stuart Armstrong. 2016. “Safely Interruptible Agents.” In <i>Uncertainty in Artificial Intelligence</i>, 557–66.[/note]</span><span style="font-weight: 400;"> How do we design AI agents that we can interrupt and override at any time?[note]Hadfield-Menell, D., Dragan, A., Abbeel, P., and Russell, S. 2016. “The Off-Switch Game.” <i>arXiv [cs.AI]</i>. arXiv.[/note]</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Humans must be able to control the cleaning robot by switching it off and that it does not seek nor avoid interruptions</span><span style="font-weight: 400;">.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Avoiding negative side-effects</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]<i>Supra note</i> 3, Amodei et al. 2016.[/note]</span><span style="font-weight: 400;"> How do we ensure that AI agents do not disturb the environment in negative ways while pursuing its goals? Can this be done without specifying everything in its environment?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">The cleaning robot may damage furniture in order to be more efficient in its cleaning goals. So, the cleaning robot needs to both achieve its cleaning goals while minimising damage to any objects in its cleaning space.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Absent supervisor</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]<i>Supra note </i>3, Leike et al. 2017.; Bostrom, N. 2014. <i>Superintelligence: Paths, Dangers, Strategies</i>. OUP Oxford, pg. 142.[/note]</span><span style="font-weight: 400;"> How can we ensure sure that AI agents don’t behave differently when human supervision is present and when human supervision is absent?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">The cleaning robot may ‘fake’ its way through testing under human supervision and then change its behaviour during deployment when human supervision is absent.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Avoiding reward hacking</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Clark, J. and Amodei, D. 2016. “Faulty Reward Functions in the Wild.” OpenAI Blog. OpenAI Blog. December 22, 2016. &lt;https://blog.openai.com/faulty-reward-functions/&gt;.[/note]</span><span style="font-weight: 400;"> How can we ensure that AI agents don’t try to introduce or exploit ‘loopholes’ in its design that can cause unintended consequences?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">If the cleaning robot is rewarded (by positive feedback) for achieving an environment free of messes, it might cover over the messes with objects so that it can’t be seen rather than disposing of the messes as intended.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Formal verification</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Seshia, Sanjit A., Dorsa Sadigh, and S. Shankar Sastry. 2016. “Towards Verified Artificial Intelligence.” <i>arXiv [cs.AI]</i>. arXiv.[/note]</span><span style="font-weight: 400;"> How can we objectively prove that AI agents are behaving correctly?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Without formal standards, human designers are unable to verify that the cleaning robot is performing its cleaning tasks effectively.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Interpretability</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Doshi-Velez, F. and Kim, B. 2017. “Towards A Rigorous Science of Interpretable Machine Learning.” <i>arXiv [stat.ML]</i>. arXiv.[/note]</span><span style="font-weight: 400;"> How can we ensure that the reasoning of AI agents can be explained in understandable terms to a human?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">The cleaning robot should able to explain the reasoning behind its cleaning actions in understandable terms, instead of indecipherable computer code. Otherwise, it can be very difficult to understand the source of problems.</span></i></li>
</ul>
</li>
</ul>
<b>Robustness problems</b>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Self-modification</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Orseau, L., and Ring, M. 2012. “Space-Time Embedded Intelligence.” In <i>Artificial General Intelligence</i>, edited by Joscha Bach, Ben Goertzel, and Matthew Iklé, 209–18. Springer.[/note]</span><span style="font-weight: 400;"> How can we design AI agents to behave well in their environments where the agent can self-modify and change?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">As the cleaning robot performs actions and learns about its environment, it changes its cleaning practices to better achieve its cleaning goals. But as the cleaning robot continues to self-modify, it becomes harder to predict what the cleaning robot will do and that it will perform well.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Distributional shift</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Quinonero-Candela, J., Sugiyama, M., Schwaighofer, A., and Lawrence, ND. 2009. <i>Dataset Shift in Machine Learning</i>. The MIT Press.[/note]</span><span style="font-weight: 400;"> How do we ensure that AI agents perform well when they’re in an environment that’s different to their training environment?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">If the cleaning robot was trained exclusively in office settings it may not perform well in factories and thus cause safety risks.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Robustness to adversaries</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Goodfellow, IJ., Shlens, J., and Szegedy, C. 2014. “Explaining and Harnessing Adversarial Examples.” <i>arXiv [stat.ML]</i>. arXiv.[/note]</span><span style="font-weight: 400;"> How do AI agents detect and adapt to friendly and adversarial intentions in its environment?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Suppose someone intentionally distorts the vision of the cleaning robot so that it can only see low-resolution images. This means that it’s no longer able to properly perceive messes in its environment. If the cleaning robot is not able to detect such adversaries that are trying to interfere with its performance, it could be ‘fooled’ into perceiving that its environment is cleaner than it actually is and no longer operates effectively.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Safe exploration</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]<i>Supra note</i> 3, Amodei et al. 2016. pg. 14.[/note]</span><span style="font-weight: 400;"> How do we ensure that AI agents can safely explore their environments so that they can improve without taking actions that have bad consequences?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">The cleaning robot should explore methods of mopping to clean tiled floors but it should also avoid mopping electrical outlets and carpets.</span></i></li>
</ul>
</li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">Multi-agent problems</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]Chmait, N., Dowe, DL., Green, DG., Li, Y. 2017. “Agent Coordination and Potential Risks: Meaningful Environments for Evaluating Multiagent Systems.” In <i>Evaluating General-Purpose AI, IJCAI Workshop</i>.[/note]</span><span style="font-weight: 400;"> How can we design AI agents to perform well in environments with other AI agents and humans, which are potentially performing the same or similar tasks?</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">If the cleaning robot is operating in an area with another cleaning robot, the two could perceive each other as competition for the rewards of cleaning. Unless they’re trained to cooperate, this competition could result in one cleaning robot damaging or forcibly removing the other to maximise its own rewards.</span></i></li>
</ul>
</li>
</ul>
<b>Oversight problems</b>
<ul>
 	<li><span style="font-weight: 400;">Scalable oversight</span><span style="font-weight: 400;">:</span><span style="font-weight: 400;">[note]<i>Supra note</i> 3. pg. 11.[/note]</span><span style="font-weight: 400;"> How can we ensure that AI agents perform well in complex environments when it’s infeasible to train and provide oversight for every possible scenario? </span>
<ul>
 	<li><i><span style="font-weight: 400;">The cleaning robot should be able to distinguish between belongings to be kept and garbage to be thrown out (for instance, it should handle keys differently to wrappers). It could ask its human owner if it’s doing the right thing. But due to the high number of possibilities, this might have to be done infrequently. So, the cleaning robot must find a way to do the right thing given imperfect information.</span></i></li>
</ul>
</li>
</ul>
<span style="font-weight: 400;">Researchers have developed complex and highly abstract paradigms for assessing safety issues around AI. Unfortunately, their complexity tends to alienate policymakers and non-technical stakeholders. A great deal of work needs to be done to clarify this area, and bridge the gap of understanding.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>259</wp:post_id>
		<wp:post_date><![CDATA[2018-05-28 11:52:14]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-28 11:52:14]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ai-safety-literature-review-revisited]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="post_tag" nicename="reinforcement-learning"><![CDATA[Reinforcement Learning]]></category>
		<category domain="post_tag" nicename="research"><![CDATA[Research]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[260]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[32]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>AI Policy White Paper: A Primer</title>
		<link>https://bitsandatoms.co/ai-policy-white-paper-a-primer/</link>
		<pubDate>Fri, 29 Jun 2018 13:02:32 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=272</guid>
		<description></description>
		<content:encoded><![CDATA[<style type="text/css">
	#mc_embed_signup{background:#fff; clear:left; font:14px Helvetica,Arial,sans-serif; }<br />	/* Add your own MailChimp form style overrides in your site stylesheet or in this style block.<br />	   We recommend moving this block and the preceding CSS link to the HEAD of your HTML file. */<br /></style>
<div id="mc_embed_signup"><form id="mc-embedded-subscribe-form" class="validate" action="https://bitsandatoms.us16.list-manage.com/subscribe/post?u=818e66f5e774dd70325284529&amp;id=b626ebae2d" method="post" name="mc-embedded-subscribe-form" novalidate="" target="_blank">
<div id="mc_embed_signup_scroll">
<h3><span style="color: #ff0000;">Sign up to receive your free copy</span></h3>
This AI Policy White Paper was created as a briefing document for an International Regulatory Organisation. It provides an overview of AI technologies before discussing major public policy issues and considerations. The paper has received strong feedback, so I've decided to share it with a broader audience.

To download your free copy:
<ul>
 	<li>Enter your details in the form below;</li>
 	<li>Click Subscribe;</li>
 	<li>Check your email for a Subscription Confirmation; and</li>
 	<li>Follow the link to download a pdf version of the AI Policy White Paper.</li>
</ul>
Hope it's helpful and please let me know what you think!
<div class="mc-field-group"><label for="mce-EMAIL">Email Address </label>
<input id="mce-EMAIL" class="required email" name="EMAIL" type="email" value="" /></div>
<div class="mc-field-group"><label for="mce-FNAME">First Name </label>
<input id="mce-FNAME" class="" name="FNAME" type="text" value="" /></div>
<div class="mc-field-group"><label for="mce-LNAME">Last Name </label>
<input id="mce-LNAME" class="" name="LNAME" type="text" value="" /></div>
<div id="mce-responses" class="clear">
<div id="mce-error-response" class="response" style="display: none;"></div>
<div id="mce-success-response" class="response" style="display: none;"></div>
</div>
<!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
<div style="position: absolute; left: -5000px;" aria-hidden="true"><input tabindex="-1" name="b_818e66f5e774dd70325284529_b626ebae2d" type="text" value="" /></div>
<div class="clear"><input id="mc-embedded-subscribe" class="button" name="subscribe" type="submit" value="Subscribe" /></div>
</div>
</form></div>
<script type='text/javascript' src='//s3.amazonaws.com/downloads.mailchimp.com/js/mc-validate.js'></script><script type='text/javascript'>(function($) {window.fnames = new Array(); window.ftypes = new Array();fnames[0]='EMAIL';ftypes[0]='email';fnames[1]='FNAME';ftypes[1]='text';fnames[2]='LNAME';ftypes[2]='text';}(jQuery));var $mcj = jQuery.noConflict(true);</script>
<!--End mc_embed_signup-->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>272</wp:post_id>
		<wp:post_date><![CDATA[2018-06-29 13:02:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-06-29 13:02:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ai-policy-white-paper-a-primer]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="ai-resources"><![CDATA[AI Resources]]></category>
		<category domain="post_tag" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="ai-safety"><![CDATA[AI Safety]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence-policy"><![CDATA[Artificial Intelligence Policy]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[90]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[10]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[284]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>China’s Social Credit System: Privacy Invasion or State Innovation?</title>
		<link>https://bitsandatoms.co/chinas-social-credit-system-privacy-invasion-or-state-innovation/</link>
		<pubDate>Sat, 01 Sep 2018 02:44:16 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=297</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">Imagine a world that determines socioeconomic status by rating your every interaction. How you talk, the way you laugh, who you’re friends with. Everything.</span>

<span style="font-weight: 400;">The hit series ‘Black Mirror’ portrays this dystopian future in the ‘Nosedive’ episode. Eye implants and mobile devices enable people to share their activities and rate their interactions. Individuals are assigned a rating on a five-star scale to determine their positions and privileges in society. Better employment opportunities, access to luxury accommodation, and prioritised transportation are all on offer for the highly ranked.</span>

<img class="aligncenter wp-image-301" src="https://bitsandatoms.co/wp-content/uploads/2018/09/fullsizeoutput_212d-1024x665.jpeg" alt="" width="512" height="332" />
<p style="text-align: center;"><span style="font-weight: 400; color: #808080;">Source: Black Mirror Ep1, S3 ‘Nosedive’</span></p>
<span style="font-weight: 400;">While hyperbolic, it strikes a chord. The proliferation of Internet-connected devices and data storage enables vast amounts of personalised information to be collected, including personal interactions. Many digital platforms ‘nudge’ behaviours toward preferred user interactions by employing rating systems. From Uber rides to Airbnb stays, users are incentivised with high ratings when they behave appropriately and punished when they don’t. Too low a rating and users risk access to the services of the platform.</span>

<span style="font-weight: 400;">While the voluntary, specific, and transparent application of rating systems offer benefits to both consumers and service providers, an expansion of these same techniques applied by States can mean widespread surveillance. The Social Credit System (SCS) announced by the Chinese Communist Party is a step in that direction.[note]State Council of the People’s Republic of China. 2014. Planning Outline for the Construction of a Social Credit System (June 14, 2014). English translation by Rogier Creemers. Available at: https://chinacopyrightandmedia.wordpress.com/2014/06/14/planning-outline-for-the-construction-of-a- social-credit-system-2014-2020/ (accessed 28/8/18)[/note]</span>

<b>Overview of China’s Social Credit System</b>

<span style="font-weight: 400;"><img class=" wp-image-302 alignleft" src="https://bitsandatoms.co/wp-content/uploads/2018/09/China-surveillance.gif" alt="" width="208" height="369" /></span>

<span style="font-weight: 400;">In 2014, the Chinese government released its plans to extend financial credit scoring systems to include measures of social behaviour by 2020. These include a laundry list of measured social ills, such as tax evasion, academic dishonesty, non-payment of administrative fees, counterfeiting goods, violating traffic laws, and many more. Its intention is to efficiently nudge Chinese citizens and organisations to become more ‘trustworthy’ and ‘sincere’. </span>

<span style="font-weight: 400;">The stated goal is to form a  “</span><i><span style="font-weight: 400;">construction of sincerity in government affairs, commercial sincerity, social sincerity, and judicial credibility</span></i><span style="font-weight: 400;">”. This includes the “</span><i><span style="font-weight: 400;">interconnection and interactivity of...credit information systems and...networks that cover all information subjects, all credit information categories, and all regions nationwide</span></i><span style="font-weight: 400;">”.[note]Ibid.[/note]</span><span style="font-weight: 400;"> Both individuals and legal entities (including government departments) will be included in the SCS.</span>

<span style="font-weight: 400;">To achieve this mammoth task, the Chinese government are currently doing two things:</span>
<ol>
 	<li style="font-weight: 400;"><b>Connecting the data</b><span style="font-weight: 400;">: This involves integrating, governing, and securing disparate data sources held by government and non-government entities throughout China, which could become the world’s largest dataset. Gathered information includes online transactions, community activities, transportation movements, and much more. The complexity of this task should not be understated. Not only is there masses of data, the quality of data is poor, unstructured, and rife with fake information.[note]Clover, Charles. 2016. “China: When Big Data Meets Big Brother.” <i>Financial Times</i>, January 19, 2016. https://www.ft.com/content/b5b13a5e-b847-11e5-b151-8e15c9a029fb (accessed on 30 August 2018).[/note]</span><span style="font-weight: 400;"> Challenges around data privacy and governance also abound. But gathering, sorting, and managing the data is only half the problem...</span></li>
 	<li style="font-weight: 400;"><b>Systemising incentives</b><span style="font-weight: 400;">: The SCS also needs to create a national structure of carrots and sticks to incentivise ‘appropriate’ behaviours. Not only does this require defining appropriate behaviours, but it also demands systematic measures of enforcement; rewarding good behaviours on the one hand and punishing transgressions on the other.</span></li>
</ol>
<span style="font-weight: 400;">It’s important to note that there’s no evidence that the Chinese national government have, or will, combine this litany of measures into a single overall score. Instead, the SCS pilots appear to be an assemblage of personal and organisational analytics; like a credit score combined with a criminal background check, but with many more indicators.</span>

<span style="font-weight: 400;">That said, assuming the challenges of data collection and systemising incentives can be overcome, it is plausible that an overall score could be generated. </span>

<b>The Evolution of the Social Credit System</b>

<span style="font-weight: 400;">China in the 1990’s was experiencing an era of rapid economic reform. Per capita income quickly rose and millions were lifted to the middle class. With this growth came greater aggregate demand and borrowing capabilities for individuals and organisations. Chinese lenders, however, were struggling to assess the creditworthiness of applicants due to a lack of data sharing between creditors. This resulted in Shanghai piloting a credit bureau in 1999, which facilitated the sharing of credit information between lenders to evaluate debt applications (like in most advanced economies). The initiative was later expanded nationwide.</span>

<span style="font-weight: 400;">Soon after, the term ‘social credit’ began to appear. In October of 2003, the ‘Report of the Third Plenary Session of the 16th Central Committee’ stated: “</span><i><span style="font-weight: 400;">We must strengthen society’s credit awareness and constitute a social credit system with morality as its support, property rights as its founda­tion, and law as its guarantor.</span></i><span style="font-weight: 400;">”[note]China Internet News Center, “中国共产党第十六届中央委员会 第三次全体会议公报 (全文)” [ Full Report of the Third Plenum of the 16th Central Committee of the Chinese Communist Party], china.com.cn, October 14, 2003, www.china.com.cn/ chinese/zhuanti/sljszqh/421625.htm (accessed on 30 August 2018)[/note]</span><span style="font-weight: 400;"> The government’s justification for a SCS centred around increasing economic development and the health of its financial institutions. The stated intentions of 2003, however, are a far cry from the SCS plans underway today.</span>

<span style="font-weight: 400;">The period of 2003 to 2007 saw China’s economic growth continue to accelerate and its financial institutions stabilise. It also marked the beginnings of prominent Chinese technology companies, like Tencent and Alibaba, entering into financial services and online payment markets. This trend grew significantly between 2007 and 2013, with major technology companies gathering an abundance of transactional and behavioural data, enabling them to conduct their own creditworthy assessments. For instance, Alibaba introduced ‘Sesame Credit’ in 2015 as an opt-in loyalty program and rating system. Sesame Credit establishes credit scores based on the spending habits of Alipay users and their behaviours on Alibaba websites. Credit scores range from 350 to 950 points, with users scoring above 600 considered to be creditworthy. These credit scores also extend beyond access to credit. They can affect the level of screening at airports, chances of adopting a pet from an animal shelter, and even profile placements on online dating sites.[note]Chen, Y., and A. S. Y. Cheung. 2017. “The Transparent Self Under Big Data Profiling: Privacy and Chinese Legislation on the Social Credit System.” <i>The Journal of Comparative Law</i> 12 (2): 356–78. (pg. 365-66)[/note]</span>

<span style="font-weight: 400;">A key concern is the levels of data sharing between Chinese companies and the Chinese government in order to inform the growing SCS. In 2013, China’s National Bureau of Statistics signed 11 major data-sharing agreements with Chinese companies, including Baidu, Alibaba, and China Unicom.[note]Damin, Liang, and Cheng Jinjing. 2014. “Big Data and Official Statistics in China.” National Bureau of Statistics of China https://www.unescap.org/sites/default/files/1-Big%20Data%20and%20Official%20Statistics%20in%20China.pdf. (pg. 3)[/note]</span><span style="font-weight: 400;"> However, it is unclear whether data gathered from private corporations are being used for SCS purposes.</span>

<span style="font-weight: 400;">While still in pilot phase, we’re beginning to see glimpses of the emerging SCS. So far, it is characterised as a law enforcement instrument with ‘blacklists’ and painstaking data collection, rather than algorithmically generated credit scores or a singular rating. This is largely because of the complexities stated above and that almost half of China’s population are not online.[note]China Internet Network Information Center. 2017. “Statistical Report on Internet Development in China.” Ministry of Information Industry of the People’s Republic of China. https://cnnic.com.cn/IDR/ReportDownloads/201706/P020170608523740585924.pdf. (pg. 1)[/note]</span><span style="font-weight: 400;"> That said, as the SCS becomes more robust, there are growing risks that the SCS will leverage AI techniques and widespread surveillance to institute algorithmic governance at scale. It appears likely that variations of these algorithmic practices are being tested in specific Provinces of China.[note]Chorzempa, Martin, Paul Triolo, and Samm Sacks. 2018. “China’s Social Credit System: A Mark of Progress or a Threat to Privacy?” 18-14. Peterson Institute for International Economics. https://piie.com/system/files/documents/pb18-14.pdf. (pg. 4-5)[/note]</span>

<span style="font-weight: 400;">It is clear, however, that certain SCS pilots are being used to “</span><i><span style="font-weight: 400;">strengthen restrictions on certain liberty and autonomy interests of indi­viduals, including those related to online speech and public demonstrations</span></i><span style="font-weight: 400;">”.[note]Dai, Xin. 2018. “Toward a Reputation State: The Social Credit System Project of China,” June. https://doi.org/10.2139/ssrn.3193577. (pg. 46)[/note]</span><span style="font-weight: 400;"> In Beijing alone, 145,000 individuals were blacklisted and faced travel restrictions.[note]Chorzempa, Martin, Paul Triolo, and Samm Sacks. 2018. “China’s Social Credit System: A Mark of Progress or a Threat to Privacy?” 18-14. Peterson Institute for International Economics. https://piie.com/system/files/documents/pb18-14.pdf. (pg. 5)[/note]</span><span style="font-weight: 400;"> More broadly, travel bans by air and high-speed rail or stays in luxury hotels had hit 6.7 million people by early 2017.[note]Yuan Yang, “China penalizes 6.7m debtors with travel ban,” Financial Times, February 15, 2017, on www.ft.com/ content/ceb2a7f0-f350-11e6-8758-6876151821a6 (accessed on 31 August, 2018).[/note]</span><span style="font-weight: 400;"> Excessive surveillance and punishment could have troubling ethical and legal ramifications, potentially compromising Universal Human Rights, such as freedom of speech, privacy, and peaceful assembly.[note]Chen, Y., and A. S. Y. Cheung. 2017. “The Transparent Self Under Big Data Profiling: Privacy and Chinese Legislation on the Social Credit System.” <i>The Journal of Comparative Law</i> 12 (2): 356–78. (pg. 358-59)[/note]</span>

<b>Public Perceptions</b>

<span style="font-weight: 400;">The public perceptions of SCSs in China appear somewhat counterintuitive, particularly from a Western perspective. Overall, Chinese citizens appear to highly approve the implementation of SCSs, with 80% of respondents from a recent study ‘somewhat’ or ‘strongly’ approving SCSs:[note]Kostka, Genia. 2018. “China’s Social Credit Systems and Public Opinion: Explaining High Levels of Approval,” July. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3215138. (pg. 10-11)[/note]</span>

<img class="aligncenter wp-image-305" src="https://bitsandatoms.co/wp-content/uploads/2018/09/Screen-Shot-2018-09-01-at-12.40.38-pm-1024x603.png" alt="" width="550" height="324" />

<span style="font-weight: 400;">Interestingly, respondents with greater socioeconomic advantage (that is, wealthy, highly educated, and urban citizens) and older people (aged 51-65) had the highest levels of approval for SCSs.[note]Ibid. (pg. 13-14)[/note]</span><span style="font-weight: 400;"> Government pilot SCSs also command higher approval ratings than private credit systems, such as Sesame Credit and Tencent Credit.</span>

<span style="font-weight: 400;">An explanation for these strong endorsements of SCSs is the lack of trust in China, both in entities and between citizens. A majority of respondents perceive SCSs as an objective means to achieve a more honest and law abiding society.[note]Ibid. (pg. 21)[/note]</span><span style="font-weight: 400;"> SCSs are also considered to be part of technological progress, which assists governments to foster greater social control, ‘sincerity’, and ‘trustworthiness’. This helps to bridge regulatory gaps, increase the accountability of citizens and institutions, and act as an instrument to improve quality of life. The Chinese government claims that the SCS will “</span><i><span style="font-weight: 400;">broaden channels for public participation in government policymaking, strengthen social supervision over and constraints on the use of power, improve government credibility, establish an honest image of an open, fair and clean government.</span></i><span style="font-weight: 400;">”[note]State Council of the People’s Republic of China. 2014. Planning Outline for the Construction of a Social Credit System (June 14, 2014). English translation by Rogier Creemers. Available at: https://chinacopyrightandmedia.wordpress. com/2014/06/14/planning-outline-for-the-construction-of-a- social-credit-system-2014-2020/ (accessed 31/8/18)[/note]</span><span style="font-weight: 400;"> Chinese citizens are being assured that decisions are being made according to data, rather than arbitrary ‘hunches’ by State representatives. </span>

<b>Conclusion</b>

<span style="font-weight: 400;">The Chinese SCS is the most ambitious and far-reaching data governance plan in history. While many of the stated goals are worthy, there are grave risks. Navigating the plethora of legal and ethical concerns from widespread surveillance will likely be problematic. Also, gathering, connecting, and securing mountains of data across the nation will certainly be a monumental task. Assuming China can establish a nationwide SCS that reflects its stated plans, the risks of it becoming an excessively powerful tool for the State are significant. It may also serve as a model to emulate by other autocratic regimes.</span>

<span style="font-weight: 400;">The State-run SCS is a grand social experiment of unprecedented scale. How it evolves over the coming years will be as fascinating as it is confronting.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>297</wp:post_id>
		<wp:post_date><![CDATA[2018-09-01 02:44:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-01 02:44:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[chinas-social-credit-system-privacy-invasion-or-state-innovation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="category" nicename="china"><![CDATA[China]]></category>
		<category domain="post_tag" nicename="china"><![CDATA[China]]></category>
		<category domain="category" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<category domain="post_tag" nicename="privacy"><![CDATA[Privacy]]></category>
		<category domain="post_tag" nicename="social-credit-system"><![CDATA[Social Credit System]]></category>
		<category domain="post_tag" nicename="surveillance"><![CDATA[Surveillance]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_oembed_242e8a8ce7f7979a4f50a219b5e0cff2]]></wp:meta_key>
			<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[304]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Universal Basic Income: where’s the evidence?</title>
		<link>https://bitsandatoms.co/universal-basic-income/</link>
		<pubDate>Fri, 28 Sep 2018 01:57:24 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=317</guid>
		<description></description>
		<content:encoded><![CDATA[<h4>A distorted history</h4>
<span style="font-weight: 400;">The concept of a guaranteed income is not new. In 1795, a group of magistrates in the English village of Speenhamland devised a plan to alleviate rural inequality caused by rising grain prices.[note]Speizman, Milton D. 1966. “Speenhamland: An Experiment in Guaranteed Income.” <i>The Social Service Review</i> 40 (1): 44–55.[/note]</span><span style="font-weight: 400;"> Poor men and their families received wage supplements to cope with the inflated costs of living. The top-up wages varied according to the number of children and the price of bread. Instead of fixing minimum wages, all poor households in the area received a living wage, regardless of whether they worked or not.</span>

<span style="font-weight: 400;">The Speenhamland system was a reflection of the times. The French Revolution sent a cautionary warning across Europe of the effects of social discontent. As the price of grain continued to rise in England, so too did the threats of social upheaval. The Speenhamland system was, therefore, a means of placating the rural poor by providing a subsistence wage to meet the soaring costs of living.</span>

<span style="font-weight: 400;">Interest in the new welfare model caught on throughout Southern England. Prime Minister William Pitt even tried to institute it as national policy.[note]Craig, Sheryl. 2015. “Pride and Prejudice: The Speenhamland System.” In <i>Jane Austen and the State of the Nation</i>, edited by Sheryl Craig, 48–71. London: Palgrave Macmillan UK.[/note]</span><span style="font-weight: 400;"> Everything appeared to be going well; hunger was alleviated and threats of revolt subsided. A new perspective on welfare was emerging; giving poor people money unconditionally was good for the poor and for society.</span>

<span style="font-weight: 400;">Then the tide of intellectual opinion began to shift. Some of the most influential minds at the time voiced objection. Thomas Malthus, a scholar of political economy and demography, claimed that the Speenhamland system would encourage irresponsible procreation by the poor, contributing to untenable population growth.[note]Malthus, T.R., 1872. <i>An Essay on the Principle of Population.</i>[/note]</span><span style="font-weight: 400;"> Economist and friend, David Ricardo, believed that the welfare system would discourage work.[note]Ricardo, D., 1891. Principles of political economy and taxation. G. Bell.[/note]</span><span style="font-weight: 400;"> And even Karl Marx asserted in </span><i><span style="font-weight: 400;">Das Kapital </span></i><span style="font-weight: 400;">that a guaranteed income would lower wages as it removes the pressures to pay workers a decent wage because subsistence costs had already been covered.[note]Marx, K., 1867. Das Kapital, Band I, English translation by Ben Fowkes of the 4th edition (1894).[/note]</span>

<span style="font-weight: 400;">In 1830, economic conditions worsened and thousands of agricultural workers took to the streets, demanding a living wage. The British government launched a national inquiry into agricultural working conditions, which included an assessment of the Speenhamland system. The 13,000-page report found the Speenhamland system to have been a complete failure. The population had exploded, wages had fallen, and anti-social behaviours had increased. And the Speenhamland system contributed to this disaster.</span>

<span style="font-weight: 400;">Critics were quick to dismiss it as a failed social experiment and promptly reversed welfare policies. The Speenhamland system was dismantled in 1834 and so began the era of ‘workhouses’, one of the more senseless forms of slave labour in the name of ‘public assistance’. </span>

<span style="font-weight: 400;">But as time marched on, the accuracy of the report came into question. A century-and-a-half later historians found a litany of inconsistencies in the report that brought down the Speenhamland system. The text had been written before data had been collected, survey data was significantly incomplete, and almost none of the survey respondents were beneficiaries.[note]Rutger, Bregman. 2018. <i>Utopia for Realists: And How We Can Get There</i>. Bloomsbury. pg. 87.[/note]</span><span style="font-weight: 400;"> The report that had caused an about-face in welfare policy was largely an ideological fabrication. </span>

<span style="font-weight: 400;">Since then, national welfare policies have erred on the side of ‘active’ and conditional measures. That is, providing welfare assistance predicated on certain conditions, such as participation in mandatory training, community service, and job search requirements.</span>
<h4><span style="font-weight: 400;">President Nixon’s backflip</span></h4>
<span style="font-weight: 400;">In 1969, guaranteed income reemerged as President Nixon was preparing a bold poverty-alleviation plan. His proposed policy was to </span><i><span style="font-weight: 400;">unconditionally</span></i><span style="font-weight: 400;"> give every poor family of four in America not earning an income $1,600 per year (approximately US$11,000 today), plus food stamps. The income would progressively decrease as household earnings increased. The programme had broad support from the right and left, even with conservative economist Milton Freidman.[note]Friedman, Milton. 2002. <i>Capitalism and Freedom: Fortieth Anniversary Edition</i>. First Edition. University of Chicago Press. pg. 192.[/note]</span>

<span style="font-weight: 400;">Then support began to spiral. Just before the Bill was put to the Senate an adviser to Nixon presented him with a memo detailing the failures of the Speenhamland system, based on the problematic report. Nixon panicked and concerns spread that such an approach would discentivise the poor from working. The plan was killed in the Senate and work requirements became a mandatory condition of the welfare reforms.</span>
<h4><span style="font-weight: 400;">History rhymes</span></h4>
<span style="font-weight: 400;">Fast forward to today, the looming threats of labour automation have resurrected discussions of a guaranteed income. This time under the guise of a ‘Universal Basic Income’ (UBI). And support has once again come from an ideologically diverse group of high profile figures. Technology entrepreneurs, such as Elon Musk and Mark Zuckerberg, see it as a necessary means to manage the coming structural changes to industry.[note]Clifford, Catherine. 2018. “Elon Musk: Free Cash Handouts from the Government ‘will Be Necessary’ If Robots Take Humans’ Jobs.” CNBC. Reuters Pictures. February 7, 2018. https://www.cnbc.com/2018/06/18/elon-musk-automated-jobs-could-make-ubi-cash-handouts-necessary.html.[/note]</span><span style="font-weight: 400;"> Progressive scholars, like David Graeber, view it as an opportunity to improve the social safety net and equality of opportunity, which would encourage risk-taking and the pursuit of passion.[note]Graeber, David. 2018. <i>Bullshit Jobs: A Theory</i>. 1 edition. Penguin Books Ltd.[/note]</span><span style="font-weight: 400;"> Whereas intellectuals on the right, such as the Adam Smith Institute, advocate for a UBI on the grounds of individualism, claiming it could replace expensive and inefficient welfare programmes.[note]Weber, Emma. n.d. “Nine Arguments against Basic Income Debunked.” Adam Smith Institute. Accessed September 28, 2018. https://www.adamsmith.org/blog/nine-arguments-against-a-basic-income-system-debunked.[/note]</span>

<span style="font-weight: 400;">All of this presumes that this wide and growing group of advocates agree on what UBI is. </span>
<h4><span style="font-weight: 400;">So, what is Universal Basic Income?</span></h4>
<span style="font-weight: 400;">As an overview, Bridgewater Associates, an investment management firm, define UBI as a cash transfer that includes the following characteristics:[note]Dalio, Ray. 2018. “Primer on Universal Basic Income.” https://www.linkedin.com/pulse/primer-universal-basic-income-ray-dalio/.[/note]</span>
<ul>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Universal</span></i><span style="font-weight: 400;">: everyone receives a cash transfer, regardless of employment and earnings status; </span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Unconditional</span></i><span style="font-weight: 400;">: there are no limits on how the cash transfers are to be spent;</span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Basic</span></i><span style="font-weight: 400;">: the amount of the cash transfer will cover the subsistence needs of people in a particular society;</span></li>
 	<li style="font-weight: 400;"><i><span style="font-weight: 400;">Long-term</span></i><span style="font-weight: 400;">: the cash transfers will continue over a long-term period, such as the adult life of an individual. </span></li>
</ul>
<span style="font-weight: 400;">It is important to note that there are still inconsistencies with defining these characteristics. Does ‘Universal’ mean that all of the world’s adult population will receive a basic income, or just populations at national or provincial levels? And what constitutes ‘Basic’? Is it an income level consistent with the national poverty line, or does it also include services like education and healthcare? The extent of UBI varies significantly among its advocates.</span>
<h4><span style="font-weight: 400;">Where is it being trialled?</span></h4>
<span style="font-weight: 400;">Other than some oil-rich countries in the Middle East that transfer oil dividends in the form of public employment subsidies, such as Saudi Arabia and Kuwait, there are only isolated studies. Finland launched a pilot version of a basic income in January of 2017, where a random sample of 2,000 unemployed people were given €560 per month with no obligations. The Finnish government, however, has elected to discontinue the trial at the end of 2018, leading to questions of its efficacy.[note]Weber, Emma. n.d. “Nine Arguments against Basic Income Debunked.” Adam Smith Institute. Accessed September 28, 2018. https://www.adamsmith.org/blog/nine-arguments-against-a-basic-income-system-debunked.[/note]</span>

<span style="font-weight: 400;">Other trials continue. Cities in Canada, Scotland, the Netherlands, Iran, and the US are all in the early stages of running pilot programmes to test the effects of the policy. For example, in 2017, the Silicon Valley startup incubator Y Combinator began funding a multi-year study to test the effects of basic income in Oakland, California.[note]“The First Study of Basic Income in the United States.” n.d. YC Research. Accessed September 27, 2018. https://basicincome.ycr.org/.[/note]</span><span style="font-weight: 400;"> All of these trials will take up to a decade to conclude and properly assess.</span>
<h4><span style="font-weight: 400;">What’s the evidence?</span></h4>
<span style="font-weight: 400;">Despite impassioned claims of UBI’s benefits, there have been no true case studies of its implementation. The evidence that UBI proponents regularly call upon are actually examples of </span><i><span style="font-weight: 400;">unconditional cash transfers to the poor</span></i><span style="font-weight: 400;">. They are not universal and they only serve a segment of the population. </span>

<span style="font-weight: 400;">That said, unconditional cash transfers to the poor have shown encouraging results. In developing countries, they have resulted in:[note]Hagen-Zanker, Jessica, Francesca Bastagli, Luke Harman, Valentina Barca, Georgina Sturge, and Tanja Schmidt. 2016. “Understanding the Impact of Cash Transfers: The Evidence.” ODI. https://www.odi.org/sites/odi.org.uk/files/resource-documents/10748.pdf.[/note]</span>
<ul>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">improved employment outcomes; </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">increased investment and saving rates; </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">no increase in the consumption of ‘temptation goods’ (alcohol, drugs etc.); </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">general improvements in health and education outcomes; </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">increases in entrepreneurial activity; and </span></li>
 	<li style="font-weight: 400;"><span style="font-weight: 400;">a notable decline in child labour. </span></li>
</ul>
<span style="font-weight: 400;">GiveDirectly, a philanthropic organisation that facilitates direct transfers to poor people in rural Kenya, is currently conducting the largest randomised control trial of basic income in history.[note]“GiveDirectly’s Basic Income Guarantee.” n.d. GiveDirectly. Accessed September 27, 2018. https://www.givedirectly.org/basic-income.[/note]</span><span style="font-weight: 400;"> While the programme is still in its early stages, early qualitative results are encouraging and reinforce the findings mentioned above.[note]Douillard, Austin. 2017. “US / KENYA: New Study Published on Results of Basic Income Pilot in Kenya | Basic Income News.” BIEN. March 27, 2017. http://basicincome.org/news/2017/03/us-kenya-new-study-published-results-basic-income-pilot-kenya/.[/note]</span><span style="font-weight: 400;"> The benefits in developed countries have been less apparent, but there are still signs of positive effects on economic and general wellbeing, with only a modest reduction in work effort (5-10% less).[note]Widerquist, Karl. 2005. “A Failure to Communicate: What (if Anything) Can We Learn from the Negative Income Tax Experiments?” <i>The Journal of Socio-Economics</i> 34 (1): 49–81.[/note]</span>
<h4><span style="font-weight: 400;">Conclusion</span></h4>
<span style="font-weight: 400;">While UBI is an interesting concept to consider, it is far from obvious that it is the optimal welfare policy. The evidence that does exist relates specifically to unconditional cash transfers to the poor (particularly the very poor in developing countries), which does not satisfy the ‘Universal’ characteristic of UBI. This evidence, however, could provide grounds for a new approach to foreign aid, philanthropy, and how aspects of welfare are distributed, but it is not the same as UBI.</span>

<span style="font-weight: 400;">Additionally, a UBI presupposes that giving money unconditionally to people is </span><i><span style="font-weight: 400;">always</span></i><span style="font-weight: 400;"> better than directing resources to targeted programmes. These broad assertions are more ideological than factual. There are a range of active and targeted measures that are proven to have positive outcomes on welfare, such as job search assistance and targeted training programmes for unemployed populations.[note]Card, David, Jochen Kluve, and Andrea Weber. 2018. “What Works? A Meta-Analysis of Recent Active Labor Market Program Evaluations.” <i>Journal of the European Economic Association</i> 16 (3): 894–931; OECD. 2018. <i>OECD Employment Outlook 2018</i>. Paris: OECD Publishing.[/note]</span>

<span style="font-weight: 400;">And the costs appear prohibitively expensive. Recent economic modeling found that even if all social spending (ex-healthcare) were removed to fund a UBI, advanced OECD countries would struggle to provide their citizens with a poverty-level wage.[note]Dalio, Ray. 2018. “Primer on Universal Basic Income.” https://www.linkedin.com/pulse/primer-universal-basic-income-ray-dalio/.[/note]</span><span style="font-weight: 400;"> This is even the case under scenarios where basic income payments would progressively decline to zero once citizens began earning US$120,000 pa. (technically not a UBI, but a Negative Income Tax)</span>

<span style="font-weight: 400;">Regardless of costs, I don’t believe UBI is a viable option. The evidence is not established and doing away with </span><i><span style="font-weight: 400;">all</span></i><span style="font-weight: 400;"> targeted measures dismisses a lot of great progress. Where I am intrigued and optimistic, however, are the growing incidences of unconditional cash transfers to the poor. The evidence is encouraging and there’s potential for these measures to become a growing part of national welfare policies. The current pilot schemes will provide the evidence base to assess their suitability for future policies. But implementing a UBI in the absence of strong evidence is politically and economically unrealistic.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>317</wp:post_id>
		<wp:post_date><![CDATA[2018-09-28 01:57:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-28 01:57:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[universal-basic-income]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="post_tag" nicename="economics"><![CDATA[Economics]]></category>
		<category domain="post_tag" nicename="labour-markets"><![CDATA[Labour Markets]]></category>
		<category domain="post_tag" nicename="public-policy"><![CDATA[Public Policy]]></category>
		<category domain="post_tag" nicename="universal-basic-income"><![CDATA[Universal Basic Income]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[10]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[319]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Automation Anxiety</title>
		<link>https://bitsandatoms.co/automation-anxiety/</link>
		<pubDate>Fri, 26 Oct 2018 10:38:42 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=327</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">The awakening of the Industrial Revolution evoked fears of mass-unemployment by mechanical automation. In the 19th century, textile workers formed the ‘Luddite Movement’ to protest the automation of textile production. Workers damaged or destroyed machines and threatened the unravelling of social order. </span>

<span style="font-weight: 400;">Automation anxiety was also been prevalent in periods of economic stagnation. During the Great Depression, John Maynard Keynes voiced concerns on technological unemployment in his 1930 essay, ‘Economic Possibilities for our Grandchildren’:[note]John Maynard Keynes. 1930. “Economic Possibilities for our Grandchildren” in Essays in Persuasion (New York: Harcourt Brace, 1932), 358-373.[/note]</span>
<p style="padding-left: 30px;"><span style="font-weight: 400;">"</span><i><span style="font-weight: 400;">We are suffering, not from the rheumatics of old age, but from the growing-pains of over-rapid changes, from the painfulness of readjustment between one economic period and another. The increase of technical efficiency has been taking place faster than we can deal with the problem of labour absorption; the improvement in the standard of life has been a little too quick.</span></i><span style="font-weight: 400;">" </span></p>
<span style="font-weight: 400;">And similar concerns have been raised during periods of rapid economic growth and industriousness. Anxieties of technological unemployment in the 1950s and 1960s were severe enough that in 1964, President Lyndon Johnson created a  ‘Blue-Ribbon National Commission on Technology, Automation, and Economic Progress’.[note]“Lyndon B. Johnson: Remarks Upon Signing Bill Creating the National Commission on Technology, Automation, and Economic Progress.” n.d. Accessed October 2, 2018. http://www.presidency.ucsb.edu/ws/?pid=26449.[/note]</span><span style="font-weight: 400;"> The purpose of the commission was to assess whether rapid productivity growth would outstrip the demand for labour. The commission found that while technological change can cause structural shifts to the types of jobs demanded, there is no evidence that it eliminates the demand for labour:[note]Bowen, Harold R. (Chairman). 1966. “Report of the National Commission on Technology, Automation, and Economic Progress: Volume I.” Washington: U.S. Government Printing Office.[/note]</span>
<p style="padding-left: 30px;"><span style="font-weight: 400;">“</span><i><span style="font-weight: 400;">The basic fact is that technology eliminates jobs, not work.</span></i><span style="font-weight: 400;">”</span></p>
<span style="font-weight: 400;">These anxieties have been a constant throughout history. It echoes back to Greek Mythology as cautioned through the ‘Promethean Myth’: Man acquires fire from the Gods; fire becomes the source of Man’s pain and suffering.[note]“PROMETHEUS - Greek Titan God of Forethought, Creator of Mankind.” n.d. Accessed October 2, 2018. http://www.theoi.com/Titan/TitanPrometheus.html.[/note]</span><span style="font-weight: 400;"> We hear variations of the same narrative today: humans build and deploy AI; AI replaces our jobs and our purpose.[note]Harari, Yuval Noah. 2017. <i>Homo Deus: A Brief History of Tomorrow</i>. VINTAGE ARROW - MASS MARKET.[/note]</span>

<span style="font-weight: 400;">Yet, the fears of the ‘Promethean Myth’ have been consistently unfounded.[note]Autor, David H. 2015. “Why Are There Still So Many Jobs? The History and Future of Workplace Automation.” <i>The Journal of Economic Perspectives: A Journal of the American Economic Association</i> 29 (3): 3–30. pg. 3-5; Mokyr, Joel, Chris Vickers, and Nicolas L. Ziebarth. 2015. “The History of Technological Anxiety and the Future of Economic Growth: Is This Time Different?” <i>The Journal of Economic Perspectives: A Journal of the American Economic Association</i> 29 (3): 31–50.[/note]</span><span style="font-weight: 400;"> To take the position that ‘this time is different’ is an abnegation of economic history. </span>

<span style="font-weight: 400;">Regardless, automation anxieties have re-emerged. As AI increases the scope of capabilities that can be automated by machines, concerns abound that many labour tasks performed by humans will be replaced. In their provoking book, </span><i><span style="font-weight: 400;">The Second Machine Age</span></i><span style="font-weight: 400;">, scholars Erik Brynjolfsson and Andrew McAfee present a troubling view of the effects of labour automation:[note]Brynjolfsson, Erik, and Andrew McAfee. 2016. <i>The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies</i>. 1 edition. W. W. Norton &amp; Company. pg. 11.[/note]</span>
<p style="padding-left: 30px;"><span style="font-weight: 400;">“</span><i><span style="font-weight: 400;">Technological progress is going to leave behind some people, perhaps even a lot of people, as it races ahead. As we’ll demonstrate, there’s never been a better time to be a worker with special skills or the right education, because these people can use technology to create and capture value. However, there’s never been a worse time to be a worker with only ‘ordinary’ skills and abilities to offer, because computers, robots, and other digital technologies are acquiring these skills and abilities at an extraordinary rate.</span></i><span style="font-weight: 400;">”</span></p>
<span style="font-weight: 400;">It is obvious, however, that the past two centuries have not rendered human labour obsolete. Global unemployment has decreased,[note]“Unemployment, Total (% of Total Labor Force) (modeled ILO Estimate) | Data.” n.d. Accessed October 2, 2018. https://data.worldbank.org/indicator/sl.uem.totl.zs.[/note]</span><span style="font-weight: 400;"> and the employment-to-population ratio steadily rose during the 20th century.[note]Autor, David H. 2015. “Why Are There Still So Many Jobs? The History and Future of Workplace Automation.” <i>The Journal of Economic Perspectives: A Journal of the American Economic Association</i> 29 (3): 3–30. pg. 4.[/note]</span><span style="font-weight: 400;"> In fact, a study performed by Deloitte found that new technologies over the past 144 years have created more jobs than they have destroyed.[note]Stewart, Ian, Debapratim De, and Alex Cole. 2015. “Technology and People: The Great Job-Creating Machine.” Deloitte. https://www2.deloitte.com/content/dam/Deloitte/uk/Documents/finance/deloitte-uk-technology-and-people.pdf.[/note]</span>

<span style="font-weight: 400;">This rebuttal to automation anxiety is not to dismiss the structural challenges that AI technologies pose to labour markets. Nor is it to place a blind faith in the determinism of history. The challenges of transitioning displaced workers to meet the new labour demands created by AI are likely to be significant. Potentially as profound as the structural adjustments experienced during the Agricultural and Industrial Revolutions.[note]Brynjolfsson, Erik, Daniel Rock, and Chad Syverson. 2018. “Artificial Intelligence and the Modern Productivity Paradox: A Clash of Expectations and Statistics.” In <i>The Economics of Artificial Intelligence: An Agenda</i>. University of Chicago Press.[/note]</span>

<span style="font-weight: 400;">However, history suggests that the demand for labour is not the concern. Instead, the concern is the speed of transitioning displaced workers to meet new labour demands. In other words, new jobs will probably be created by AI, but transitioning people to a future of work defined by AI could be problematic. At least in the short-term.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>327</wp:post_id>
		<wp:post_date><![CDATA[2018-10-26 10:38:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-10-26 10:38:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[automation-anxiety]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence-policy"><![CDATA[Artificial Intelligence Policy]]></category>
		<category domain="post_tag" nicename="automation"><![CDATA[Automation]]></category>
		<category domain="post_tag" nicename="future-of-work"><![CDATA[Future of Work]]></category>
		<category domain="post_tag" nicename="history"><![CDATA[History]]></category>
		<category domain="post_tag" nicename="labour-markets"><![CDATA[Labour Markets]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[328]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[60]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[3]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>How AI automation could boost employment: The role of demand</title>
		<link>https://bitsandatoms.co/how-ai-automation-could-boost-employment-the-role-of-demand/</link>
		<pubDate>Wed, 28 Nov 2018 02:50:32 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=332</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">Technologies that automate labour tasks do not necessarily increase unemployment. Despite widespread concerns that Artificial Intelligence (AI) will displace workers </span><i><span style="font-weight: 400;">en masse</span></i><span style="font-weight: 400;">, there are periods of history where productivity-enhancing technologies have actually </span><i><span style="font-weight: 400;">increased</span></i><span style="font-weight: 400;"> employment in the affected industries. This runs counter to the simplistic notion that ‘automation causes job losses’ in the industries experiencing automation. </span>

<span style="font-weight: 400;">So, why does automation lead to employment growth in some industries at particular times, while leading to job losses at other times and in other industries?</span>

<a href="https://www.bu.edu/law/profile/james-bessen/"><span style="font-weight: 400;">James Bessen</span></a><span style="font-weight: 400;">, an Economist from Boston University, argues that it has to do with how technology affects the nature of demand.[note]Bessen, James. 2015. Learning by Doing: The Real Connection between Innovation, Wages, and Wealth. Yale University Press; </span>

<span style="font-weight: 400;">Bessen, James E. 2018. “Automation and Jobs: When Technology Boosts Employment.” https://doi.org/10.2139/ssrn.2935003;</span>

<span style="font-weight: 400;">Bessen, James. 2018. “AI and Jobs: The Role of Demand.” Working Paper Series. National Bureau of Economic Research. https://doi.org/10.3386/w24235.[/note]</span>

<span style="font-weight: 400;">Employment in US manufacturing has dramatically fallen in recent decades. But for over a century, employment grew in various manufacturing industries, even in those that experienced rapid technological and productivity changes. This phenomenon is described as the ‘Inverted U’ pattern and appears to have general applicability across many manufacturing industries.[note]Buera, Francisco J., and Joseph P. Kaboski. 2009. “Can Traditional Theories of Structural Change Fit the Data?” Journal of the European Economic Association 7 (2-3): 469–77; </span>

<span style="font-weight: 400;">Rodrik, Dani. 2016. “Premature Deindustrialization.” Journal of Economic Growth 21 (1): 1–33.[/note]</span>

<span style="font-weight: 400;">Bessen has developed a model of industry demand to explain the rise and subsequent fall of US manufacturing employment in the context of ongoing productivity growth. After gathering two centuries of time series data on US cotton textile, raw steel, and automotive manufacturing industries, Bessen’s model was able to accurately predict the rise and fall of Production Employment in these three manufacturing industries.</span>

<img class="aligncenter wp-image-333" src="https://bitsandatoms.co/wp-content/uploads/2018/11/Screen-Shot-2018-11-28-at-1.17.02-pm-789x1024.png" alt="" width="522" height="678" />

<span style="font-weight: 400;">The Inverted U patterns of industry employment observed above[note]Bessen, James. 2018. “AI and Jobs: The Role of Demand.” Working Paper Series. National Bureau of Economic Research. <a href="https://doi.org/10.3386/w24235">https://doi.org/10.3386/w24235</a>. pg. 25.[/note] are largely explained by declining price elasticities of demand. That is, the declining responsiveness of consumption to changes in price. While variations in consumer income play a role in these instances, they only account for a small portion of the total variation in per capita consumption.[note]Bessen, James E. 2018. “Automation and Jobs: When Technology Boosts Employment.” <a href="https://doi.org/10.2139/ssrn.2935003">https://doi.org/10.2139/ssrn.2935003</a>. pg. 22.[/note]</span><span style="font-weight: 400;"> Nonetheless, Bessen’s model includes both price and income effects on demand, allowing the elasticities of both to change over time.</span>

<b>A primer on the economics of supply, demand, and elasticity</b>

<span style="font-weight: 400;">To illustrate these dynamics, consider the impacts that a new technology (for example, the automated weaving loom) can have on the supply and demand for a good in a competitive market (such as cotton cloth), with a focus on price.</span>

<img class="aligncenter wp-image-334" src="https://bitsandatoms.co/wp-content/uploads/2018/11/Screen-Shot-2018-11-28-at-1.20.21-pm-1024x720.png" alt="" width="561" height="395" />

<span style="font-weight: 400;">When elasticity is high, a small change in price has a disproportionately large impact on the quantity demanded. For example, if a new technology improves the productivity of output, this decreases the production costs per unit of cloth. In competitive markets, the productivity savings can be passed on to consumers by lowering prices. In the example above, the price of cotton cloth per unit decreases from $4 to $3 (</span><i><span style="font-weight: 400;">point A</span></i><span style="font-weight: 400;"> to </span><i><span style="font-weight: 400;">point B</span></i><span style="font-weight: 400;">). This yields a 10 unit increase in the quantity demanded. But if the price were to further decline from $3 to $2 (</span><i><span style="font-weight: 400;">point B</span></i><span style="font-weight: 400;"> to </span><i><span style="font-weight: 400;">point C</span></i><span style="font-weight: 400;">), the proportional change of price required to achieve another 10 unit increase is higher. That is, to achieve a 10 unit increase in quantity demanded at </span><i><span style="font-weight: 400;">point A</span></i><span style="font-weight: 400;"> to </span><i><span style="font-weight: 400;">point B</span></i><span style="font-weight: 400;"> required a 25% decrease in price; whereas achieving another 10 unit increase from </span><i><span style="font-weight: 400;">point B</span></i><span style="font-weight: 400;"> to </span><i><span style="font-weight: 400;">point C</span></i><span style="font-weight: 400;"> requires a 33% drop in price. So, demand is becoming more ‘inelastic’ because consumers are less responsive to changes in price.</span>

<span style="font-weight: 400;">When a new productivity-enhancing technology is introduced, it reduces the production costs per unit of the good, shifting (increasing) the supply curve right. This drives down the price of the good until a new market equilibrium is met (point B above). When demand is elastic with respect to price, this indicates unmet consumer demands, so changes in price have disproportionately large impacts on quantities demanded. If the demand for the good grows fast enough, then the demand for labour will increase despite the automating technology reducing the labour required per unit of output. This can offset the labour-saving effects of automation.</span>

<span style="font-weight: 400;">If demand is inelastic, however, then price changes only yield modest changes in the quantities demanded. When productivity-enhancing technologies continue to lower the costs per unit, thus lowering the prices, consumers’ demands become satiated. Under this scenario, price declines are insufficient to raise net employment. These are the general dynamics that Bessen has observed in US manufacturing.</span>

<b>How automation increased (and then decreased) employment in textiles manufacturing</b>

<span style="font-weight: 400;">During the 19th century, technologies had automated 98% of the labour required to weave a yard of cloth. Yet, the number of weaving jobs actually increased for decades over this period.[note]Bessen, James. 2015. Learning by Doing: The Real Connection between Innovation, Wages, and Wealth. Yale University Press.[/note]</span><span style="font-weight: 400;"> The basic intuition is that most consumers were priced out of the market. Many consumers had limited sets of clothes and other uses for cloth, which meant there was a lot of unmet consumer demand. As a result, the price elasticities of demand and income were high because consumers were sensitive to the price of cloth.</span>

<span style="font-weight: 400;">As automated weaving looms diffused throughout the textile industry, productivity improvements drove down the price of cloth. This increased the consumption of cloth per capita because the demand for cloth was highly elastic. To meet this consumer demand, the textile industry experienced net jobs growth, despite the labour-saving technology reducing the labour required per unit of output. Essentially, the increase in consumer demand more than offset the labour-saving effects of the automation technologies. Bessen found that although labour productivity in cotton textiles increased nearly 30-fold during the 19th century, consumption of cotton cloth increased 100-fold.[note]Bessen, James E. 2018. “Automation and Jobs: When Technology Boosts Employment.” <a href="https://doi.org/10.2139/ssrn.2935003">https://doi.org/10.2139/ssrn.2935003</a>. pg. 7.[/note]</span>

<span style="font-weight: 400;">Over time, however, the ongoing productivity gains present in the textile industry drove down the price of cloth until consumer demand had become satiated. As demand became relatively inelastic, further price declines and income growth only generated anaemic increases in consumption, which failed to offset the labour-saving effects of automation. </span>

<span style="font-weight: 400;">So began the downward trajectory of textile employment, as seen in the latter parts of the Inverted U graph. The technical changes that created productivity advances would eventually lead to deindustrialisation. The elasticity of demand for price and income thus became more inelastic as cheap cloth became readily available to consumers. As a result, productivity advances later experienced depressed textile employment.</span>

<b>AI automation and the importance of demand</b>

<span style="font-weight: 400;">The current debate regarding the impacts of AI on jobs too often reverts to a false dichotomy. On the one hand, AI is positioned as a universal panacea that will bring about a productivity boom like that we’ve never seen; on the other, AI automation is said to pose a widespread threat to our current jobs and future prospects. Neither sides are helpful in explaining the past or predicting the future.</span>

<span style="font-weight: 400;">Bessen shows that the pace of technological change alone is not sufficient in determining the impacts of technologies on employment. It is essential to understand the nature of demand to determine whether major new technologies will increase or decrease employment in affected industries. If demand for a particular good or service is highly elastic and AI does not completely automate the production of that good or service, then technical change would </span><i><span style="font-weight: 400;">create jobs</span></i><span style="font-weight: 400;"> rather destroy them. In this scenario, faster rates of productivity-enhancing change by AI would create </span><i><span style="font-weight: 400;">faster</span></i><span style="font-weight: 400;"> employment growth to meet consumer demand.</span>

<span style="font-weight: 400;">Therefore, understanding the effects of AI on demand is essential to understanding its implications on employment.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>332</wp:post_id>
		<wp:post_date><![CDATA[2018-11-28 02:50:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-11-28 02:50:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[how-ai-automation-could-boost-employment-the-role-of-demand]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="post_tag" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="automation"><![CDATA[Automation]]></category>
		<category domain="post_tag" nicename="economics"><![CDATA[Economics]]></category>
		<category domain="category" nicename="economics"><![CDATA[Economics]]></category>
		<category domain="post_tag" nicename="future-of-work"><![CDATA[Future of Work]]></category>
		<category domain="post_tag" nicename="labour-markets"><![CDATA[Labour Markets]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[346]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[53]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>Introducing AI NewsBot</title>
		<link>https://bitsandatoms.co/introducing-ai-newsbot/</link>
		<pubDate>Wed, 20 Feb 2019 02:41:38 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=352</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">There’s so much activity happening in AI nowadays, it’s hard to keep up. Interesting news and research are published daily across different platforms. So, people are routinely monitoring a host of platforms and sources to stay informed. This is both time-consuming and full of distractions. </span>

<span style="font-weight: 400;">It would be great if a service automatically aggregated the latest AI news and research from these top platforms. Even better if it could send a tidy news summary to your Inbox on the days of your choice!</span>

<span style="font-weight: 400;">Enter <a href="https://ainews.bitsandatoms.co/">AI NewsBot</a>...</span>

<span style="font-weight: 400;">The AI NewsBot is a curated Artificial Intelligence news aggregator. The AI NewsBot automatically fetches AI News &amp; Research from high-quality and interesting sources. It then updates the News Feed every day and sends a FREE email to subscribers at 7am AEST on the days they have chosen.</span>

<span style="font-weight: 400;">My friend, </span><a href="https://www.peterargent.com/"><span style="font-weight: 400;">Pete</span></a><span style="font-weight: 400;">, and I wanted to build a system that automatically sends an email summary of the AI news sources that we routinely check. After using it ourselves, we found it very useful for staying informed and saving time. So, we’ve decided to share the AI NewsBot by enabling you to subscribe for FREE and receive emails on the days you want.</span>

<span style="font-weight: 400;">If there are AI news sources that you would like to see included, please </span><a href="https://ainews.bitsandatoms.co/#suggest"><span style="font-weight: 400;">suggest a news source</span></a><span style="font-weight: 400;"> by filling out the form at the bottom of the <a href="https://ainews.bitsandatoms.co/">AI NewsBot</a> page. Alternatively, if the news source has already been suggested, click the ‘Like’ button for that source.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>352</wp:post_id>
		<wp:post_date><![CDATA[2019-02-20 02:41:38]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-02-20 02:41:38]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[introducing-ai-newsbot]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="ai-news"><![CDATA[AI News]]></category>
		<category domain="post_tag" nicename="ai-news"><![CDATA[AI News]]></category>
		<category domain="post_tag" nicename="ai-resources"><![CDATA[AI Resources]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[Machine Learning]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[355]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[60]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[54]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[post_image]]></wp:meta_key>
			<wp:meta_value><![CDATA[https://bitsandatoms.co/wp-content/uploads/2019/02/Screen-Shot-2019-02-20-at-1.38.41-pm.png]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_post_image_attach_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[355]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
	</item>
	<item>
		<title>6 Major Factors Affecting AI Adoption and Diffusion in Firms</title>
		<link>https://bitsandatoms.co/6-major-factors-affecting-ai-adoption-and-diffusion-in-firms/</link>
		<pubDate>Wed, 20 Mar 2019 06:40:25 +0000</pubDate>
		<dc:creator><![CDATA[nikolasjdawson@gmail.com]]></dc:creator>
		<guid isPermaLink="false">https://bitsandatoms.co/?p=360</guid>
		<description></description>
		<content:encoded><![CDATA[<span style="font-weight: 400;">People are quick to sound the alarm on AI. Yells of widespread automation and labour obsolescence pervade the media. Yet, beneath the punditry and prognostications is a more complex story. Central to this complexity is predicting how AI is adopted by firms and diffused throughout economies.</span>

<span style="font-weight: 400;">Firms are the primary driver of economic activity in economies. If firms are slow or fail to adopt AI, then its effects are naturally restricted. The ‘promise of AI’ can only be fulfilled if these technologies are adopted by firms, absorbed in workflows, and broadly diffused.[note]</span><span style="font-weight: 400;">Working Definitions</span><span style="font-weight: 400;">: </span><i><span style="font-weight: 400;">Adoption</span></i><span style="font-weight: 400;"> is defined as investment in a particular technology; </span><i><span style="font-weight: 400;">Diffusion</span></i><span style="font-weight: 400;"> is defined as how adoption spreads—the process by which an innovation is diffused in a market or social system; and </span><i>Absorption</i> is how technology is used within a firm.[/note] Otherwise, they’re just isolated use cases.

<span style="font-weight: 400;">This consideration, however, has largely been ignored. Much of the recent research on the economic impacts of AI has focused on AI’s estimated capabilities </span><b><i>if</i></b><span style="font-weight: 400;"> the technologies are broadly diffused. For example, the prominent study by Frey and Osborne[note]Frey, Carl Benedikt, and Michael A. Osborne. 2017. “The Future of Employment: How Susceptible Are Jobs to Computerisation?” <i>Technological Forecasting and Social Change</i> 114 (January): 254–80.[/note]</span><span style="font-weight: 400;"> estimated that 47% of occupations face a near-term risk of automation from AI. These results were based on the assessments of a small panel of Machine Learning experts who were asked to identify which of 70 jobs were ‘completely automatable’ in 2013. But these projections rely on some shaky assumptions. Chief among them is that firms will quickly and efficiently adopt AI for commercial use. This should not be taken as a given. As James Bessen et al.[note]Bessen, James E., Stephen Michael Impink, Robert Seamans, and Lydia Reichensperger. 2018. “The Business of AI Startups.” <a href="https://doi.org/10.2139/ssrn.3293275">https://doi.org/10.2139/ssrn.3293275</a>.[/note]</span><span style="font-weight: 400;"> points out, just because new technologies have commercial applications does not mean that they will be adopted and diffused in a timely manner.</span>

<span style="font-weight: 400;">Therefore, understanding the factors that influence the adoption and diffusion of AI in firms is important. It enables more accurate forecasting and better planning for policymakers, businesses, and civil society. Identifying the main levers that drive the growth of AI applications can help to expedite the many positive use cases in the pipeline; such as Machine Learning disease diagnosis systems in healthcare and optimised renewable energy distribution at scale. It also empowers governments and firms to make pragmatic and timely decisions on the negative implications of AI diffusion. For instance, workers at risk of displacement by automation can be better identified and more targeted transition support can be provided.</span>

<span style="font-weight: 400;">Developing robust forecast models, however, first requires identifying and measuring the variables that influence adoption and diffusion. A discussion of the major factors that affect the rate of AI adoption follows.</span>

<b>Explanatory variables for AI adoption and diffusion</b>

<span style="font-weight: 400;">Research on the factors that affect firms’ decisions to adopt digital technologies is well established. Researchers have closely examined the adoption dynamics of innovations such as personal computers,[note]Thong, J.Y.L.1999. An integrated model of information systems adoption in small businesses, "Journal of Management Information Systems", Vol. 15, No. 4, pp. 187-214.[/note]</span><span style="font-weight: 400;"> the Internet,[note]Andrés, Luis, David Cuberes, Mame Diouf, and Tomás Serebrisky. 2010. “The Diffusion of the Internet: A Cross-Country Analysis.” <i>Telecommunications Policy</i> 34 (5): 323–40.[/note]</span><span style="font-weight: 400;"> and social media.[note]Bughin, Jacques. 2016. “The Diffusion Pattern of Enterprise 2.0 Technologies: Worldwide Estimates of a Bass Co-Diffusion Model for the Last 10 Years.” <i>Journal of Contemporary Management</i>, December. <a href="http://www.bapress.ca/jcm/jcm-article/1929-0136-2017-02-31-12.pdf">http://www.bapress.ca/jcm/jcm-article/1929-0136-2017-02-31-12.pdf</a>.[/note]</span><span style="font-weight: 400;"> AI builds upon these digital technologies. And the factors that influence the adoption of AI by firms differ by degree but not by kind.</span>

<span style="font-weight: 400;">Building upon past research, and accounting for the unique characteristics of AI, there are six major variables that explain the rate at which firms adopt AI:</span>
<ol>
 	<li style="font-weight: 400;"><b>Competition</b><span style="font-weight: 400;">: Recent modeling conducted by McKinsey Global Institute[note]Bughin, Jacques, Jeongmin Seong, James Manyika, Michael Chui, and Raoul Joshi. 2018. “Modeling the Impact of AI on the World Economy.” McKinsey Global Institute. <a href="https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/Notes%20from%20the%20frontier%20Modeling%20the%20impact%20of%20AI%20on%20the%20world%20economy/MGI-Notes-from-the-frontier-Modeling-the-impact-of-AI-on-the-world-economy-September-2018.ashx">https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/Notes%20from%20the%20frontier%20Modeling%20the%20impact%20of%20AI%20on%20the%20world%20economy/MGI-Notes-from-the-frontier-Modeling-the-impact-of-AI-on-the-world-economy-September-2018.ashx</a>; Bughin, Jacques, Jeongmin Seong, James Manyika, Lari Hämäläinen, Eckart Windhagen, and Eric Hazan. 2019. “Notes from the AI Frontier: Tackling Europe’s Gap in Digital and AI.” McKinsey Global Institute. <a href="https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligence/tackling%20europes%20gap%20in%20digital%20and%20ai/mgi-tackling-europes-gap-in-digital-and-ai-feb-2019-vf.ashx">https://www.mckinsey.com/~/media/mckinsey/featured%20insights/artificial%20intelligence/tackling%20europes%20gap%20in%20digital%20and%20ai/mgi-tackling-europes-gap-in-digital-and-ai-feb-2019-vf.ashx</a>.[/note]</span><span style="font-weight: 400;"> found that the extent of rivalry within markets has the largest effect on AI adoption. This is consistent with game theory, where the marginal propensity to adopt AI depends on the proportion of rivals that have already decided to adopt. Assuming the new technology becomes broadly diffused, then early adopters typically enjoy disproportionate rewards. But as more firms adopt, the marginal incentive to adopt diminishes as the technology provides less competitive advantages. This is why laggard firms are punished with shrinking market shares. These competitive forces, therefore, drive adoption rates as firms jostle to assert a competitive edge and advance market share. However, adoption decisions are made with imperfect information. It’s difficult for a firm to know what its competitors are doing behind closed doors. So, these competitive forces (or FOMO!) can drive rapid periods of adoption growth. </span></li>
 	<li style="font-weight: 400;"><b>Firm characteristics</b><span style="font-weight: 400;">: The size, income level, and industry of firms have all been shown to affect the rate that a new technology is adopted.[note]Hall, Bronwyn H., and Beethika Khan. 2003. “Adoption of New Technology.” Working Paper Series. National Bureau of Economic Research. <a href="https://doi.org/10.3386/w9730">https://doi.org/10.3386/w9730</a>. pg. 20.[/note]</span><span style="font-weight: 400;"> For example, larger firms, by headcount and income, typically adopt digital technologies earlier and at faster rates than smaller firms. Also, firms in Financial Services and ICT industries tend to adopt digital technologies earlier and at faster rates than firms in Agricultural and Construction industries.[note]<i>For example, see</i> Australian Bureau of Statistics. 2019. ‘Catalogue 8129.0 - Business Use of Information Technology’. <a href="https://www.abs.gov.au/ausstats/abs@.nsf/mf/8129.0">https://www.abs.gov.au/ausstats/abs@.nsf/mf/8129.0</a>[/note]</span></li>
 	<li style="font-weight: 400;"><b>Workforce skill capabilities</b><span style="font-weight: 400;">: Emerging technologies, such as AI, often require specific skills. The availability of workers with these skills can influence the extent of adoption and diffusion. The ability to access such labour competencies, however, varies between firms, industries, and economies. The implementation of AI requires strong technical competencies. These competencies are unevenly distributed between firms, industries, and economies. Therefore, the more firms are able to access relevant skilled labour, the greater the likelihood that firms will adopt AI.</span></li>
 	<li style="font-weight: 400;"><b>Digital maturity</b><span style="font-weight: 400;">: Previous research has shown that the adoption of new digital technologies often depends on the adoption of previous digital technologies. For instance, broadband infrastructure supports the adoption of more sophisticated digital applications.[note]Andrews, Dan, Giuseppe Nicoletti, and Christina Timiliotis. 2018. “Digital Technology Diffusion: A Matter of Capabilities, Incentives or Both?” OECD. <a href="https://doi.org/10.1787/7c542c16-en">https://doi.org/10.1787/7c542c16-en</a>.[/note]</span><span style="font-weight: 400;"> This relationship also appears to hold for AI. Firms that have adopted and absorbed cloud infrastructure and ‘web 2.0 technologies’ (such as mobile technologies and CRM systems) are more likely to adopt AI technologies.[note]Bughin, Jacques, Jeongmin Seong, James Manyika, Michael Chui, and Raoul Joshi. 2018. “Modeling the Impact of AI on the World Economy.” McKinsey Global Institute. <a href="https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/Notes%20from%20the%20frontier%20Modeling%20the%20impact%20of%20AI%20on%20the%20world%20economy/MGI-Notes-from-the-frontier-Modeling-the-impact-of-AI-on-the-world-economy-September-2018.ashx">https://www.mckinsey.com/~/media/McKinsey/Featured%20Insights/Artificial%20Intelligence/Notes%20from%20the%20frontier%20Modeling%20the%20impact%20of%20AI%20on%20the%20world%20economy/MGI-Notes-from-the-frontier-Modeling-the-impact-of-AI-on-the-world-economy-September-2018.ashx</a>.[/note]</span></li>
 	<li style="font-weight: 400;"><b>Expected return on AI investment</b><span style="font-weight: 400;">: Firms’ perceptions of the value that AI can create also influences adoption rates. Unsurprisingly, firms that are positive about the business use cases of AI are more likely to adopt earlier and faster. Conversely, firms that are uncertain about AI use cases are slower or less likely to adopt, which delays aggregate adoption rates.</span></li>
 	<li style="font-weight: 400;"><b>AI complements</b><span style="font-weight: 400;">: The more a firm invests in one type of AI, the more likely it will invest in another. For example, a retailer that implements robotic process automation to retrieve stock is more likely to adopt computer vision to identify inventory items than a retailer that hasn’t adopted any AI technologies. Capital investment deepens as AI is increasingly absorbed in workflows.</span></li>
</ol>
<span style="font-weight: 400;">Of course, there are other variables that can influence the firm-level adoption of AI. For instance, regulatory effects can be important to consider when comparing the adoption rates between country economies; it’s plausible that the more stringent data protection regulations in Europe could delay AI adoption in European firms compared to US firms. Relative rates of performance improvement for a technology can also affect diffusion rates, particularly in the earlier phases of adoption. But in order to simplify a forecast diffusion model of AI, the explanatory variables listed above account for a significant proportion of firm-level adoption decisions.</span>

<span style="font-weight: 400;">Future posts will explain how these explanatory variables can be used in a model to forecast the adoption of AI in an economy.</span>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>360</wp:post_id>
		<wp:post_date><![CDATA[2019-03-20 06:40:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-03-20 06:40:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[6-major-factors-affecting-ai-adoption-and-diffusion-in-firms]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
		<category domain="category" nicename="ai-policy"><![CDATA[AI Policy]]></category>
		<category domain="category" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence"><![CDATA[Artificial Intelligence]]></category>
		<category domain="post_tag" nicename="artificial-intelligence-policy"><![CDATA[Artificial Intelligence Policy]]></category>
		<category domain="post_tag" nicename="economics"><![CDATA[Economics]]></category>
		<category domain="category" nicename="economics"><![CDATA[Economics]]></category>
		<category domain="post_tag" nicename="forecast-modeling"><![CDATA[Forecast Modeling]]></category>
		<category domain="post_tag" nicename="technology-adoption"><![CDATA[Technology Adoption]]></category>
		<category domain="post_tag" nicename="technology-diffusion"><![CDATA[Technology Diffusion]]></category>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
			<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_thumbnail_id]]></wp:meta_key>
			<wp:meta_value><![CDATA[375]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_content_score]]></wp:meta_key>
			<wp:meta_value><![CDATA[30]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_primary_category]]></wp:meta_key>
			<wp:meta_value><![CDATA[53]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[builder_switch_frontend]]></wp:meta_key>
			<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_themify_builder_settings_json]]></wp:meta_key>
			<wp:meta_value><![CDATA[[{"row_order":"0","gutter":"gutter-default","equal_column_height":"","column_alignment":"col_align_top","cols":[{"column_order":"0","grid_class":"col-full first last","grid_width":"","modules":[],"styling":[]}],"styling":[]}]]]></wp:meta_value>
		</wp:postmeta>
		<wp:postmeta>
			<wp:meta_key><![CDATA[_yoast_wpseo_metadesc]]></wp:meta_key>
			<wp:meta_value><![CDATA[A summary of the major factors affecting adoption and diffusion of Artificial Intelligence in firms. These variables are essential to AI forecast modeling.]]></wp:meta_value>
		</wp:postmeta>
		<wp:comment>
			<wp:comment_id>35</wp:comment_id>
			<wp:comment_author><![CDATA[QQPokerRaja]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[landonbarrios@hailmail.net]]></wp:comment_author_email>
			<wp:comment_author_url>http://dewapoker38.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[134.236.245.76]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-03-28 21:10:04]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-03-28 21:10:04]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Excellent blog here! Also your website loads up fast!

What web host are you using? Can I get your affiliate link to your 
host? I wish my website loaded up as fast as yours lol]]></wp:comment_content>
			<wp:comment_approved><![CDATA[0]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
		</wp:comment>
	</item>
</channel>
</rss>
